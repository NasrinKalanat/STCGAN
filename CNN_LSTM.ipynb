{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMho+cIsf8R3fyr89kNJya"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eAIci8i_2hTc"},"outputs":[],"source":[" import sys\n","sys.path.append('/data/nak168/spatial_temporal/stream_img/CGAN')"]},{"cell_type":"code","source":["# !pip install Ninja"],"metadata":{"id":"5rZpFwv623Co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import glob\n","import xml.etree.ElementTree as ET\n","import os\n","import sys\n","import math\n","from collections import defaultdict\n","\n","# from tables import Column\n","from PIL import Image\n","import json\n","import pandas as pd\n","import datetime\n","from datetime import datetime, timedelta, timezone\n","import requests\n","import shutil\n","import time\n","from skimage.metrics import mean_squared_error, structural_similarity, normalized_root_mse\n","from sklearn.preprocessing import StandardScaler\n","import h5py\n","\n","import torch\n","import torch.utils.data\n","import torch.utils.data as Data\n","import torch.utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, models, transforms\n","from torch.optim import lr_scheduler"],"metadata":{"id":"5YZ_YXxf26Dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# one folder\n","\n","# def download_image(url, file_path, file_name=\"\"):\n","#   # print(\"downloading \"+url)\n","#   full_path = file_path + file_name\n","#   res = requests.get(url, stream = True)\n","#   if res.status_code == 200:\n","#     with open(full_path,'wb') as f:\n","#         shutil.copyfileobj(res.raw, f)\n","#     # print('Image sucessfully Downloaded: ',file_name)\n","#   else:\n","#     print('Image Couldn\\'t be retrieved '+ res.status_code)\n","\n","def download_image(url, file_path, file_name=\"\"):\n","    # download from image url and import it as a numpy array\n","    full_path = file_path + file_name\n","    res = requests.get(url, stream=True) # get full image\n","    if res.status_code == 200:\n","      img=res.raw\n","      img = Image.open(img)\n","      img = img.resize((360,360))\n","      # img = ImageOps.grayscale(img) # grayscale\n","      img = img.tobytes() # convert to bytes\n","      img = bytearray(img) # create byte array\n","      img = np.asarray(img, dtype=\"uint8\") # 360x360 array\n","      img = img.reshape(360, 360, 3)\n","      np.save(full_path,img)\n","\n","      # with open(full_path, 'rb') as f:\n","      #   img = np.load(full_path)\n","      #   img=img.astype('float32') / 255\n","      # fig,ax = plt.subplots(1)\n","      # ax.imshow(img)\n","\n","    else:\n","      print('Image Couldn\\'t be retrieved '+ res.status_code)\n","    return img\n","\n","class ImageNetVidDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, path, fol, phase=\"train\", len_seq=1, transform=None, mask_frac=0):\n","\n","    self.len_seq=len_seq\n","    self.transform = transform\n","\n","    cur_path = path\n","\n","    # load\n","    images, temps, dates=self.load_data(cur_path, fol)\n","\n","    # sort\n","    images, temps, dates=self.sort_data(images, temps, dates)\n","\n","    # normalize\n","    self.normalizer = StandardScaler()\n","    temps = self.normalizer.fit_transform(temps)\n","    # temps=np.log(temps+1)\n","\n","    # # mask\n","    # random_indices = np.random.choice(temps.shape[0], size=int(temps.shape[0] * mask_frac), replace=False)\n","    # temps[random_indices,:]=[None]\n","\n","    images, temps, dates=self.generate_many2many_data2(self.len_seq, images, temps, dates)\n","    # split to train, val, test\n","    self.images,self.labels,self.dates=self.data_split(images,temps,dates,phase,mask_frac)\n","    # self.images,self.labels,self.dates=self.generate_many2many_data_split(self.len_seq, images, temps, dates, phase)\n","\n","    # # download images (do just for first time)\n","    # if not os.path.exists(cur_path+\"/\"+fol+\"/images/\"):\n","    #   os.mkdir(cur_path+\"/\"+fol+\"/images/\")\n","    # delay=0.001\n","    # for i, (id,url) in enumerate(zip(self.imgs.image_id,self.imgs.url)):\n","    #   count=0\n","    #   while True:\n","    #       try:\n","    #         download_image(url,cur_path+fol+\"/images/\"+ str(id)+\".jpg\")\n","    #         self.imgs.ix[i,\"image_path\"]=cur_path+fol+\"/images/\"+ str(id)+\".jpg\"\n","    # #         download_image2(url,cur_path+fol+\"/images/\"+ str(id)+\".npy\")\n","    # #         self.imgs.loc[:,\"image_path\"].iloc[i]=cur_path+fol+\"/images/\"+ str(id)+\".npy\"\n","    #         time.sleep(delay)\n","    #         break\n","    #       except Exception as e:\n","    #         # raise e\n","    #         print(\"error: \", e, \"url: \",url)\n","    #         if count>5:\n","    #             break\n","    #         count+=1\n","    #         time.sleep(10*count)\n","\n","  def half_up_minute(self, x):\n","    delta = timedelta(minutes=15)\n","    ref_time = datetime(1970,1,1, tzinfo=x.tzinfo)\n","    return ref_time + round((x - ref_time) / delta) * delta\n","\n","  def _loadimage(self, path, url):\n","    # # load \".jpg\" files\n","    # with open(path, 'rb') as f:\n","    #   img = Image.open(f)\n","    #   return img.convert('RGB')\n","    try:\n","      with open(path, 'rb') as f:\n","        img = np.load(path,allow_pickle=True)\n","        return img\n","    except:\n","      img = download_image(url,path)\n","      with open(path, 'rb') as f:\n","        img = np.load(path,allow_pickle=True)\n","        return img\n","\n","  def denormalize(self, pred):\n","    return self.normalizer.inverse_transform(pred)\n","\n","  def load_data_from_h5(h5Path):\n","      f = h5py.File(h5Path, 'r')\n","      images = f['image']\n","      temps = f['temps']\n","      depths = f['depths']\n","      dates = f['dates']\n","\n","      return images, temps, depths, dates\n","\n","  def load_data(self, cur_path, fol):\n","      imgfile=pd.read_csv(cur_path + fol + '/images.csv',dtype={'station_name':str,'station_id':int,'image_id':int,'timestamp':str,'filename':str,'url':str}, parse_dates=['timestamp'])\n","      valuesfile = pd.read_csv(cur_path + fol + '/values.csv',dtype={'station_name':str,'station_id':int,'dataset_id':int,'series_id':int,'variable_id':str,'timestamp':str,'value':float}, parse_dates=['timestamp'])\n","      stationfile = pd.read_csv(cur_path + fol + '/station.csv')\n","\n","      # preprocessing time\n","      imgfile['timestamp'] = imgfile['timestamp'].map(self.half_up_minute)\n","      valuesfile['timestamp'] = valuesfile['timestamp'].map(self.half_up_minute)\n","\n","      data = imgfile.merge(valuesfile,on=[\"station_id\",\"timestamp\"])\n","\n","      times=data['timestamp'].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","      images=[]\n","      temps=[]\n","      dates=[]\n","      tmp=[]\n","      depths=[]\n","      for i, (id,url,v,t) in enumerate(zip(data.image_id,data.url,data.value,times)):\n","        # images.append(np.array(Image.fromarray(self._loadimage(cur_path+fol+\"/images/\"+ str(id)+\".npy\",url)).convert('L')))\n","        # img_cur=self._loadimage(cur_path+fol+\"/images/\"+ str(id)+\".npy\",url)\n","        # if self.transform is not None:\n","        #   img_cur=self.transform(img_cur)\n","\n","        images.append([cur_path+fol+\"/images/\"+ str(id)+\".npy\",url,id])\n","\n","        temps.append([v])\n","        dates.append([t.encode()])\n","\n","      # # import h5py\n","      #   tmp.append(np.array(Image.fromarray(self._loadimage(cur_path+fol+\"/images/\"+ str(id)+\".npy\",url)).convert('L')))\n","      #   depths.append([0])\n","\n","      # with h5py.File('/content/drive/My Drive/spatial_temporal/stream_img/fpe-westbrook/h5_2019.h5','w') as f:\n","      #   f['image'] = np.array(tmp)\n","      #   f['dates'] = np.array(dates)\n","      #   f['temps'] = np.array(temps)\n","      #   f['depths'] = np.array(depths)\n","\n","      dates=np.char.decode(np.array(dates))\n","      images=np.array(images)\n","      temps=np.array(temps)\n","\n","      return images, temps, dates\n","\n","\n","  def sort_data(self, images, temps, dates):\n","      index = np.argsort(dates, axis=0)\n","      index = index.reshape(index.shape[0],)\n","      return images[index], temps[index], dates[index]\n","\n","  def generate_many2many_data2(self, time_step, images, temps, dates):\n","      img_len = images.shape[0]\n","      train_x = []\n","      train_y = []\n","      train_d = []\n","      for i in range(0, img_len - time_step + 1, time_step):   # non-overlap\n","          train_x.append(images[i : i + time_step][:])\n","          train_y.append(temps[i : i + time_step][:])\n","          # train_y.append(np.concatenate((temps[i : i + time_step][:], depths[i : i + time_step][:]), axis=1))\n","          train_d.append(dates[i : i + time_step][:])\n","      train_x = np.array(train_x)\n","      train_y = np.array(train_y)\n","      train_d = np.array(train_d)\n","\n","      return train_x, train_y, train_d\n","\n","  def generate_many2many_data_split(self, time_step, images, temps, dates, phase):\n","      img_len = images.shape[0]\n","      train_x = []\n","      train_y = []\n","      train_d = []\n","      if phase==\"train\":\n","        for i in range(0, img_len - time_step + 1, 2*time_step):   # non-overlap\n","            train_x.append(images[i : i + time_step][:])\n","            train_y.append(temps[i : i + time_step][:])\n","            train_d.append(dates[i : i + time_step][:])\n","        train_x = np.array(train_x)\n","        train_y = np.array(train_y)\n","        train_d = np.array(train_d)\n","      elif phase==\"val\":\n","        for i in range(time_step, img_len - time_step + 1, 2*time_step):   # non-overlap\n","            train_x.append(images[i : i + int(time_step/2)][:])\n","            train_y.append(temps[i : i + int(time_step/2)][:])\n","            train_d.append(dates[i : i + int(time_step/2)][:])\n","        train_x = np.array(train_x)\n","        train_y = np.array(train_y)\n","        train_d = np.array(train_d)\n","      else:\n","        for i in range(int(3*time_step/2), img_len - time_step + 1, 2*time_step):   # non-overlap\n","            train_x.append(images[i : i + int(time_step/2)][:])\n","            train_y.append(temps[i : i + int(time_step/2)][:])\n","            train_d.append(dates[i : i + int(time_step/2)][:])\n","        train_x = np.array(train_x)\n","        train_y = np.array(train_y)\n","        train_d = np.array(train_d)\n","      return train_x, train_y, train_d\n","\n","  def data_split(self, t_x,t_y,t_d,phase,mask_frac):\n","      if phase==\"train\":\n","        train_x1 = t_x[:round(t_x.shape[0] * 0.375)]\n","        train_y1 = t_y[:round(t_y.shape[0] * 0.375)]\n","        train_d1 = t_d[:round(t_d.shape[0] * 0.375)]\n","        train_x2 = t_x[round(t_x.shape[0] * 0.875):]\n","        train_y2 = t_y[round(t_y.shape[0] * 0.875):]\n","        train_d2 = t_d[round(t_d.shape[0] * 0.875):]\n","\n","        imgs = np.concatenate((train_x1, train_x2), axis=0)\n","        labels = np.concatenate((train_y1, train_y2), axis=0)\n","        dates = np.concatenate((train_d1, train_d2), axis=0)\n","\n","        if mask_frac==-1:\n","            # mask sequentially\n","            labels[:round(t_y.shape[0] * 0.375),:,:] = [None]\n","        else:\n","            # mask randomly\n","            random_indices = np.random.choice(labels.size, size=int(labels.size * mask_frac), replace=False)\n","            row_indices, col_indices = random_indices//labels.shape[1], random_indices%labels.shape[1]\n","            labels[row_indices, col_indices,:]=[None]\n","\n","      elif phase==\"val\":\n","        imgs = t_x[round(t_x.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","        labels = t_y[round(t_y.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","        dates = t_d[round(t_d.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","      else:\n","        imgs = t_x[round(t_x.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","        labels = t_y[round(t_y.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","        dates = t_d[round(t_d.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","      return imgs, labels, dates\n","\n","  def __getitem__(self, id):\n","    # print(id)\n","    # imgs=torch.zeros((self.len_seq,360,360),dtype=torch.float32)\n","    imgs=torch.zeros((self.len_seq,3,360,360),dtype=torch.float32)\n","    t=torch.zeros((self.len_seq,1),dtype=torch.float32)\n","    lbl=torch.zeros((self.len_seq,1),dtype=torch.float32)\n","    ids=np.zeros((self.len_seq,1))\n","    for i in range(self.len_seq):\n","      cur_path,url=self.images[id][i][0],self.images[id][i][1]\n","      img_cur=self._loadimage(cur_path,url)\n","      if self.transform is not None:\n","        img_cur=self.transform(img_cur) # The pixel values of the tensor are of type float32 and range from 0 to 1 (transforms.ToTensor() method includes normalization as part of its functionality, by dividing the pixel values by 255 to rescale them to the range [0, 1]).\n","      imgs[i,...]=img_cur\n","\n","      t[i,...]=torch.tensor([datetime.strptime(self.dates[id][i][0], \"%Y-%m-%dT%H:%M:%SZ\").timestamp()])\n","      ids[i,...]=self.images[id][i][2]\n","\n","      # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur))).permute(2,0,1) # The pixel values of the tensor are of type uint8 and range from 0 to 255.\n","      # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur).convert('L')))\n","\n","      # t=np.array([np.datetime64(self.dates[id][0][0]).astype(np.float32)])\n","    # return imgs.squeeze(), torch.from_numpy(t), torch.from_numpy(self.labels[id][0].astype(np.float32)), self.images[id][0][2]\n","    return imgs.squeeze(), t, torch.from_numpy(self.labels[id].astype(np.float32)), torch.from_numpy(ids)\n","\n","  def __len__(self):\n","      return self.images.shape[0]"],"metadata":{"id":"mXEllKCH28ub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # one folder\n","\n","# # def download_image(url, file_path, file_name=\"\"):\n","# #   # print(\"downloading \"+url)\n","# #   full_path = file_path + file_name\n","# #   res = requests.get(url, stream = True)\n","# #   if res.status_code == 200:\n","# #     with open(full_path,'wb') as f:\n","# #         shutil.copyfileobj(res.raw, f)\n","# #     # print('Image sucessfully Downloaded: ',file_name)\n","# #   else:\n","# #     print('Image Couldn\\'t be retrieved '+ res.status_code)\n","\n","# def download_image(url, file_path, file_name=\"\"):\n","#     # download from image url and import it as a numpy array\n","#     full_path = file_path + file_name\n","#     res = requests.get(url, stream=True) # get full image\n","#     if res.status_code == 200:\n","#       img=res.raw\n","#       img = Image.open(img)\n","#       img = img.resize((360,360))\n","#       # img = ImageOps.grayscale(img) # grayscale\n","#       img = img.tobytes() # convert to bytes\n","#       img = bytearray(img) # create byte array\n","#       img = np.asarray(img, dtype=\"uint8\") # 360x360 array\n","#       img = img.reshape(360, 360, 3)\n","#       np.save(full_path,img)\n","\n","#       # with open(full_path, 'rb') as f:\n","#       #   img = np.load(full_path)\n","#       #   img=img.astype('float32') / 255\n","#       # fig,ax = plt.subplots(1)\n","#       # ax.imshow(img)\n","\n","#     else:\n","#       print('Image Couldn\\'t be retrieved '+ res.status_code)\n","#     return img\n","\n","# class ImageNetVidDataset(torch.utils.data.Dataset):\n","\n","#   def __init__(self, path, fol_A, fol_B, phase=\"train\", len_seq=1, transform=None):\n","\n","#     self.len_seq=len_seq\n","#     self.transform = transform\n","\n","#     cur_path = path+'/fpe-westbrook/'\n","\n","#     # load\n","#     images_A, temps_A, dates_A, images_B, temps_B, dates_B=self.load_data(cur_path, fol_A, fol_B)\n","\n","#     # sort\n","#     images_A, temps_A, dates_A=self.sort_data(images_A, temps_A, dates_A)\n","#     images_B, temps_B, dates_B=self.sort_data(images_B, temps_B, dates_B)\n","\n","#     # normalize\n","#     self.normalizer = StandardScaler()\n","#     temps_A = self.normalizer.fit_transform(temps_A)\n","#     temps_B = self.normalizer.fit_transform(temps_B)\n","\n","#     images_A, temps_A, dates_A=self.generate_many2many_data2(self.len_seq, images_A, temps_A, dates_A)\n","#     images_B, temps_B, dates_B=self.generate_many2many_data2(self.len_seq, images_B, temps_B, dates_B)\n","\n","#     # split to train, val, test\n","#     self.images_A,self.labels_A,self.dates_A=self.data_split(images_A, temps_A, dates_A,phase)\n","#     self.images_B,self.labels_B,self.dates_B=self.data_split(images_B, temps_B, dates_B,phase)\n","\n","#     # # download images (do just for first time)\n","#     # if not os.path.exists(cur_path+\"/\"+fol+\"/images/\"):\n","#     #   os.mkdir(cur_path+\"/\"+fol+\"/images/\")\n","#     # delay=0.001\n","#     # for i, (id,url) in enumerate(zip(self.imgs.image_id,self.imgs.url)):\n","#     #   count=0\n","#     #   while True:\n","#     #       try:\n","#     #         download_image(url,cur_path+fol+\"/images/\"+ str(id)+\".jpg\")\n","#     #         self.imgs.ix[i,\"image_path\"]=cur_path+fol+\"/images/\"+ str(id)+\".jpg\"\n","#     # #         download_image2(url,cur_path+fol+\"/images/\"+ str(id)+\".npy\")\n","#     # #         self.imgs.loc[:,\"image_path\"].iloc[i]=cur_path+fol+\"/images/\"+ str(id)+\".npy\"\n","#     #         time.sleep(delay)\n","#     #         break\n","#     #       except Exception as e:\n","#     #         # raise e\n","#     #         print(\"error: \", e, \"url: \",url)\n","#     #         if count>5:\n","#     #             break\n","#     #         count+=1\n","#     #         time.sleep(10*count)\n","\n","#   def half_up_minute(self, x):\n","#     delta = timedelta(minutes=5)\n","#     ref_time = datetime(1970,1,1, tzinfo=x.tzinfo)\n","#     return ref_time + round((x - ref_time) / delta) * delta\n","\n","#   def _loadimage(self, path, url):\n","#     # # load \".jpg\" files\n","#     # with open(path, 'rb') as f:\n","#     #   img = Image.open(f)\n","#     #   return img.convert('RGB')\n","#     try:\n","#       with open(path, 'rb') as f:\n","#         img = np.load(path,allow_pickle=True)\n","#         return img\n","#     except:\n","#       img = download_image(url,path)\n","#       with open(path, 'rb') as f:\n","#         img = np.load(path,allow_pickle=True)\n","#         return img\n","\n","#   def denormalize(self, pred):\n","#     return self.normalizer.inverse_transform(pred)\n","\n","#   def load_data_from_h5(h5Path):\n","#       f = h5py.File(h5Path, 'r')\n","#       images = f['image']\n","#       temps = f['temps']\n","#       depths = f['depths']\n","#       dates = f['dates']\n","\n","#       return images, temps, depths, dates\n","\n","#   def df2np(self, cur_path, fol, data):\n","\n","#       # times=data['timestamp'].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","#       times=data['timestamp']\n","\n","#       images=[]\n","#       temps=[]\n","#       dates=[]\n","#       for i, (id,url,v,t) in enumerate(zip(data.image_id,data.url,data.value,times)):\n","#         # images.append(np.array(Image.fromarray(self._loadimage(cur_path+fol+\"/images/\"+ str(id)+\".npy\",url)).convert('L')))\n","#         # img_cur=self._loadimage(cur_path+fol+\"/images/\"+ str(id)+\".npy\",url)\n","#         # if self.transform is not None:\n","#         #   img_cur=self.transform(img_cur)\n","\n","#         images.append([cur_path+fol+\"/images/\"+ str(id)+\".npy\",url])\n","\n","#         temps.append([v])\n","#         dates.append([t.encode()])\n","#         # dates.append([d])\n","\n","#       dates=np.char.decode(np.array(dates))\n","#       # dates=np.array(dates)\n","#       images=np.array(images)\n","#       temps=np.array(temps)\n","\n","#       return images, temps, dates\n","\n","#   def load_data(self, cur_path, fol_A, fol_B):\n","#       fol=fol_A\n","#       imgfile=pd.read_csv(cur_path + fol + '/images.csv',dtype={'station_name':str,'station_id':int,'image_id':int,'timestamp':str,'filename':str,'url':str}, parse_dates=['timestamp'])\n","#       valuesfile = pd.read_csv(cur_path + fol + '/values.csv',dtype={'station_name':str,'station_id':int,'dataset_id':int,'series_id':int,'variable_id':str,'timestamp':str,'value':float}, parse_dates=['timestamp'])\n","#       stationfile = pd.read_csv(cur_path + fol + '/station.csv')\n","\n","#       # preprocessing time\n","#       imgfile['timestamp'] = imgfile['timestamp'].map(self.half_up_minute)\n","#       valuesfile['timestamp'] = valuesfile['timestamp'].map(self.half_up_minute)\n","\n","#       data_A = imgfile.merge(valuesfile,on=[\"station_id\",\"timestamp\"])\n","#       data_A['timestamp']=data_A['timestamp'].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","#       fol=fol_B\n","#       imgfile=pd.read_csv(cur_path + fol + '/images.csv',dtype={'station_name':str,'station_id':int,'image_id':int,'timestamp':str,'filename':str,'url':str}, parse_dates=['timestamp'])\n","#       valuesfile = pd.read_csv(cur_path + fol + '/values.csv',dtype={'station_name':str,'station_id':int,'dataset_id':int,'series_id':int,'variable_id':str,'timestamp':str,'value':float}, parse_dates=['timestamp'])\n","#       stationfile = pd.read_csv(cur_path + fol + '/station.csv')\n","\n","#       # preprocessing time\n","#       imgfile['timestamp'] = imgfile['timestamp'].map(self.half_up_minute)\n","#       valuesfile['timestamp'] = valuesfile['timestamp'].map(self.half_up_minute)\n","\n","#       data_B = imgfile.merge(valuesfile,on=[\"station_id\",\"timestamp\"])\n","#       data_B['timestamp']=data_B['timestamp'].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","#       data_A = data_A.loc[data_A.timestamp.isin(data_B.timestamp)]\n","#       data_B = data_B.loc[data_B.timestamp.isin(data_A.timestamp)]\n","\n","#       data_A=data_A.drop_duplicates(subset=['timestamp'],keep='first')\n","#       # tmp=data_A.groupby(['timestamp'])['timestamp'].count()\n","#       # print(tmp.loc[tmp>1])\n","#       data_B=data_B.drop_duplicates(subset=['timestamp'],keep='first')\n","#       # tmp=data_B.groupby(['timestamp'])['timestamp'].count()\n","#       # print(tmp.loc[tmp>1])\n","\n","#       images_A, temps_A, dates_A = self.df2np(cur_path, fol_A, data_A)\n","#       images_B, temps_B, dates_B = self.df2np(cur_path, fol_B, data_B)\n","#       return images_A, temps_A, dates_A, images_B, temps_B, dates_B\n","\n","#   def sort_data(self, images, temps, dates):\n","#       index = np.argsort(dates, axis=0)\n","#       index = index.reshape(index.shape[0],)\n","#       return images[index], temps[index], dates[index]\n","\n","\n","#   def generate_train_data(self, time_step, images, temps, depths):\n","#       img_len = images.shape[0]\n","#       train_x = []\n","#       train_y = []\n","#       for i in range(img_len- time_step + 1):\n","#           train_x.append(images[i : i + time_step][:])\n","#           train_y.append([temps[i + time_step - 1][0]])\n","#           # train_y.append([temps[i+time_step-1][0], depths[i+time_step-1][0]])\n","#       train_x = np.array(train_x)\n","#       train_y = np.array(train_y)\n","\n","#       return train_x, train_y\n","\n","\n","#   def generate_many2many_data(self, time_step, images, temps):\n","#       img_len = images.shape[0]\n","#       train_x = []\n","#       train_y = []\n","#       for i in range(0, img_len - time_step + 1, round(time_step / 2)):   # overlap 50%\n","#           train_x.append(images[i : i + time_step][:])\n","#           train_y.append(temps[i : i + time_step][:])\n","#           # train_y.append(np.concatenate((temps[i : i + time_step][:], depths[i : i + time_step][:]), axis=1))\n","#       train_x = np.array(train_x)\n","#       train_y = np.array(train_y)\n","\n","#       return train_x, train_y\n","\n","#   def generate_many2many_data2(self, time_step, images, temps, dates):\n","#       img_len = images.shape[0]\n","#       train_x = []\n","#       train_y = []\n","#       train_d = []\n","#       for i in range(0, img_len - time_step + 1, time_step):   # non-overlap\n","#           train_x.append(images[i : i + time_step][:])\n","#           train_y.append(temps[i : i + time_step][:])\n","#           # train_y.append(np.concatenate((temps[i : i + time_step][:], depths[i : i + time_step][:]), axis=1))\n","#           train_d.append(dates[i : i + time_step][:])\n","#       train_x = np.array(train_x)\n","#       train_y = np.array(train_y)\n","#       train_d = np.array(train_d)\n","\n","#       return train_x, train_y, train_d\n","\n","#   def data_split(self, t_x,t_y,t_d,phase):\n","#       if phase==\"train\":\n","#         train_x1 = t_x[:round(t_x.shape[0] * 0.375)]\n","#         train_y1 = t_y[:round(t_y.shape[0] * 0.375)]\n","#         train_d1 = t_d[:round(t_d.shape[0] * 0.375)]\n","#         train_x2 = t_x[round(t_x.shape[0] * 0.875):]\n","#         train_y2 = t_y[round(t_y.shape[0] * 0.875):]\n","#         train_d2 = t_d[round(t_d.shape[0] * 0.875):]\n","#         imgs = np.concatenate((train_x1, train_x2), axis=0)\n","#         labels = np.concatenate((train_y1, train_y2), axis=0)\n","#         dates = np.concatenate((train_d1, train_d2), axis=0)\n","#       elif phase==\"val\":\n","#         imgs = t_x[round(t_x.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","#         labels = t_y[round(t_y.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","#         dates = t_d[round(t_d.shape[0] * 0.675):round(t_x.shape[0] * 0.875)]\n","#       else:\n","#         imgs = t_x[round(t_x.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","#         labels = t_y[round(t_y.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","#         dates = t_d[round(t_d.shape[0] * 0.375):round(t_x.shape[0] * 0.675)]\n","#       return imgs, labels, dates\n","\n","#   def __getitem__(self, id):\n","\n","#     # imgs=torch.zeros((self.len_seq,360,360),dtype=torch.float32)\n","#     imgs_A=torch.zeros((self.len_seq,3,360,360),dtype=torch.float32)\n","#     for i in range(self.len_seq):\n","#       cur_path,url=self.images_A[id][i][0],self.images_A[id][i][1]\n","#       img_cur=self._loadimage(cur_path,url)\n","#       if self.transform is not None:\n","#         img_cur=self.transform(img_cur) # The pixel values of the tensor are of type float32 and range from 0 to 1 (transforms.ToTensor() method includes normalization as part of its functionality, by dividing the pixel values by 255 to rescale them to the range [0, 1]).\n","#       imgs_A[i,...]=img_cur\n","\n","#       # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur))).permute(2,0,1) # The pixel values of the tensor are of type uint8 and range from 0 to 255.\n","#       # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur).convert('L')))\n","\n","#     imgs_B=torch.zeros((self.len_seq,3,360,360),dtype=torch.float32)\n","#     for i in range(self.len_seq):\n","#       cur_path,url=self.images_B[id][i][0],self.images_B[id][i][1]\n","#       img_cur=self._loadimage(cur_path,url)\n","#       if self.transform is not None:\n","#         img_cur=self.transform(img_cur) # The pixel values of the tensor are of type float32 and range from 0 to 1 (transforms.ToTensor() method includes normalization as part of its functionality, by dividing the pixel values by 255 to rescale them to the range [0, 1]).\n","#       imgs_B[i,...]=img_cur\n","\n","#       # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur))).permute(2,0,1) # The pixel values of the tensor are of type uint8 and range from 0 to 255.\n","#       # imgs[i,...]=torch.from_numpy(np.array(Image.fromarray(img_cur).convert('L')))\n","\n","#     # return imgs, torch.from_numpy(self.labels[id].astype(np.float32)), False\n","#     return imgs_A.squeeze(), torch.from_numpy(self.labels_A[id].astype(np.float32)), imgs_B.squeeze(), torch.from_numpy(self.labels_B[id].astype(np.float32))\n","\n","#   def __len__(self):\n","#       return self.images_A.shape[0]"],"metadata":{"id":"Fl8uMJ8R3AQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transform = transforms.Compose([  # transforms.Compose : a class that calls the functions in a list consecutively\n","        transforms.ToTensor() # ToTensor : convert numpy image to torch.Tensor type\n","    ])\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# #transform for image\n","# transform = transforms.Compose([\n","#         transforms.RandomHorizontalFlip(),\n","#         transforms.Resize((255,255)),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize((0.485, 0.456, 0.406),\n","#                              (0.229, 0.224, 0.225))])\n","#transform for nparray\n","transform=transforms.Compose([\n","    transforms.ToPILImage(),\n","    # transforms.CenterCrop(340),\n","    # transforms.Resize(360, Image.BICUBIC),\n","    # transforms.Resize((360,360)),\n","    # transforms.Grayscale(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    # torch.squeeze,\n","    # np.array\n","    ])\n"],"metadata":{"id":"jbLnXkR83EzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CMU 16-726 Learning-Based Image Synthesis / Spring 2023, Assignment 3\n","# The code is based on this paper:\n","# Differentiable Augmentation for Data-Efficient GAN Training\n","# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n","# https://arxiv.org/pdf/2006.10738\n","\n","def DiffAugment(x, policy='', channels_first=True):\n","    if policy:\n","        if not channels_first:\n","            x = x.permute(0, 3, 1, 2)\n","        for p in policy.split(','):\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x)\n","        if not channels_first:\n","            x = x.permute(0, 2, 3, 1)\n","        x = x.contiguous()\n","    return x\n","\n","\n","def rand_brightness(x):\n","    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n","    return x\n","\n","\n","def rand_saturation(x):\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x):\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n","    return x\n","\n","\n","def rand_translation(x, ratio=0.125):\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, ratio=0.5):\n","    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","    mask[grid_batch, grid_x, grid_y] = 0\n","    x = x * mask.unsqueeze(1)\n","    return x\n","\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'translation': [rand_translation],\n","    'cutout': [rand_cutout],\n","}\n"],"metadata":{"id":"t_ttdEcD3JyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CMU 16-726 Learning-Based Image Synthesis / Spring 2023, Assignment 3\n","# The code base is based on the great work from CSC 321, U Toronto\n","# https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip\n","# CSC 321, Assignment 4\n","#\n","# This file contains the models used for both parts of the assignment:\n","#\n","#   - DCGenerator        --> Used in the vanilla GAN in Part 1\n","#   - CycleGenerator     --> Used in the CycleGAN in Part 2\n","#   - DCDiscriminator    --> Used in both the vanilla GAN in Part 1\n","#   - PatchDiscriminator --> Used in the CycleGAN in Part 2\n","# For the assignment, you are asked to create the architectures of these\n","# three networks by filling in the __init__ and forward methods in the\n","# DCGenerator, CycleGenerator, DCDiscriminator, and PatchDiscriminator classes.\n","# Feel free to add and try your own models\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","def up_conv(in_channels, out_channels, kernel_size, stride=1, padding=1,\n","            scale_factor=2, norm='batch', activ=None):\n","    \"\"\"Create a transposed-convolutional layer, with optional normalization.\"\"\"\n","    layers = []\n","    layers.append(nn.Upsample(scale_factor=scale_factor, mode='nearest'))\n","    layers.append(nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size, stride, padding, bias=norm is None\n","    ))\n","    if norm == 'batch':\n","        layers.append(nn.BatchNorm2d(out_channels))\n","    elif norm == 'instance':\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","    if activ == 'relu':\n","        layers.append(nn.ReLU())\n","    elif activ == 'leaky':\n","        layers.append(nn.LeakyReLU())\n","    elif activ == 'tanh':\n","        layers.append(nn.Tanh())\n","\n","    return nn.Sequential(*layers)\n","\n","\n","def conv(in_channels, out_channels, kernel_size, stride=2, padding=1,\n","         norm='batch', init_zero_weights=False, activ=None, groups=1):\n","    \"\"\"Create a convolutional layer, with optional normalization.\"\"\"\n","    layers = []\n","    conv_layer = nn.Conv2d(\n","        in_channels=in_channels, out_channels=out_channels,\n","        kernel_size=kernel_size, stride=stride, padding=padding,\n","        groups=groups, bias=norm is None\n","    )\n","    if init_zero_weights:\n","        conv_layer.weight.data = 0.001 * torch.randn(\n","            out_channels, in_channels, kernel_size, kernel_size\n","        )\n","    layers.append(conv_layer)\n","\n","    if norm == 'batch':\n","        layers.append(nn.BatchNorm2d(out_channels))\n","    elif norm == 'instance':\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","    if activ == 'relu':\n","        layers.append(nn.ReLU())\n","    elif activ == 'leaky':\n","        layers.append(nn.LeakyReLU())\n","    elif activ == 'tanh':\n","        layers.append(nn.Tanh())\n","    return nn.Sequential(*layers)\n","\n","\n","class ResnetBlock(nn.Module):\n","\n","    def __init__(self, conv_dim, norm, activ, groups=1):\n","        super().__init__()\n","        self.conv_layer = conv(\n","            in_channels=conv_dim, out_channels=conv_dim,\n","            kernel_size=3, stride=1, padding=1, norm=norm,\n","            activ=activ, groups=groups\n","        )\n","\n","    def forward(self, x):\n","        out = x + self.conv_layer(x)\n","        return out\n","\n","\n","class CycleGenerator(nn.Module):\n","    \"\"\"Architecture of the generator network.\"\"\"\n","\n","    def __init__(self, conv_dim=64, init_zero_weights=False, norm='instance', c_dim=4,\n","                 cond_args           = {},\n","                 mapping_kwargs      = {}):\n","        super().__init__()\n","\n","        ###########################################\n","        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","        ###########################################\n","\n","        # 1. Define the encoder part of the generator\n","        self.conv1 = conv(3,32,4,2,1,'instance','relu')\n","        self.conv2 = conv(32,64,4,2,1,'instance','relu')\n","\n","        # 2. Define the transformation part of the generator\n","        self.resnet_block1 = ResnetBlock(64,'instance','relu')\n","        self.resnet_block2 = ResnetBlock(64, 'instance', 'relu')\n","        self.resnet_block3 = ResnetBlock(64, 'instance', 'relu')\n","\n","        # 3. Define the decoder part of the generator\n","        self.up_conv1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","        self.up_conv2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","        # self.cond_xform = ConditioningTransform(cond_args=cond_args, num_ws=1)\n","        # if c_dim > 0:\n","        #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        #   self.c_dim=c_dims_M\n","        #   self.mapping = MappingNetworkCond(z_dim=conv_dim, c_dim=c_dims_M, w_dim=3, num_ws=1, **mapping_kwargs)\n","\n","\n","    def forward(self, x, c=None):\n","        \"\"\"\n","        Generate an image conditioned on an input image.\n","\n","        Input\n","        -----\n","            x: BS x 3 x 32 x 32\n","\n","        Output\n","        ------\n","            out: BS x 3 x 32 x 32\n","        \"\"\"\n","        ###########################################\n","        ##   FILL THIS IN: FORWARD PASS   ##\n","        ###########################################\n","\n","        # if self.c_dim > 0:\n","        #   cs = self.cond_xform(c)\n","        #   c_iter = iter(cs.unbind(dim=1))\n","        #   x = self.mapping(x, cs[:,0,:])\n","\n","        x=self.conv1(x)\n","        x=self.conv2(x)\n","        x=self.resnet_block1(x)\n","        x=self.resnet_block2(x)\n","        x=self.resnet_block3(x)\n","        x=self.up_conv1(x)\n","        x=self.up_conv2(x)\n","        return x\n","\n","\n","class DCDiscriminator(nn.Module):\n","    \"\"\"Architecture of the discriminator network.\"\"\"\n","\n","    def __init__(self, conv_dim=64, norm='instance', c_dim=4,\n","                 cond_args           = {},\n","                 mapping_kwargs      = {}):\n","        super().__init__()\n","        self.conv1 = conv(3, 32, 4, 2, 1, norm, False, 'relu')\n","        self.conv2 = conv(32, 64, 4, 2, 1, norm, False, 'relu')\n","        self.conv3 = conv(64, 128, 4, 2, 1, norm, False, 'relu')\n","        self.conv4 = conv(128, 256, 4, 2, 1, norm, False, 'relu')\n","        self.conv5 = conv(256, 1, 4, 2, 0,None)\n","\n","        # self.cond_xform = ConditioningTransform(cond_args=cond_args)\n","        # if c_dim > 0:\n","        #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        #   self.c_dim=c_dims_M\n","        #   self.cmap_dim=c_dim\n","        #   self.mapping = MappingNetworkCond(z_dim=conv_dim, c_dim=c_dims_M, w_dim=3, num_ws=1, **mapping_kwargs)\n","\n","    def forward(self, x, c=None):\n","        \"\"\"Forward pass, x is (B, C, H, W).\"\"\"\n","\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","\n","        # if self.c_dim > 0:\n","        #     cs = self.cond_xform(c, broadcast=False)\n","        #     cmap = self.mapping(None, cs)\n","\n","        #     x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n","\n","        return x.squeeze()\n","\n","\n","class PatchDiscriminator(nn.Module):\n","    \"\"\"Architecture of the patch discriminator network.\"\"\"\n","\n","    def __init__(self, conv_dim=64, norm='batch', c_dim=4,\n","                 cond_args           = {},\n","                 mapping_kwargs      = {}):\n","        super().__init__()\n","\n","        ###########################################\n","        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","        ###########################################\n","\n","        # Hint: it should look really similar to DCDiscriminator.\n","\n","        self.conv1 = conv(3, 32, 4, 2, 1, norm, False, 'relu')\n","        self.conv2 = conv(32, 64, 4, 2, 1, norm, False, 'relu')\n","        self.conv3 = conv(64, 128, 4, 2, 1, norm, False, 'relu')\n","        self.conv4 = conv(128, 1, 4, 2, 1, None)\n","\n","        # self.cond_xform = ConditioningTransform(cond_args=cond_args, num_ws=1)\n","        # if c_dim > 0:\n","        #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        #   self.c_dim=c_dims_M\n","        #   self.cmap_dim=c_dim\n","        #   self.mapping = MappingNetworkCond(z_dim=0, c_dim=c_dims_M, w_dim=c_dim, num_ws=1, **mapping_kwargs)\n","\n","        #   self.fc = FullyConnectedLayer(1 * (22 ** 2), 1)\n","        #   self.out = FullyConnectedLayer(1, 1 if self.cmap_dim == 0 else self.cmap_dim)\n","\n","    def forward(self, x, c=None):\n","\n","        ###########################################\n","        ##   FILL THIS IN: FORWARD PASS   ##\n","        ###########################################\n","\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","\n","        # if self.c_dim > 0:\n","        # x = self.fc(x.flatten(1))\n","        # x = self.out(x)\n","        #     cs = self.cond_xform(c, broadcast=False)\n","        #     cmap = self.mapping(None, cs)\n","\n","        #     x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n","\n","        return x\n","\n","\n","class deep_resnet34(nn.Module):\n","  def __init__(self, conv_dim=64, norm='instance', c_dim=4,\n","                 cond_args           = {},\n","                 mapping_kwargs      = {}):\n","      super(deep_resnet34, self).__init__()\n","      resnet = models.resnet34(pretrained=True)\n","      layers = list(resnet.children())[:8]\n","      self.features1 = nn.Sequential(*layers[:6])\n","      self.fc1 = nn.Sequential(nn.Linear(512, 32),nn.BatchNorm1d(32),nn.ReLU(inplace=True),nn.Dropout(0.4))\n","      self.fc2 = nn.Linear(32, 1)\n","      # self.dropout = nn.Dropout(0.2)\n","\n","      # self.cond_xform = ConditioningTransform(cond_args=cond_args)\n","      # if c_dim > 0:\n","      #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","      #   self.c_dim=c_dims_M\n","      #   self.cmap_dim=c_dim\n","      #   self.mapping = MappingNetworkCond(z_dim=0, c_dim=c_dims_M, w_dim=3, num_ws=1, **mapping_kwargs)\n","\n","  def forward(self, x, c):\n","\n","      # cmap = None\n","      # if self.c_dim > 0:\n","      #     cs = self.cond_xform(c, broadcast=False)\n","      #     cmap = self.mapping(None, cs)\n","\n","      #     x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n","\n","      x = self.features1(x)\n","      # print(\"feat: \",x.shape)\n","      x = F.relu(x)\n","      x = nn.AdaptiveAvgPool2d((2,2))(x)\n","      # print(\"pool: \", x.shape)\n","      x = x.view(x.shape[0], -1)\n","\n","      x = self.fc1(x)\n","      x = self.fc2(x)\n","\n","      return x\n","\n","class CNN_LSTM(nn.Module):\n","    def __init__(self, cond_args = {}):\n","        super(CNN_LSTM, self).__init__()\n","        self.Conv = nn.Sequential(                                                                                      # (N, 1, 360, 360)\n","            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=(9, 9), stride=5, padding=2, bias=False),             # (N, 96, 72, 72)\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(4, 4), stride=2, padding=0),                                                      # (N, 96, 35, 35)\n","\n","            nn.Conv2d(in_channels=96, out_channels=192, kernel_size=(5, 5), stride=1, padding=2, bias=False),           # (N, 192, 35, 35)\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=0),                                                      # (N, 192, 17, 17)\n","\n","            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=(3, 3), stride=1, padding=1, bias=False),          # (N, 384, 17, 17)\n","            nn.ReLU(inplace=True),\n","            # nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=0),                                                      # (N, 384, 8, 8)\n","\n","            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3, 3), stride=1, padding=1, bias=False),          # (N, 256, 8, 8)\n","            nn.ReLU(inplace=True),\n","            # nn.MaxPool2d(kernel_size=(4, 4), stride=2, padding=0),                                                      # (N, 256, 3, 3)\n","\n","            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=1, padding=1, bias=False),          # (N, 256, 3, 3)\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=0),                                                      # (N, 256, 1, 1)\n","        )\n","\n","        # self.BN = nn.BatchNorm1d(batch_size)\n","\n","        self.LSTM = nn.LSTM(input_size=256*8*8, hidden_size=1024)               # (seq_len, N, 1024)\n","\n","        self.output = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(1024, 64),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            # nn.Linear(256, 64),\n","            # nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x, c):\n","\n","#         len_seq = x.shape[1]       # 时间步长  # x.shape: (N, step, 360, 360)\n","\n","        x=x.reshape(batch_size, len_seq, x.shape[1], x.shape[2], x.shape[3])\n","        gray=transforms.Grayscale()\n","        input_lstm = torch.empty(len_seq, batch_size, 256*8*8)           # 创建个空张量,作为lstm输入\n","        for i in range(len_seq):\n","            cnn_x = self.Conv(gray((x[:, i, :, :, :]+1)*255./2)/255.)   # 第i张图片经过CNN input:(N, 1, 360, 360);output:(N, 256, 8, 8)\n","            # cnn_x = self.Conv(x[:, i, :, :, :])   # 第i张图片经过CNN input:(N, 1, 360, 360);output:(N, 256, 8, 8)\n","            cnn_x = cnn_x.data.cpu()\n","            cnn_x_cpu_flatten = cnn_x.view(cnn_x.shape[0], -1)      # (N, 256*8*8)\n","            input_lstm[i] = cnn_x_cpu_flatten                               # 存入第i张图片的特征向量\n","\n","        input_lstm = input_lstm.data.cuda()  # (seq_len, N, 256*8*8)\n","        # input_lstm = self.BN(input_lstm)\n","        output_lstm, _ = self.LSTM(input_lstm)     # input:(seq_len, N, 256*8*8); output:（seq_len，N，1024）\n","\n","#         output_lstm = output_lstm.reshape(output_lstm.shape[0] * output_lstm.shape[1], 1024) # (seq_len * N，1024）\n","\n","#         output = self.output(output_lstm)        # input:(seq_len * N, 1024); output:(seq_len * N, 1)\n","\n","        output = output_lstm.permute(1, 0, 2)  # (N, seq_len, 1)\n","        output = output.reshape(output.shape[0] * output.shape[1], 1024)\n","        output = self.output(output)        # input:(N * seq_len, 1024); output:(N * seq_len, 1)\n","\n","        return output       # (seq_len * N, 1)\n","\n","# if __name__ == \"__main__\":\n","#     a = torch.rand(4, 3, 64, 64)\n","#     D = PatchDiscriminator()\n","#     print(D(a).shape)\n","#     G = CycleGenerator()\n","#     print(G(a).shape)"],"metadata":{"id":"BVZeN8uH3NIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dnnlib.util import EasyDict\n","\n","def_comb = EasyDict(type='mult', degree=-1)\n","specs = { d['type']: d for d in [\n","    dict(type='none',     dequant='gauss', noise=0.0,                             dims=0, lr=1, lin_lr=1e-2),\n","    dict(type='concat',   dequant='gauss', noise=0.0, noise_f_int=[],             dims=1, lr=1, lin_lr=1e-2), # 1 frame overlap in conditioning\n","    dict(type='fourier',  dequant='gauss', noise=0.0, noise_f_int=[], noise_f=[], dims=2, lr=1, lin_lr=1e-2, f_manual=[], include_lin=True),\n","    dict(type='f_concat', dequant='gauss', noise=0.0, noise_f_int=[], noise_f=[], dims=2, lr=1, lin_lr=1e-2, f_manual=[], include_lin=True), # fourier features given to concat\n","]}"],"metadata":{"id":"xIDX-IL_3R9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Iterable\n","def _set_noise_global(c, noise, n_frames, n_days):\n","    scale = parse_noises([noise], c, n_frames, n_days)[0]\n","    c.cond_args.noise = scale # global noise, measured in frame deltas\n","    c.cond_args.noise_f = [] # disable per-f noise\n","    return f'-n{str(noise).replace(\" \", \"\")}'\n","\n","def _set_noise_per_f(c, noises, n_frames, n_days):\n","    scales = parse_noises(noises, c, n_frames, n_days)\n","    assert np.all(np.diff(scales) <= 0), 'Noises not descending, are you sure?'\n","    c.cond_args.noise = 0        # disable global noise\n","    c.cond_args.noise_f = scales # in frame deltas, for [1e-3f] + [manual freqs]\n","    return '-n_' + '_'.join([str(f).replace(' ', '') for f in noises])\n","\n","def _set_freqs(c, base_freqs, explicit_lin):\n","    assert c.cond_args.type in ['fourier', 'f_concat']\n","\n","    desc = ''\n","    if explicit_lin:\n","        c.cond_args.f_manual = [*base_freqs]\n","        c.cond_args.include_lin = True\n","        c.cond_args.dims = 2*(len(c.cond_args.f_manual) + 1)\n","\n","        desc += '-fman_lin'\n","    else:\n","        c.cond_args.f_manual = [1e-3, *base_freqs]\n","        c.cond_args.include_lin = False\n","        c.cond_args.dims = 2*len(c.cond_args.f_manual)\n","        desc += '-fman_impl'\n","\n","    if base_freqs:\n","        desc += \"_\" + \"_\".join([str(int(round(f))) for f in base_freqs])\n","\n","    return desc\n","\n","# Map noise magnitude to frame deltas\n","def days(fr_tot, d_tot):\n","    return fr_tot / d_tot # one sigma in both directions\n","def hours(fr_tot, d_tot):\n","    return days(fr_tot, d_tot) / 24\n","def weeks(fr_tot, d_tot):\n","    return days(fr_tot, d_tot) * 7\n","def months(fr_tot, d_tot):\n","    return days(fr_tot, d_tot) * (365.25/12) # avg days in month\n","def years(fr_tot, d_tot):\n","    return days(fr_tot, d_tot) * 365.25\n","\n","# Convert strings like '2.5years' to sigmas\n","def parse_noises(noises, c=None, n_frames=None, n_days=None):\n","    ret = []\n","    for n in noises:\n","        if isinstance(n, (float, int)):\n","            ret.append(n)\n","        elif 'hour' in n:\n","            ret.append(hours(n_frames, n_days)*float(n.split('hour')[0]))\n","        elif 'day' in n:\n","            ret.append(days(n_frames, n_days)*float(n.split('day')[0]))\n","        elif 'week' in n:\n","            ret.append(weeks(n_frames, n_days)*float(n.split('week')[0]))\n","        elif 'month' in n:\n","            ret.append(months(n_frames, n_days)*float(n.split('month')[0]))\n","        elif 'year' in n:\n","            ret.append(years(n_frames, n_days)*float(n.split('year')[0]))\n","        else:\n","            raise RuntimeError(f'Unkown noise scale: {n}')\n","\n","    assert len(ret) == len(noises)\n","    return ret\n","\n","def cond_desc(c, opts, cond_type, frames, f=[], noise=[], mask=None, explicit_lin=True):\n","    try:\n","\n","        # Must know number of days in sequence\n","        days = 100 if opts.days is None else opts.days\n","        assert days is not None, 'Number of days not in dataset metadata, must specify manually with --days'\n","\n","        c.cond_args = EasyDict(specs[cond_type])\n","\n","        if cond_type in ['fourier', 'f_concat']:\n","            freqs = f or list(filter(lambda f: f > 1, [days/365.25, days])) # only cylces of over 1Hz\n","            _set_freqs(c, freqs, explicit_lin)\n","\n","        if 'auto' in noise:\n","            noise_lin = [] if days < 365.25 else [f'{0.2 * days / 365.25:.2f} years'] # fifth of whole sequence length\n","            _set_noise_per_f(c, [*noise_lin, '4 days', 0], frames, days) # lin, years, days\n","        elif len(noise) == 1:\n","            _set_noise_global(c, noise, frames, days)\n","        elif isinstance(noise, Iterable) and len(noise) > 0:\n","            _set_noise_per_f(c, noise, frames, days) # lin, years, days\n","\n","        return\n","\n","    except IOError as err:\n","        raise click.ClickException(f'--data: {err}')"],"metadata":{"id":"_MkJJxU03YoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_utils import misc\n","from torch_utils import persistence\n","from torch_utils.ops import conv2d_resample\n","from torch_utils.ops import bias_act\n","from torch_utils.ops import fma\n","\n","class ConditioningTransform(torch.nn.Module):\n","    def __init__(self,\n","        cond_args = {},   # Conditioning parameters.\n","        num_ws = None,    # Number of layers to broadcast c to.\n","        add_noise = False,\n","    ):\n","        super().__init__()\n","        self.cond_args = cond_args\n","        self.explicit_lin = cond_args.get('include_lin', False)\n","        self.num_ws = num_ws\n","        self.num_f = len(self.get_frequencies())\n","        self.add_noise = add_noise # only active in D\n","\n","    def get_frequencies(self):\n","        if self.cond_args.type not in ['fourier', 'f_concat']:\n","            return []\n","\n","        freqs = self.cond_args.f_manual\n","        if self.explicit_lin:\n","            freqs = [-1.0] + freqs\n","\n","        return np.sort(freqs).astype(np.float32)\n","\n","    # Measured in mean frame intervals\n","    def add_noise_gauss(self, c, scales):\n","        assert len(scales) in [1, self.num_f]\n","        if self.training and self.add_noise:\n","            s = torch.tensor(scales, dtype=torch.float32, device=c.device)\n","            c = c + s*self.cond_args.t_delta*torch.randn_like(c) # expands last dim to len(scales)\n","        return c\n","\n","    # Measured in cycles\n","    def add_noise_f_int(self, c, noise_tuples):\n","        if self.training and self.add_noise:\n","            scales = torch.tensor([s for s,_ in noise_tuples], dtype=torch.float32, device=c.device)\n","            ifreqs = torch.tensor([1/f for _,f in noise_tuples], dtype=torch.float32, device=c.device)\n","            noises = ifreqs*torch.round(scales*torch.randn(*c.shape, len(noise_tuples), device=c.device)) # (B, 1, n_noises)\n","            c = c + noises.sum(axis=-1)\n","        return c\n","\n","    def check_shapes(self, c):\n","        assert self.cond_args.type in ['fourier', 'f_concat'] or self.cond_args.dims % 2 == 0, 'Fourier cond: number of dims not divisible by two'\n","        assert list(c.shape[1:]) in [[0], [1], [self.num_f], [self.num_ws, 1], [self.num_ws, self.num_f]], \\\n","            f'Invalid c shape - supported: [(B, 1), (B, #freq), (B, #layer, 1), (B, #layer, #freq)]' # broadcast along trailing dimension\n","\n","    def add_noises(self, c):\n","        # Global noise (separate from dequantization noise)\n","        c = self.add_noise_gauss(c, [self.cond_args.noise])\n","\n","        # Add global integer jump noise at given frequnecies (any cond mechanism)\n","        if self.cond_args.get('noise_f_int'):\n","            c = self.add_noise_f_int(c, self.cond_args.noise_f_int)\n","\n","        # Add per-freqency noise (fourier cond)\n","        # Expands and broadcasts along trailing dimension\n","        if self.cond_args.type in ['fourier', 'f_concat'] and self.cond_args.noise_f:\n","            c = self.add_noise_gauss(c, self.cond_args.noise_f)\n","\n","        return c\n","\n","    # Supports pre-broadcasted inputs:\n","    # [B, 1]: global c\n","    # [B, #layer, 1]: one c per layer\n","    # [B, #freq]: one c per frequency\n","    # [B, #layer, #freq]: one c per frequency per layer\n","    def forward(self, c, broadcast=True):\n","        self.check_shapes(c)\n","        self.add_noises(c)\n","\n","        if self.cond_args.type in ['fourier', 'f_concat']:\n","            # Interleave cosines and sines\n","            freqs = torch.from_numpy(self.get_frequencies()).to(c.device)\n","            cos = torch.cos(2*np.pi*freqs*c) # [B, ca.dims/2]\n","            sin = torch.sin(2*np.pi*freqs*c) # [B, ca.dims/2]\n","\n","            if self.explicit_lin:\n","                if c.ndim == 2:\n","                    cos[:, 0] = 1\n","                    sin[:, 0] = self.cond_args.lin_lr * c[:, 0]\n","                else:\n","                    cos[:, :, 0] = 1\n","                    sin[:, :, 0] = self.cond_args.lin_lr * c[:, :, 0]\n","\n","            # Interleaved: [cos0, sin0, cos1, sin1, ...]\n","            c = torch.stack((cos, sin), dim=-1).view(*c.shape[:-1], -1)\n","        else:\n","            pass # c passed through unchanged\n","\n","        if broadcast:\n","            assert self.num_ws is not None, 'num_ws not provided for broadcast'\n","            if c.ndim == 2: # not already broadcasted (due to per-layer input)\n","                c = c.unsqueeze(1).repeat_interleave(self.num_ws, dim=1)\n","            misc.assert_shape(c, [None, self.num_ws, self.cond_args.dims])\n","\n","        return c"],"metadata":{"id":"Ipgu1eiZ3Zkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modulated_conv2d(\n","    x,                          # Input tensor of shape [batch_size, in_channels, in_height, in_width].\n","    weight,                     # Weight tensor of shape [out_channels, in_channels, kernel_height, kernel_width].\n","    styles,                     # Modulation coefficients of shape [batch_size, in_channels].\n","    noise           = None,     # Optional noise tensor to add to the output activations.\n","    up              = 1,        # Integer upsampling factor.\n","    down            = 1,        # Integer downsampling factor.\n","    padding         = 0,        # Padding with respect to the upsampled image.\n","    resample_filter = None,     # Low-pass filter to apply when resampling activations. Must be prepared beforehand by calling upfirdn2d.setup_filter().\n","    demodulate      = True,     # Apply weight demodulation?\n","    flip_weight     = True,     # False = convolution, True = correlation (matches torch.nn.functional.conv2d).\n","    fused_modconv   = True,     # Perform modulation, convolution, and demodulation as a single fused operation?\n","):\n","    batch_size = x.shape[0]\n","    out_channels, in_channels, kh, kw = weight.shape\n","    misc.assert_shape(weight, [out_channels, in_channels, kh, kw]) # [OIkk]\n","    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n","    misc.assert_shape(styles, [batch_size, in_channels]) # [NI]\n","\n","    # Pre-normalize inputs to avoid FP16 overflow.\n","    if x.dtype == torch.float16 and demodulate:\n","        weight = weight * (1 / np.sqrt(in_channels * kh * kw) / weight.norm(float('inf'), dim=[1,2,3], keepdim=True)) # max_Ikk\n","        styles = styles / styles.norm(float('inf'), dim=1, keepdim=True) # max_I\n","\n","    # Calculate per-sample weights and demodulation coefficients.\n","    w = None\n","    dcoefs = None\n","    if demodulate or fused_modconv:\n","        w = weight.unsqueeze(0) # [NOIkk]\n","        w = w * styles.reshape(batch_size, 1, -1, 1, 1) # [NOIkk]\n","    if demodulate:\n","        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n","    if demodulate and fused_modconv:\n","        w = w * dcoefs.reshape(batch_size, -1, 1, 1, 1) # [NOIkk]\n","\n","    # Execute by scaling the activations before and after the convolution.\n","    if not fused_modconv:\n","        x = x * styles.to(x.dtype).reshape(batch_size, -1, 1, 1)\n","        x = conv2d_resample.conv2d_resample(x=x, w=weight.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, flip_weight=flip_weight)\n","        if demodulate and noise is not None:\n","            x = fma.fma(x, dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1), noise.to(x.dtype))\n","        elif demodulate:\n","            x = x * dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1)\n","        elif noise is not None:\n","            x = x.add_(noise.to(x.dtype))\n","        return x\n","\n","    # Execute as one fused op using grouped convolution.\n","    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n","        batch_size = int(batch_size)\n","    misc.assert_shape(x, [batch_size, in_channels, None, None])\n","    x = x.reshape(1, -1, *x.shape[2:])\n","    w = w.reshape(-1, in_channels, kh, kw)\n","    x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, groups=batch_size, flip_weight=flip_weight)\n","    x = x.reshape(batch_size, -1, *x.shape[2:])\n","    if noise is not None:\n","        x = x.add_(noise)\n","    return x\n","\n","\n","class FullyConnectedLayer(torch.nn.Module):\n","    def __init__(self,\n","        in_features,                # Number of input features.\n","        out_features,               # Number of output features.\n","        bias            = True,     # Apply additive bias before the activation function?\n","        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n","        lr_multiplier   = 1,        # Learning rate multiplier.\n","        bias_init       = 0,        # Initial value for the additive bias.\n","    ):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.activation = activation\n","        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n","        self.bias = torch.nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n","        lr_scales = lr_multiplier / np.sqrt(in_features) # scalar or vector\n","        self.weight_gain = torch.nn.Parameter(lr_scales) if torch.is_tensor(lr_scales) else lr_scales\n","        self.bias_gain = lr_multiplier\n","\n","    def forward(self, x):\n","        w = self.weight.to(x.dtype) * self.weight_gain\n","        b = self.bias\n","        if b is not None:\n","            b = b.to(x.dtype)\n","            if self.bias_gain != 1:\n","                b = b * self.bias_gain\n","\n","        if self.activation == 'linear' and b is not None:\n","            x = torch.addmm(b.unsqueeze(0), x, w.t())\n","        else:\n","            x = x.matmul(w.t())\n","            x = bias_act.bias_act(x, b, act=self.activation)\n","        return x\n","\n","    def extra_repr(self):\n","        return f'in_features={self.in_features:d}, out_features={self.out_features:d}, activation={self.activation:s}'\n","\n","\n","def normalize_2nd_moment(x, dim=1, eps=1e-8):\n","    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()\n","\n","\n","class MappingNetwork(torch.nn.Module):\n","    def __init__(self,\n","        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n","        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n","        w_dim,                      # Intermediate latent (W) dimensionality.\n","        num_ws,                     # Number of intermediate latents to output, None = do not broadcast.\n","        num_layers      = 8,        # Number of mapping layers.\n","        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n","        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n","        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n","        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n","        w_avg_beta      = 0.998,    # Decay for tracking the moving average of W during training, None = do not track.\n","    ):\n","        super().__init__()\n","        self.z_dim = z_dim\n","        self.c_dim = c_dim\n","        self.w_dim = w_dim\n","        self.num_ws = num_ws\n","        self.num_layers = num_layers\n","        self.w_avg_beta = w_avg_beta\n","\n","        if embed_features is None:\n","            embed_features = w_dim\n","        if c_dim == 0:\n","            embed_features = 0\n","        if layer_features is None:\n","            layer_features = w_dim\n","        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n","\n","        if c_dim > 0:\n","            self.embed = FullyConnectedLayer(c_dim, embed_features)\n","        for idx in range(num_layers):\n","            in_features = features_list[idx]\n","            out_features = features_list[idx + 1]\n","            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n","            setattr(self, f'fc{idx}', layer)\n","\n","        if num_ws is not None and w_avg_beta is not None:\n","            self.register_buffer('w_avg', torch.zeros([w_dim]))\n","\n","    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","        # Embed, normalize, and concat inputs.\n","        x = None\n","        with torch.autograd.profiler.record_function('input'):\n","            if self.z_dim > 0:\n","                misc.assert_shape(z, [None, self.z_dim])\n","                x = normalize_2nd_moment(z.to(torch.float32))\n","            if self.c_dim > 0:\n","                misc.assert_shape(c, [None, self.c_dim])\n","                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n","                x = torch.cat([x, y], dim=1) if x is not None else y\n","\n","        # Main layers.\n","        for idx in range(self.num_layers):\n","            layer = getattr(self, f'fc{idx}')\n","            x = layer(x)\n","\n","        # Update moving average of W.\n","        if update_emas and self.w_avg_beta is not None:\n","            with torch.autograd.profiler.record_function('update_w_avg'):\n","                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n","\n","        # Broadcast.\n","        if self.num_ws is not None:\n","            with torch.autograd.profiler.record_function('broadcast'):\n","                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n","\n","        # Apply truncation.\n","        if truncation_psi != 1:\n","            with torch.autograd.profiler.record_function('truncate'):\n","                assert self.w_avg_beta is not None\n","                if self.num_ws is None or truncation_cutoff is None:\n","                    x = self.w_avg.lerp(x, truncation_psi)\n","                else:\n","                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n","        return x\n","\n","    def extra_repr(self):\n","        return f'z_dim={self.z_dim:d}, c_dim={self.c_dim:d}, w_dim={self.w_dim:d}, num_ws={self.num_ws:d}'\n","\n","class CondStyle(torch.nn.Module):\n","    def __init__(self, channels, w_dim, c_dim, cond_args):\n","        super().__init__()\n","        self.c_dim = c_dim\n","        self.cond_args = cond_args\n","        self.w_affine = FullyConnectedLayer(w_dim, channels, bias_init=1)\n","        if cond_args.type == 'fourier':\n","            self.c_to_scales = FullyConnectedLayer(c_dim, channels, bias=False, lr_multiplier=cond_args.lr)\n","            self.c_to_scales.weight.data *= 1e-6\n","            self.c_to_scales.weight.data[:, 0] += 1 # init DC to ~1 (w passed through initially)\n","\n","    def forward(self, w, c):\n","        styles = self.w_affine(w)\n","\n","        if self.cond_args.type == 'fourier':\n","            scales = self.c_to_scales(c)\n","            styles = styles * scales\n","\n","        return styles"],"metadata":{"id":"OL35ZtUo3eaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tlgan method\n","class CycleTStyleGenerator(nn.Module):\n","    \"\"\"Architecture of the generator network.\"\"\"\n","\n","    def __init__(self,         z_dim=64,                      # Input latent (Z) dimensionality.\n","        c_dim=4,                      # Conditioning label (C) dimensionality.\n","        w_dim=64,                      # Intermediate latent (W) dimensionality.\n","        num_ws=1,\n","        cond_args           = {},   # Conditioning arguments for all subnetworks.\n","        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n","        ):\n","        super().__init__()\n","\n","        ###########################################\n","        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","        ###########################################\n","\n","        # 1. Define the encoder part of the generator\n","        self.conv1 = conv(3,32,4,2,1,'instance','relu')\n","        self.conv2 = conv(32,64,4,2,1,'instance','relu')\n","\n","        # 2. Define the transformation part of the generator\n","        self.resnet_block1 = ResnetBlock(64,'instance','relu')\n","        self.resnet_block2 = ResnetBlock(64, 'instance', 'relu')\n","        self.resnet_block3 = ResnetBlock(64, 'instance', 'relu')\n","\n","        # 3. Define the decoder part of the generator\n","        self.up_conv1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","        self.up_conv2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","        self.cond_args = cond_args\n","\n","        self.cond_xform = ConditioningTransform(cond_args=self.cond_args, num_ws=num_ws)\n","        # c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        # self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dims_M, w_dim=w_dim, num_ws=num_ws, num_layers=1, **mapping_kwargs)\n","\n","        # self.style_encoder = Encoder(input_channels=3, output_channels=w_dim, num_layers=3)\n","\n","        in_channels=64\n","        out_channels = 64\n","        kernel_size =3\n","\n","        # self.scaled_styles = CondStyle(in_channels, w_dim, c_dim, cond_args)\n","        self.scaled_styles = CondScale(in_channels, w_dim, c_dim, cond_args)\n","        # self.scaled_styles = CondScale(8, c_dim)\n","\n","        memory_format = torch.contiguous_format\n","        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n","\n","    def forward(self, x, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","        \"\"\"\n","        Generate an image conditioned on an input image.\n","\n","        Input\n","        -----\n","            x: BS x 3 x 32 x 32\n","\n","        Output\n","        ------\n","            out: BS x 3 x 32 x 32\n","        \"\"\"\n","        ###########################################\n","        ##   FILL THIS IN: FORWARD PASS   ##\n","        ###########################################\n","\n","        # pass\n","\n","        # style = self.style_encoder(x)\n","        # style = style.view(style.size(0), -1)\n","        cs = self.cond_xform(c, broadcast=True)\n","        c_iter = iter(cs.unbind(dim=1))\n","\n","        x=self.conv1(x)\n","        x=self.conv2(x)\n","\n","        # c_cur=next(c_iter)\n","        # c_cur=c_cur.narrow(1, 0, 1)\n","        # c_cur=self.scaled_styles(c_cur)\n","\n","        # style = self.mapping(x, cs[:,0,:])\n","        # self.resnet_block1.conv_layer[0].weight.data=self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * next(c_iter)\n","\n","        styles = self.scaled_styles(c=next(c_iter))\n","\n","        y = modulated_conv2d(x=x, weight=self.weight, styles=styles, padding=1, demodulate=True, flip_weight=False, fused_modconv=False)\n","        x=x+y\n","\n","        y = modulated_conv2d(x=x, weight=self.weight, styles=styles, padding=1, demodulate=True, flip_weight=False, fused_modconv=False)\n","        x=x+y\n","\n","        y = modulated_conv2d(x=x, weight=self.weight, styles=styles, padding=1, demodulate=True, flip_weight=False, fused_modconv=False)\n","        x=x+y\n","\n","        x=self.up_conv1(x)\n","        x=self.up_conv2(x)\n","        return x"],"metadata":{"id":"oYAUBJ1v3io0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# my method\n","# class Encoder(nn.Module):\n","#     \"\"\"Encoder network to map input image to style vector.\"\"\"\n","\n","#     def __init__(self, input_channels=3, output_channels=64, num_layers=3):\n","#         super().__init__()\n","\n","#         layers = []\n","#         in_channels = input_channels\n","#         for i in range(num_layers):\n","#             out_channels = output_channels * 2 ** i\n","#             layers.append(conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, norm='instance', activ='relu'))\n","#             in_channels = out_channels\n","\n","#         layers.append(conv(in_channels, output_channels, kernel_size=4, stride=1, padding=1, norm='instance', activ='relu'))\n","#         self.layers = nn.Sequential(*layers)\n","\n","#     def forward(self, x):\n","#         return self.layers(x)\n","\n","\n","class MyFullyConnectedLayer(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, bias=True, lr_multiplier=1):\n","        super().__init__()\n","        self.linear = torch.nn.Linear(in_channels, out_channels, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x=F.relu(x)\n","        return x\n","\n","\n","class MappingNetworkCond(torch.nn.Module):\n","    def __init__(self,\n","        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n","        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n","        w_dim= None,                      # Intermediate latent (W) dimensionality.\n","        num_ws = 1,                     # Number of intermediate latents to output, None = do not broadcast.\n","        num_layers      = 1,        # Number of mapping layers.\n","        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n","        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n","        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n","    ):\n","        super().__init__()\n","        self.z_dim = z_dim\n","        self.c_dim = c_dim\n","        self.num_layers = num_layers\n","\n","        embed_features=w_dim\n","        if c_dim == 0:\n","            embed_features = 0\n","        if layer_features is None:\n","            layer_features = z_dim\n","        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n","\n","        if c_dim > 0:\n","            self.embed = FullyConnectedLayer(c_dim, embed_features)\n","        for idx in range(num_layers):\n","            in_features = features_list[idx]\n","            out_features = features_list[idx + 1]\n","            layer = FullyConnectedLayer(in_features, out_features)\n","            setattr(self, f'fc{idx}', layer)\n","\n","    def forward(self, x, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","        # Embed, normalize, and concat inputs.\n","        with torch.autograd.profiler.record_function('input'):\n","            if self.z_dim>0:\n","              x = normalize_2nd_moment(z.to(torch.float32))\n","            if self.c_dim>0:\n","              c = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n","            if self.c_dim > 0:\n","              if x is not None and x.ndim > 2:\n","                  misc.assert_shape(c, [None, self.c_dim])\n","                  y = c.to(torch.float32).unsqueeze(2).unsqueeze(3).expand(-1, -1, x.shape[2], x.shape[3])\n","                  x = torch.cat([x, y], dim=1) if x is not None else y\n","              else:\n","                misc.assert_shape(c, [None, self.c_dim])\n","                y = self.embed(c.to(torch.float32))\n","                x = torch.cat([x, y], dim=1) if x is not None else y\n","\n","        return x\n","\n","class CondScale(torch.nn.Module):\n","    def __init__(self, channels, w_dim, c_dim, cond_args):\n","        super().__init__()\n","        self.c_dim = c_dim\n","        self.cond_args = cond_args\n","        if w_dim is not None:\n","            self.w_affine = FullyConnectedLayer(w_dim, channels, bias_init=1)\n","        if cond_args.type == 'fourier':\n","            self.c_to_scales = FullyConnectedLayer(c_dim, channels, bias=False, lr_multiplier=cond_args.lr)\n","            self.c_to_scales.weight.data *= 1e-6\n","            self.c_to_scales.weight.data[:, 0] += 1 # init DC to ~1 (w passed through initially)\n","\n","    def forward(self, w=None, c=None):\n","        if w is not None:\n","          styles = self.w_affine(w)\n","        else:\n","          styles=None\n","\n","        if self.cond_args.type == 'fourier':\n","            scales = self.c_to_scales(c)\n","            if styles is not None:\n","              styles = styles * scales\n","            else: styles=scales\n","\n","        return styles\n","\n","\n","class MyCondScale(torch.nn.Module):\n","    def __init__(self, channels, c_dim, cond_args):\n","        super().__init__()\n","        self.c_dim = c_dim\n","        self.cond_args = cond_args\n","        if cond_args.type == 'fourier':\n","            self.c_to_scales = MyFullyConnectedLayer(c_dim, channels)\n","            self.c_to_scales.linear.weight.data *= 1e-6\n","            self.c_to_scales.linear.weight.data[:, 0] += 1 # init DC to ~1 (w passed through initially)\n","\n","    def forward(self, c):\n","\n","        if self.cond_args.type == 'fourier':\n","            scales = self.c_to_scales(c)\n","\n","        return scales\n","\n","\n","class AdaIn(nn.Module):\n","    def __init__(self, in_channels, style_dim):\n","        super().__init__()\n","        self.norm = nn.InstanceNorm2d(in_channels, affine=False)\n","        self.style_scale_transform = nn.Linear(style_dim, in_channels)\n","        self.style_shift_transform = nn.Linear(style_dim, in_channels)\n","\n","    def forward(self, x, style):\n","        normalized = self.norm(x)\n","        style_scale = self.style_scale_transform(style).unsqueeze(2).unsqueeze(3)\n","        style_shift = self.style_shift_transform(style).unsqueeze(2).unsqueeze(3)\n","        transformed = style_scale * normalized + style_shift\n","        return transformed"],"metadata":{"id":"VS4cz9wE3pDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class CycleTStyleGenerator(nn.Module):\n","#     \"\"\"Architecture of the generator network.\"\"\"\n","\n","#     def __init__(self,         z_dim=64,                      # Input latent (Z) dimensionality.\n","#         c_dim=4,                      # Conditioning label (C) dimensionality.\n","#         w_dim=64,                      # Intermediate latent (W) dimensionality.\n","#         num_ws=1,\n","#         cond_args           = {},   # Conditioning arguments for all subnetworks.\n","#         mapping_kwargs      = {},   # Arguments for MappingNetwork.\n","#         ):\n","#         super().__init__()\n","\n","#         ###########################################\n","#         ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","#         ###########################################\n","\n","#         # 1. Define the encoder part of the generator\n","#         self.conv1 = conv(3,32,4,2,1,'instance','relu')\n","#         self.conv2 = conv(32,64,4,2,1,'instance','relu')\n","\n","#         # 2. Define the transformation part of the generator\n","#         self.resnet_block1 = ResnetBlock(64,'instance','relu', groups=batch_size)\n","#         self.resnet_block2 = ResnetBlock(64, 'instance', 'relu', groups=batch_size)\n","#         self.resnet_block3 = ResnetBlock(64, 'instance', 'relu', groups=batch_size)\n","\n","#         # 3. Define the decoder part of the generator\n","#         self.up_conv1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","#         self.up_conv2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","#         self.cond_args = cond_args\n","\n","#         self.cond_xform = ConditioningTransform(cond_args=self.cond_args, num_ws=num_ws)\n","#         # c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","#         # self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dims_M, **mapping_kwargs)\n","#         self.style_encoder = Encoder(input_channels=3, output_channels=w_dim, num_layers=3)\n","\n","#         # in_channels=3\n","#         # self.scaled_styles = CondStyle(in_channels, w_dim, c_dim, cond_args)\n","#         self.scaled_styles = CondScale(8, c_dim, cond_args)\n","\n","#         self.adain = AdaIn(w_dim, c_dim)\n","\n","#     def forward(self, x, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","#         \"\"\"\n","#         Generate an image conditioned on an input image.\n","\n","#         Input\n","#         -----\n","#             x: BS x 3 x 32 x 32\n","\n","#         Output\n","#         ------\n","#             out: BS x 3 x 32 x 32\n","#         \"\"\"\n","#         ###########################################\n","#         ##   FILL THIS IN: FORWARD PASS   ##\n","#         ###########################################\n","\n","#         # pass\n","\n","#         # style = self.style_encoder(x)\n","#         # style = style.view(style.size(0), -1)\n","#         cs = self.cond_xform(c, broadcast=True)\n","#         c_iter = iter(cs.unbind(dim=1))\n","\n","#         # style = self.mapping(x, cs[:,0,:], truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n","#         # style = self.scaled_styles(style,cs)\n","\n","#         x=self.conv1(x)\n","#         x=self.conv2(x)\n","\n","#         c_cur=next(c_iter)\n","#         # c_cur=self.scaled_styles(c_cur)\n","\n","#         # self.resnet_block1.conv_layer[0].weight.data=(self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * c_cur).reshape(-1, 64,3,3)\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block1(x)\n","\n","#         # self.resnet_block2.conv_layer[0].weight.data=torch.sum(self.resnet_block2.conv_layer[0].weight.data * c_cur, dim=[0])\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block2(x)\n","\n","#         # self.resnet_block3.conv_layer[0].weight.data=torch.sum(self.resnet_block3.conv_layer[0].weight.data * c_cur, dim=[0])\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block3(x)\n","\n","#         x = self.adain(x, c_cur)\n","\n","#         x=self.up_conv1(x)\n","#         x=self.up_conv2(x)\n","#         return x\n","\n","\n","class MyCycleTStyleGenerator(nn.Module):\n","    \"\"\"Architecture of the generator network.\"\"\"\n","\n","    def __init__(self,         z_dim=64,                      # Input latent (Z) dimensionality.\n","        c_dim=4,                      # Conditioning label (C) dimensionality.\n","        w_dim=64,                      # Intermediate latent (W) dimensionality.\n","        num_ws=1,\n","        cond_args           = {},   # Conditioning arguments for all subnetworks.\n","        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n","        ):\n","        super().__init__()\n","\n","        ###########################################\n","        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","        ###########################################\n","\n","        # 1. Define the encoder part of the generator\n","        self.conv1 = conv(3,32,4,2,1,'instance','relu')\n","        self.conv2 = conv(32,64,4,2,1,'instance','relu')\n","\n","        # 2. Define the transformation part of the generator\n","        self.resnet_block1 = ResnetBlock(64,'instance','relu')\n","        self.resnet_block2 = ResnetBlock(64, 'instance', 'relu')\n","        self.resnet_block3 = ResnetBlock(64, 'instance', 'relu')\n","\n","        # 3. Define the decoder part of the generator\n","        self.up_conv1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","        self.up_conv2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","        self.cond_args = cond_args\n","\n","        self.cond_xform = ConditioningTransform(cond_args=self.cond_args, num_ws=num_ws)\n","        # if self.c_dim > 0:\n","        #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        #   self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dims_M, **mapping_kwargs)\n","        # self.style_encoder = Encoder(input_channels=3, output_channels=w_dim, num_layers=3)\n","\n","        # in_channels=3\n","        # self.scaled_styles = CondStyle(in_channels, w_dim, c_dim, cond_args)\n","        # self.scaled_styles = CondScale(8, w_dim, c_dim, cond_args)\n","        self.scaled_styles = MyCondScale(8, c_dim, cond_args)\n","\n","        self.adain = AdaIn(w_dim, c_dim)\n","\n","    def forward(self, x, c=None, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","        \"\"\"\n","        Generate an image conditioned on an input image.\n","\n","        Input\n","        -----\n","            x: BS x 3 x 32 x 32\n","\n","        Output\n","        ------\n","            out: BS x 3 x 32 x 32\n","        \"\"\"\n","        ###########################################\n","        ##   FILL THIS IN: FORWARD PASS   ##\n","        ###########################################\n","\n","        # pass\n","\n","        # style = self.style_encoder(x)\n","        # style = style.view(style.size(0), -1)\n","\n","\n","        cs = self.cond_xform(c, broadcast=True)\n","        c_iter = iter(cs.unbind(dim=1))\n","        # x = self.mapping(x, cs[:,0,:], truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n","\n","        # style = self.scaled_styles(x,cs)\n","\n","        x=self.conv1(x)\n","        x=self.conv2(x)\n","\n","        c_cur=next(c_iter)\n","        # c_cur=self.scaled_styles(c_cur)\n","\n","        # self.resnet_block1.conv_layer[0].weight.data=(self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * c_cur).reshape(-1, 64,3,3)\n","        x = self.adain(x, c_cur)\n","        x=self.resnet_block1(x)\n","\n","        # self.resnet_block2.conv_layer[0].weight.data=torch.sum(self.resnet_block2.conv_layer[0].weight.data * c_cur, dim=[0])\n","        x = self.adain(x, c_cur)\n","        x=self.resnet_block2(x)\n","\n","        # self.resnet_block3.conv_layer[0].weight.data=torch.sum(self.resnet_block3.conv_layer[0].weight.data * c_cur, dim=[0])\n","        x = self.adain(x, c_cur)\n","        x=self.resnet_block3(x)\n","\n","        x = self.adain(x, c_cur)\n","\n","        x=self.up_conv1(x)\n","        x=self.up_conv2(x)\n","        return x"],"metadata":{"id":"XPxCiSnN3sgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class CycleTStyleGenerator(nn.Module):\n","#     \"\"\"Architecture of the generator network.\"\"\"\n","\n","#     def __init__(self,         z_dim=64,                      # Input latent (Z) dimensionality.\n","#         c_dim=4,                      # Conditioning label (C) dimensionality.\n","#         w_dim=64,                      # Intermediate latent (W) dimensionality.\n","#         num_ws=1,\n","#         cond_args           = {},   # Conditioning arguments for all subnetworks.\n","#         mapping_kwargs      = {},   # Arguments for MappingNetwork.\n","#         ):\n","#         super().__init__()\n","\n","#         ###########################################\n","#         ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","#         ###########################################\n","\n","#         # 1. Define the encoder part of the generator\n","#         self.conv1 = conv(3,32,4,2,1,'instance','relu')\n","#         self.conv2 = conv(32,64,4,2,1,'instance','relu')\n","\n","#         # 2. Define the transformation part of the generator\n","#         self.resnet_block1 = ResnetBlock(64,'instance','relu', groups=batch_size)\n","#         self.resnet_block2 = ResnetBlock(64, 'instance', 'relu', groups=batch_size)\n","#         self.resnet_block3 = ResnetBlock(64, 'instance', 'relu', groups=batch_size)\n","\n","#         # 3. Define the decoder part of the generator\n","#         self.up_conv1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","#         self.up_conv2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","#         self.cond_args = cond_args\n","\n","#         self.cond_xform = ConditioningTransform(cond_args=self.cond_args, num_ws=num_ws)\n","#         # c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","#         # self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dims_M, **mapping_kwargs)\n","#         self.style_encoder = Encoder(input_channels=3, output_channels=w_dim, num_layers=3)\n","\n","#         # in_channels=3\n","#         # self.scaled_styles = CondStyle(in_channels, w_dim, c_dim, cond_args)\n","#         self.scaled_styles = CondScale(8, c_dim, cond_args)\n","\n","#         self.adain = AdaIn(w_dim, c_dim)\n","\n","#     def forward(self, x, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","#         \"\"\"\n","#         Generate an image conditioned on an input image.\n","\n","#         Input\n","#         -----\n","#             x: BS x 3 x 32 x 32\n","\n","#         Output\n","#         ------\n","#             out: BS x 3 x 32 x 32\n","#         \"\"\"\n","#         ###########################################\n","#         ##   FILL THIS IN: FORWARD PASS   ##\n","#         ###########################################\n","\n","#         # pass\n","\n","#         # style = self.style_encoder(x)\n","#         # style = style.view(style.size(0), -1)\n","#         cs = self.cond_xform(c, broadcast=True)\n","#         c_iter = iter(cs.unbind(dim=1))\n","\n","#         # style = self.mapping(x, cs[:,0,:], truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n","#         # style = self.scaled_styles(style,cs)\n","\n","#         x=self.conv1(x)\n","#         x=self.conv2(x)\n","\n","#         c_cur=next(c_iter)\n","#         # c_cur=self.scaled_styles(c_cur)\n","\n","#         # self.resnet_block1.conv_layer[0].weight.data=(self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * c_cur).reshape(-1, 64,3,3)\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block1(x)\n","\n","#         # self.resnet_block2.conv_layer[0].weight.data=torch.sum(self.resnet_block2.conv_layer[0].weight.data * c_cur, dim=[0])\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block2(x)\n","\n","#         # self.resnet_block3.conv_layer[0].weight.data=torch.sum(self.resnet_block3.conv_layer[0].weight.data * c_cur, dim=[0])\n","#         x = self.adain(x, c_cur)\n","#         x=self.resnet_block3(x)\n","\n","#         x = self.adain(x, c_cur)\n","\n","#         x=self.up_conv1(x)\n","#         x=self.up_conv2(x)\n","#         return x\n","\n","\n","class MyCycleTVStyleGenerator(nn.Module):\n","    \"\"\"Architecture of the generator network.\"\"\"\n","\n","    def __init__(self,         z_dim=64,                      # Input latent (Z) dimensionality.\n","        c_dim=4,                      # Conditioning label (C) dimensionality.\n","        w_dim=64,                      # Intermediate latent (W) dimensionality.\n","        num_ws=1,\n","        cond_args           = {},   # Conditioning arguments for all subnetworks.\n","        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n","        ):\n","        super().__init__()\n","\n","        ###########################################\n","        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n","        ###########################################\n","\n","        # 1. Define the encoder part of the generator\n","        self.conv1_1 = conv(3,32,4,2,1,'instance','relu')\n","        self.conv2_1 = conv(32,64,4,2,1,'instance','relu')\n","\n","        self.conv1_2 = conv(3,32,4,2,1,'instance','relu')\n","        self.conv2_2 = conv(32,64,4,2,1,'instance','relu')\n","\n","        # 2. Define the transformation part of the generator\n","        self.resnet_block1 = ResnetBlock(64,'instance','relu')\n","        self.resnet_block2 = ResnetBlock(64, 'instance', 'relu')\n","        self.resnet_block3 = ResnetBlock(64, 'instance', 'relu')\n","\n","        # 3. Define the decoder part of the generator\n","        self.up_conv1_1 = up_conv(64,32,3,1,1,2,'instance','relu')\n","        self.up_conv2_1 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","        self.up_conv1_2 = up_conv(64,32,3,1,1,2,'instance','relu')\n","        self.up_conv2_2 = up_conv(32,3,3,1,1,2,None,'tanh')\n","\n","        self.conv_layer = conv(\n","            in_channels=conv_dim, out_channels=conv_dim,\n","            kernel_size=3, stride=1, padding=1, norm=norm,\n","            activ=activ, groups=groups\n","        )\n","\n","        self.cond_args = cond_args\n","\n","        self.cond_xform = ConditioningTransform(cond_args=self.cond_args, num_ws=num_ws)\n","        # if self.c_dim > 0:\n","        #   c_dims_M = {'concat': c_dim, 'f_concat': cond_args.dims}.get(cond_args.type, 0)\n","        #   self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dims_M, **mapping_kwargs)\n","        # self.style_encoder = Encoder(input_channels=3, output_channels=w_dim, num_layers=3)\n","\n","        # in_channels=3\n","        # self.scaled_styles = CondStyle(in_channels, w_dim, c_dim, cond_args)\n","        # self.scaled_styles = CondScale(8, w_dim, c_dim, cond_args)\n","        self.scaled_styles = MyCondScale(8, c_dim, cond_args)\n","\n","        self.adain = AdaIn(w_dim, c_dim)\n","\n","        self.avgpool=nn.AvgPool2d((90,90))\n","        self.conv3=conv(64*2, 64*90*90,1,1,0)\n","\n","    def forward(self, x, c=None, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n","        \"\"\"\n","        Generate an image conditioned on an input image.\n","\n","        Input\n","        -----\n","            x: BS x 3 x 32 x 32\n","\n","        Output\n","        ------\n","            out: BS x 3 x 32 x 32\n","        \"\"\"\n","        ###########################################\n","        ##   FILL THIS IN: FORWARD PASS   ##\n","        ###########################################\n","\n","        x1=x[::2]\n","        c1=c[::2]\n","        x2=x[1::2]\n","        c2=c[1::2]\n","\n","        # style = self.style_encoder(x)\n","        # style = style.view(style.size(0), -1)\n","\n","\n","        cs1 = self.cond_xform(c1, broadcast=True)\n","        c_iter1 = iter(cs1.unbind(dim=1))\n","        # x = self.mapping(x, cs[:,0,:], truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n","\n","        # style = self.scaled_styles(x,cs)\n","\n","        x1=self.conv1_1(x1)\n","        x1=self.conv2_1(x1)\n","\n","        c_cur1=next(c_iter1)\n","        # c_cur=self.scaled_styles(c_cur)\n","\n","        # self.resnet_block1.conv_layer[0].weight.data=(self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * c_cur).reshape(-1, 64,3,3)\n","        tmp = self.adain(x1, c_cur1)\n","        x1=x1+self.conv_layer(tmp)\n","\n","        # self.resnet_block2.conv_layer[0].weight.data=torch.sum(self.resnet_block2.conv_layer[0].weight.data * c_cur, dim=[0])\n","        tmp = self.adain(x1, c_cur1)\n","        x1=x1+self.conv_layer(tmp)\n","\n","        # self.resnet_block3.conv_layer[0].weight.data=torch.sum(self.resnet_block3.conv_layer[0].weight.data * c_cur, dim=[0])\n","        tmp = self.adain(x1, c_cur1)\n","        x1=x1+self.conv_layer(tmp)\n","\n","#         x1 = self.adain(x1, c_cur1)\n","        x1_avg=self.avgpool(x1)\n","\n","        cs2 = self.cond_xform(c2, broadcast=True)\n","        c_iter2 = iter(cs2.unbind(dim=1))\n","        # x = self.mapping(x, cs[:,0,:], truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n","\n","        # style = self.scaled_styles(x,cs)\n","\n","        x2=self.conv1_2(x2)\n","        x2=self.conv2_2(x2)\n","\n","        c_cur2=next(c_iter2)\n","        # c_cur=self.scaled_styles(c_cur)\n","\n","        # self.resnet_block1.conv_layer[0].weight.data=(self.resnet_block1.conv_layer[0].weight.data.unsqueeze(0) * c_cur).reshape(-1, 64,3,3)\n","        tmp = self.adain(x2, c_cur2)\n","        x2=self.conv_layer(tmp)+x2\n","\n","        # self.resnet_block2.conv_layer[0].weight.data=torch.sum(self.resnet_block2.conv_layer[0].weight.data * c_cur, dim=[0])\n","        tmp = self.adain(x2, c_cur2)\n","        x2=self.conv_layer(tmp)+x2\n","\n","        # self.resnet_block3.conv_layer[0].weight.data=torch.sum(self.resnet_block3.conv_layer[0].weight.data * c_cur, dim=[0])\n","        tmp = self.adain(x2, c_cur2)\n","        x2=self.conv_layer(tmp)+x2\n","\n","#         x2 = self.adain(x2, c_cur2)\n","        x2_avg=self.avgpool(x2)\n","\n","        x=torch.cat((x1_avg,x2_avg),1)\n","        x=self.conv3(x)\n","        x=x.reshape(x.shape[0],64,90,90)\n","\n","#         x1=torch.cat((x,x1),1)\n","        x1=self.up_conv1_1(x)\n","        x1=self.up_conv2_1(x1)\n","\n","#         x2=torch.cat((x,x2),1)\n","        x2=self.up_conv1_2(x)\n","        x2=self.up_conv2_2(x2)\n","        out=torch.cat((x1,x2),0)\n","\n","        idx = np.zeros(out.shape[0],dtype=np.int32)\n","        idx[::2] = [i for i in range(x1.shape[0])]\n","        idx[1::2] = [i for i in range(x1.shape[0], out.shape[0])]\n","        out=torch.index_select(out,0,torch.tensor(idx,device=out.device))\n","\n","        return out"],"metadata":{"id":"K9L6fbaRHqeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import click"],"metadata":{"id":"-OMXxxlb31QB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dequant = lambda c: c"],"metadata":{"id":"4YV2rpvz353Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion_cls =nn.MSELoss().to('cuda')"],"metadata":{"id":"dNZb_q4AYRUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def val_loop(dataloader_X, dataloader_Y, X_normalizer, Y_normalizer, iteration, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y, opts):\n","    \"\"\"Runs the training loop.\n","        * Saves checkpoint every opts.checkpoint_every iterations\n","        * Saves generated samples every opts.sample_every iterations\n","    \"\"\"\n","    # # Create generators and discriminators\n","    G_XtoY.eval(), G_YtoX.eval(), D_X.eval(), D_Y.eval(), Cls_X.eval(), Cls_Y.eval()\n","    cx = EasyDict() # Main config dict.\n","    cond_desc(cx, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_X))\n","    cy = EasyDict() # Main config dict.\n","    cond_desc(cy, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_Y))\n","\n","    iter_X = iter(dataloader_X)\n","    iter_Y = iter(dataloader_Y)\n","\n","    iter_per_epoch = min(len(iter_X), len(iter_Y))\n","    d_real_loss_hist = D_Y_loss_hist = D_X_loss_hist = d_fake_loss_hist = g_loss_hist=0\n","    Cls_Y_loss_hist = Cls_X_loss_hist=Cls_Y2X_loss_hist = Cls_X2Y_loss_hist=0\n","\n","    with torch.no_grad():\n","        for itr in range(1, iter_per_epoch):\n","            # Reset data_iter for each epoch\n","            if itr % iter_per_epoch == 0:\n","                iter_X = iter(dataloader_X)\n","                iter_Y = iter(dataloader_Y)\n","\n","            images_X, c_X, label_X, id_X = next(iter_X)\n","            images_Y, c_Y, label_Y, id_Y = next(iter_Y)\n","\n","            images_X = images_X.squeeze(0)\n","            c_X = c_X.squeeze(0)\n","            label_X = label_X.squeeze(0)\n","#             label_X = label_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_X = label_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_X = label_X.reshape(label_X.shape[0] * label_X.shape[1], label_X.shape[2])  # (seq_len * N, 1)\n","\n","            images_Y = images_Y.squeeze(0)\n","            c_Y = c_Y.squeeze(0)\n","            label_Y = label_Y.squeeze(0)\n","#             label_Y = label_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_Y = label_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_Y = label_Y.reshape(label_Y.shape[0] * label_Y.shape[1], label_Y.shape[2])  # (seq_len * N, 1)\n","\n","            c_X = dequant(c_X)\n","            c_Y = dequant(c_Y)\n","            images_X = utils.to_var(images_X)\n","            label_X = utils.to_var(label_X)\n","            images_Y = utils.to_var(images_Y)\n","            label_Y = utils.to_var(label_Y)\n","            c_X = utils.to_var(c_X)\n","            c_Y = utils.to_var(c_Y)\n","\n","            # 1. Compute the discriminator losses on real images\n","            D_X_loss = torch.mean((D_X(images_X, c_X)-1)**2)\n","            D_Y_loss = torch.mean((D_Y(images_Y, c_Y)-1)**2)\n","\n","            d_real_loss = D_X_loss + D_Y_loss\n","\n","            # 2. Generate domain-X-like images based on real images in domain Y\n","            fake_X = G_YtoX(images_Y, c_X)\n","\n","            # 3. Compute the loss for D_X\n","            D_fake_X_loss = torch.mean(D_X(fake_X.detach(), c_X)**2)\n","\n","            # 4. Generate domain-Y-like images based on real images in domain X\n","            fake_Y=G_XtoY(images_X, c_Y)\n","\n","            # 5. Compute the loss for D_Y\n","            D_fake_Y_loss = torch.mean(D_Y(fake_Y.detach(), c_Y)**2)\n","\n","            d_fake_loss = D_fake_X_loss + D_fake_Y_loss\n","            d_total_loss = d_real_loss + d_fake_loss\n","\n","            # Class loss\n","            pred_fake_Y = Cls_X(fake_Y.detach(), c_Y)\n","            Cls_X2Y_loss = torch.mean((pred_fake_Y-label_X)**2)\n","            pred_real_X = Cls_X(images_X, c_X)\n","            Cls_X_loss = torch.mean((pred_real_X-label_X)**2)\n","\n","            pred_fake_X = Cls_Y(fake_X.detach(), c_X)\n","            Cls_Y2X_loss = torch.mean((pred_fake_X-label_Y)**2)\n","            pred_real_Y = Cls_Y(images_Y, c_Y)\n","            Cls_Y_loss = torch.mean((pred_real_Y-label_Y )**2)\n","\n","            Cls_total_loss=Cls_X2Y_loss + Cls_Y_loss + Cls_Y2X_loss + Cls_X_loss\n","\n","            # 1. Generate domain-X-like images based on real images in domain Y\n","            fake_X = G_YtoX(images_Y, c_X)\n","\n","            # 2. Compute the generator loss based on domain X\n","            g_loss = torch.mean((D_X(fake_X, c_X)-1)**2)\n","\n","            if opts.use_cycle_consistency_loss:\n","                # 3. Compute the cycle consistency loss (the reconstruction loss)\n","                cycle_consistency_loss = torch.mean(torch.abs(images_Y-G_XtoY(fake_X, c_Y)))\n","\n","                g_loss += opts.lambda_cycle * cycle_consistency_loss\n","\n","            # X--Y-->X CYCLE\n","            # 1. Generate domain-Y-like images based on real images in domain X\n","            fake_Y = G_XtoY(images_X, c_Y)\n","\n","            # 2. Compute the generator loss based on domain Y\n","            g_loss += torch.mean((D_Y(fake_Y, c_Y)-1)**2)\n","\n","            if opts.use_cycle_consistency_loss:\n","                # 3. Compute the cycle consistency loss (the reconstruction loss)\n","                cycle_consistency_loss = torch.mean(torch.abs(images_X - G_YtoX(fake_Y, c_X)))\n","\n","                g_loss += opts.lambda_cycle * cycle_consistency_loss\n","\n","            pred_fake_T = Cls_Y(fake_Y, c_Y)\n","            Cls_S2T_loss = torch.mean((pred_fake_T-label_Y)**2)\n","            pred_fake_S = Cls_X(fake_X, c_X)\n","            Cls_T2S_loss = torch.mean((pred_fake_S-label_X)**2)\n","            if opts.use_cycle_consistency_loss:\n","                Cls_cycle_loss = torch.mean((Cls_Y(G_XtoY(fake_X, c_Y),c_Y)-label_Y)**2) + torch.mean((Cls_X(G_YtoX(fake_Y, c_X),c_X)-label_X)**2)\n","\n","            g_loss+=Cls_S2T_loss + Cls_T2S_loss + Cls_cycle_loss\n","\n","            d_real_loss_hist += d_real_loss\n","            D_Y_loss_hist += D_Y_loss\n","            D_X_loss_hist += D_X_loss\n","            d_fake_loss_hist += d_fake_loss\n","            g_loss_hist += g_loss\n","            Cls_Y_loss_hist += Cls_Y_loss\n","            Cls_X_loss_hist += Cls_X_loss\n","            Cls_Y2X_loss_hist += Cls_Y2X_loss\n","            Cls_X2Y_loss_hist += Cls_X2Y_loss\n","\n","            ## Save the generated samples\n","            #save_samples_test(itr, images_Y, c_Y, id_Y, label_Y, images_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts)\n","            #save_samples_fixed(itr, fixed_Y, c_fixed_Y, id_fixed_Y, label_fixed_Y, fixed_X, c_fixed_X, id_fixed_X, label_fixed_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts)\n","\n","    print(\"####### validation #######\")\n","    print(\n","        'Iteration [{}] | d_real_loss: {:6.4f} | '\n","        'd_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n","        'd_fake_loss: {:6.4f} | g_loss: {:6.4f} | Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}| Cls_Y2X_loss: {:6.4f} | Cls_X2Y_loss: {:6.4f}'.format(\n","            iter_per_epoch, d_real_loss_hist.item()/iter_per_epoch,\n","            D_Y_loss_hist.item()/iter_per_epoch, D_X_loss_hist.item()/iter_per_epoch,\n","            d_fake_loss_hist.item()/iter_per_epoch, g_loss_hist.item()/iter_per_epoch,\n","            Cls_Y_loss_hist.item()/iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_loss_hist.item()/iter_per_epoch, Cls_X2Y_loss_hist.item()/iter_per_epoch\n","\n","        ))\n","    print(\"###########################\")\n","\n","    # plot the losses in tensorboard\n","    logger.add_scalar('Cls_val/XY/fake', Cls_X2Y_loss_hist/iter_per_epoch, iteration)\n","    logger.add_scalar('Cls_val/Y/real', Cls_Y_loss_hist/iter_per_epoch, iteration)\n","    logger.add_scalar('Cls_val/YX/fake', Cls_Y2X_loss_hist/iter_per_epoch, iteration)\n","    logger.add_scalar('Cls_val/X/real', Cls_X_loss_hist/iter_per_epoch, iteration)\n","\n","    return (Cls_Y_loss_hist + Cls_X_loss_hist + Cls_Y2X_loss_hist + Cls_X2Y_loss_hist)/iter_per_epoch\n"],"metadata":{"id":"vj8UMWLh4IpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pickle import NONE\n","# CMU 16-726 Learning-Based Image Synthesis / Spring 2023, Assignment 3\n","#\n","# The code base is based on the great work from CSC 321, U Toronto\n","# https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip\n","# This is the main training file for the second part of the assignment.\n","#\n","# Usage:\n","# ======\n","#    To train with the default hyperparamters:\n","#       python cycle_gan.py\n","#\n","#    To train with cycle consistency loss:\n","#       python cycle_gan.py --use_cycle_consistency_loss\n","#\n","#\n","#    For optional experimentation:\n","#    -----------------------------\n","#    If you have a powerful computer (ideally with a GPU),\n","#    then you can obtain better results by\n","#    increasing the number of filters used in the generator\n","#    and/or discriminator, as follows:\n","#      python cycle_gan.py --g_conv_dim=64 --d_conv_dim=64\n","\n","import argparse\n","import os\n","\n","import imageio\n","from torch.utils.tensorboard import SummaryWriter\n","import torch\n","import torch.optim as optim\n","import numpy as np\n","\n","import utils\n","from data_loader import get_data_loader\n","# from models import CycleGenerator, DCDiscriminator, PatchDiscriminator\n","from diff_augment import DiffAugment\n","from model_classifier import deeplabv2_resnet101\n","policy = 'color,translation,cutout'\n","\n","\n","SEED = 11\n","\n","# Set the random seed manually for reproducibility.\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","\n","\n","def print_models(G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y):\n","    \"\"\"Prints model information for the generators and discriminators.\n","    \"\"\"\n","    print(\"                 G_XtoY                \")\n","    print(\"---------------------------------------\")\n","    print(G_XtoY)\n","    print(\"---------------------------------------\")\n","\n","    print(\"                 G_YtoX                \")\n","    print(\"---------------------------------------\")\n","    print(G_YtoX)\n","    print(\"---------------------------------------\")\n","\n","    print(\"                  D_X                  \")\n","    print(\"---------------------------------------\")\n","    print(D_X)\n","    print(\"---------------------------------------\")\n","\n","    print(\"                  D_Y                  \")\n","    print(\"---------------------------------------\")\n","    print(D_Y)\n","    print(\"---------------------------------------\")\n","\n","    print(\"                  Cls_X                  \")\n","    print(\"---------------------------------------\")\n","    print(Cls_X)\n","    print(\"---------------------------------------\")\n","\n","    print(\"                  Cls_Y                  \")\n","    print(\"---------------------------------------\")\n","    print(Cls_Y)\n","    print(\"---------------------------------------\")\n","\n","def create_model(opts, cx, cy):\n","    \"\"\"Builds the generators and discriminators.\n","    \"\"\"\n","\n","    model_dict = {'deep_resnet34': deep_resnet34, \"CNN_LSTM\": CNN_LSTM}\n","    Cls_X = model_dict[opts.classifier](cond_args=cx.cond_args)\n","    Cls_Y = model_dict[opts.classifier](cond_args=cy.cond_args)\n","\n","#     print_models(G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y)\n","\n","    if torch.cuda.is_available():\n","\n","        Cls_X.cuda()\n","\n","\n","        print('Models moved to GPU.')\n","\n","    return Cls_X\n","\n","\n","def checkpoint(iteration, Cls_X, opts, Cls_X_loss, best=False):\n","    \"\"\"Save generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\"\"\"\n","    if best:\n","\n","        Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X-best.pkl')\n","\n","\n","    else:\n","        Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X_iter%d-Cls_X_loss%f.pkl' % (iteration, Cls_X_loss))\n","\n","    torch.save(Cls_X.state_dict(), Cls_X_path)\n","\n","def merge_images(sources, cond, targets, opts, k=10):\n","    \"\"\"\n","    Creates a grid consisting of pairs of columns, where the first column in\n","    each pair contains images source images and the second column in each pair\n","    contains images generated by the CycleGAN from the corresponding images in\n","    the first column.\n","    \"\"\"\n","    b, _, h, w = sources.shape\n","    row = int(np.sqrt(b))+1\n","    merged = np.zeros([3, row * h, row * w * 2])\n","    cond = cond.cpu().numpy()\n","    time=[]\n","    for idx, (s, t) in enumerate(zip(sources, targets)):\n","        dt_back = np.datetime64(int(cond[idx,0]), 's')\n","        time.append(dt_back)\n","        i = idx // row\n","        j = idx % row\n","        # merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n","        # merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n","        merged[:, i * h:(i + 1) * h, (j * 2) * w:(j * 2 + 1) * w] = s\n","        merged[:, i * h:(i + 1) * h, (j * 2 + 1) * w:(j * 2 + 2) * w] = t\n","    return merged.transpose(1, 2, 0), time\n","\n","def save_samples(iteration, fixed_Y, c_Y, fixed_X, c_X, G_YtoX, G_XtoY, opts):\n","    \"\"\"Saves samples from both generators X->Y and Y->X.\n","    \"\"\"\n","\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","\n","    X, fake_X = utils.to_data(fixed_X), utils.to_data(fake_X)\n","    Y, fake_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y)\n","\n","    merged, time = merge_images(X, c_X, fake_Y, opts)\n","    path = os.path.join(\n","        opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration)\n","    )\n","    merged = np.uint8(255 * (merged + 1) / 2)\n","    imageio.imwrite(path, merged)\n","    print('Saved {}'.format(path))\n","    print(\"X-Y\", time)\n","\n","    merged, time = merge_images(Y, c_Y, fake_X, opts)\n","    path = os.path.join(\n","        opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration)\n","    )\n","    merged = np.uint8(255 * (merged + 1) / 2)\n","    imageio.imwrite(path, merged)\n","    print('Saved {}'.format(path))\n","    print(\"Y-X\", time)\n","\n","def save_images(sources, conds_s, label, targets, conds_t, pred, opts, iteration, pref, id_s, id_t, k=10, tp=\"fixed\"):\n","\n","    _, _, h, w = sources.shape\n","    time=[]\n","    for idx, (s, t) in enumerate(zip(sources, targets)):\n","\n","        if idx==0:\n","            dt_s = np.datetime64(int(conds_s[idx,0]), 's')\n","            path = os.path.join(\n","              opts.sample_dir, tp+'-{}-{}-{}-{}-{:06d}-{}.png'.format(pref, id_s[idx], id_t[idx], dt_s, iteration, label[idx])\n","            )\n","            s = np.uint8(255 * (s + 1) / 2)\n","            imageio.imwrite(path, s.transpose(1, 2, 0))\n","\n","        dt_t = np.datetime64(int(conds_t[idx,0]), 's')\n","        path = os.path.join(\n","            opts.sample_dir, tp+'-{}-{}-{}-{}-{:06d}-fake-{}.png'.format(pref, id_s[idx], id_t[idx], dt_t, iteration, pred[idx])\n","        )\n","        t = np.uint8(255 * (t + 1) / 2)\n","        imageio.imwrite(path, t.transpose(1, 2, 0))\n","\n","\n","def save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_Y, label_Y, fixed_X, c_fixed_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","#     X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X)-1), np.exp(utils.to_data(pred_X))-1\n","#     Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y)-1), np.exp(utils.to_data(pred_Y))-1\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_fixed_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y)\n","    save_images(Y, c_fixed_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X)\n","\n","def training_loop(cur_path, fol_X, fol_Y, opts):\n","    \"\"\"Runs the training loop.\n","        * Saves checkpoint every opts.checkpoint_every iterations\n","        * Saves generated samples every opts.sample_every iterations\n","    \"\"\"\n","\n","    X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"train\", len_seq=opts.batch_size*opts.len_seq, transform=transform, mask_frac=opts.mask_frac)\n","    Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"train\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","    # # Create  dataloaders for images from the two domains X and Y\n","    dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","    dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","\n","    # Create generators and discriminators\n","    cx = EasyDict() # Main config dict.\n","    cond_desc(cx, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_X))\n","    cy = EasyDict() # Main config dict.\n","    cond_desc(cy, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_Y))\n","    Cls_X = create_model(opts,cx,cy)\n","\n","\n","    cls_params = list(Cls_X.parameters())\n","\n","    # Create optimizers for the generators and discriminators\n","\n","    cls_optimizer = optim.Adam(cls_params, opts.lr, [opts.beta1, opts.beta2])\n","\n","    # Set the learning rate scheduler\n","\n","    cls_scheduler = lr_scheduler.StepLR(cls_optimizer, step_size=3, gamma=0.1)\n","\n","    iter_X = iter(dataloader_X)\n","    iter_Y = iter(dataloader_Y)\n","    # itr = iter(dataloader)\n","\n","    # Get some fixed data from domains X and Y for sampling.\n","    # These are images that are held constant throughout training,\n","    # that allow us to inspect the model's performance.\n","    fixed_X, c_fixed_X, label_fixed_X, id_fixed_X = next(iter_X)\n","    fixed_Y, c_fixed_Y, label_fixed_Y, id_fixed_Y = next(iter_Y)\n","    # fixed_X, fixed_label_X, fixed_Y, fixed_label_Y = next(itr)\n","\n","    fixed_X = fixed_X.squeeze(0)\n","    c_fixed_X = c_fixed_X.squeeze(0)\n","    label_fixed_X = label_fixed_X.squeeze(0)\n","\n","    fixed_Y = fixed_Y.squeeze(0)\n","    c_fixed_Y = c_fixed_Y.squeeze(0)\n","    label_fixed_Y = label_fixed_Y.squeeze(0)\n","\n","    fixed_c_X = dequant(c_fixed_X)\n","    fixed_c_Y = dequant(c_fixed_Y)\n","    fixed_X = utils.to_var(fixed_X)\n","    fixed_Y = utils.to_var(fixed_Y)\n","    label_fixed_X = utils.to_var(label_fixed_X)\n","    label_fixed_Y = utils.to_var(label_fixed_Y)\n","    c_fixed_X = utils.to_var(c_fixed_X)\n","    c_fixed_Y = utils.to_var(c_fixed_Y)\n","\n","    iter_per_epoch = min(len(iter_X), len(iter_Y))\n","    # iter_per_epoch = len(itr)\n","    min_loss=math.inf\n","    for iteration in range(1, opts.train_iters + 1):\n","\n","        # Reset data_iter for each epoch\n","        if iteration % iter_per_epoch == 0:\n","            iter_X = iter(dataloader_X)\n","            iter_Y = iter(dataloader_Y)\n","            # itr = iter(dataloader)\n","\n","        images_X, c_X, label_X, id_X = next(iter_X)\n","        # print(\"X\")\n","        images_Y, c_Y, label_Y, id_Y = next(iter_Y)\n","        # images_X, label_X, images_Y, label_Y = next(itr)\n","        # print(\"Y\")\n","\n","        images_X = images_X.squeeze(0)\n","        c_X = c_X.squeeze(0)\n","        label_X = label_X.squeeze(0)\n","\n","        images_Y = images_Y.squeeze(0)\n","        c_Y = c_Y.squeeze(0)\n","        label_Y = label_Y.squeeze(0)\n","\n","        c_X = dequant(c_X)\n","        c_Y = dequant(c_Y)\n","        images_X = utils.to_var(images_X)\n","        label_X = utils.to_var(label_X)\n","        images_Y = utils.to_var(images_Y)\n","        label_Y = utils.to_var(label_Y)\n","        c_X = utils.to_var(c_X)\n","        c_Y = utils.to_var(c_Y)\n","\n","#         # TRAIN THE DISCRIMINATORS\n","#         # 1. Compute the discriminator losses on real images\n","#         D_X_loss = torch.mean((D_X(images_X, c_X)-1)**2)\n","#         D_Y_loss = torch.mean((D_Y(images_Y, c_Y)-1)**2)\n","\n","#         d_real_loss = D_X_loss + D_Y_loss\n","\n","#         # 2. Generate domain-X-like images based on real images in domain Y\n","#         fake_X = G_YtoX(images_Y, c_X)\n","\n","#         # 3. Compute the loss for D_X\n","#         D_fake_X_loss = torch.mean(D_X(fake_X.detach(), c_X)**2)\n","\n","#         # 4. Generate domain-Y-like images based on real images in domain X\n","#         fake_Y=G_XtoY(images_X, c_Y)\n","\n","#         # 5. Compute the loss for D_Y\n","#         D_fake_Y_loss = torch.mean(D_Y(fake_Y.detach(), c_Y)**2)\n","\n","#         d_fake_loss = D_fake_X_loss + D_fake_Y_loss\n","#         d_total_loss = d_real_loss + d_fake_loss\n","\n","#         # sum up the losses and update D_X and D_Y\n","#         d_optimizer.zero_grad()\n","#         d_total_loss.backward()\n","#         d_optimizer.step()\n","\n","# #         # Step the learning rate scheduler\n","# #         d_scheduler.step()\n","\n","#         # plot the losses in tensorboard\n","#         logger.add_scalar('D/XY/real', D_X_loss, iteration)\n","#         logger.add_scalar('D/YX/real', D_Y_loss, iteration)\n","#         logger.add_scalar('D/YX/fake', D_fake_X_loss, iteration)\n","#         logger.add_scalar('D/XY/fake', D_fake_Y_loss, iteration)\n","\n","        mask=torch.isnan(label_X)\n","\n","        pred_real_X = Cls_X(images_X, c_X)\n","        Cls_X_loss = criterion_cls(pred_real_X[~mask], label_X[~mask])  if torch.sum(~mask).item()>0 else 0.\n","\n","#         pred_fake_X = Cls_X(fake_X.detach(), c_Y)\n","#         Cls_Y2X_loss = criterion_cls(pred_fake_X, label_Y)\n","\n","#         pred_fake_Y = Cls_Y(fake_Y.detach(), c_X)\n","#         Cls_X2Y_loss = criterion_cls(pred_fake_Y[~mask], label_X[~mask]) if torch.sum(~mask).item()>0 else 0.\n","#         Cls_X2Y_loss += criterion_cls(pred_fake_Y[mask], pred_real_X[mask])  if torch.sum(mask).item()>0 else 0.\n","\n","#         pred_real_Y = Cls_Y(images_Y, c_Y)\n","#         Cls_Y_loss = criterion_cls(pred_real_Y, label_Y)\n","\n","#         # augmentation\n","#         pred_fake_Y = Cls_X(fake_Y.detach(), c_X)\n","#         Cls_X2Y_aug_loss = criterion_cls(pred_fake_Y[~mask], label_X[~mask]) if torch.sum(~mask).item()>0 else 0.\n","#         Cls_X2Y_aug_loss += criterion_cls(pred_fake_Y[mask], pred_real_X[mask])  if torch.sum(mask).item()>0 else 0.\n","\n","#         pred_fake_X = Cls_Y(fake_X.detach(), c_Y)\n","#         Cls_Y2X_aug_loss = criterion_cls(pred_fake_X, label_Y)\n","\n","#         cls_total_loss = (Cls_X2Y_loss + Cls_X_loss + Cls_Y_loss + Cls_Y2X_loss) * opts.lambda_cls + (Cls_X2Y_aug_loss + Cls_Y2X_aug_loss) * opts.lambda_aug\n","        cls_total_loss = Cls_X_loss\n","\n","        # sum up the losses and update D_X and D_Y\n","        if cls_total_loss!=0:\n","            cls_optimizer.zero_grad()\n","            cls_total_loss.backward()\n","            cls_optimizer.step()\n","\n","#         # Step the learning rate scheduler\n","#         cls_scheduler.step()\n","\n","        # plot the losses in tensorboard\n","#         logger.add_scalar('Cls/XY/fake', Cls_X2Y_loss, iteration)\n","#         logger.add_scalar('Cls/XY_aug/fake', Cls_X2Y_aug_loss, iteration)\n","#         logger.add_scalar('Cls/Y/real', Cls_Y_loss, iteration)\n","#         logger.add_scalar('Cls/YX/fake', Cls_Y2X_loss, iteration)\n","#         logger.add_scalar('Cls/YX_aug/fake', Cls_Y2X_aug_loss, iteration)\n","            logger.add_scalar('Cls/X/real', Cls_X_loss, iteration)\n","\n","#         # TRAIN THE GENERATORS\n","#         # 1. Generate domain-X-like images based on real images in domain Y\n","#         fake_X = G_YtoX(images_Y, c_X)\n","\n","#         # 2. Compute the generator loss based on domain X\n","#         g_loss = torch.mean((D_X(fake_X, c_X)-1)**2)\n","#         logger.add_scalar('G/XY/fake', g_loss, iteration)\n","\n","#         if opts.use_cycle_consistency_loss:\n","#             # 3. Compute the cycle consistency loss (the reconstruction loss)\n","#             cycle_consistency_loss = torch.mean(torch.abs(images_Y-G_XtoY(fake_X, c_Y)))\n","\n","#             g_loss += opts.lambda_cycle * cycle_consistency_loss\n","#             logger.add_scalar('G/XY/cycle', opts.lambda_cycle * cycle_consistency_loss, iteration)\n","\n","#         # X--Y-->X CYCLE\n","#         # 1. Generate domain-Y-like images based on real images in domain X\n","#         fake_Y = G_XtoY(images_X, c_Y)\n","\n","#         # 2. Compute the generator loss based on domain Y\n","#         g_loss += torch.mean((D_Y(fake_Y, c_Y)-1)**2)\n","#         logger.add_scalar('G/YX/fake', g_loss, iteration)\n","\n","#         if opts.use_cycle_consistency_loss:\n","#             # 3. Compute the cycle consistency loss (the reconstruction loss)\n","#             cycle_consistency_loss = torch.mean(torch.abs(images_X - G_YtoX(fake_Y, c_X)))\n","\n","#             g_loss += opts.lambda_cycle * cycle_consistency_loss\n","#             logger.add_scalar('G/YX/cycle', cycle_consistency_loss, iteration)\n","\n","#         # # Semantic loss\n","#         # pred_fake_T = Cls(fake_Y)\n","#         # pred_real_S = Cls(images_X)\n","#         # loss_semantic_S2T = criterion_semantic(pred_fake_T, label_X) * opts.trade_off_semantic\n","#         # pred_fake_S = Cls(fake_X)\n","#         # pred_real_T = Cls(images_Y)\n","#         # loss_semantic_T2S = criterion_semantic(pred_fake_S, pred_real_T) * opts.trade_off_semantic\n","\n","# #         pred_fake_T = Cls_Y(fake_Y, c_Y)\n","# #         Cls_S2T_loss = criterion_cls(pred_fake_T, label_Y)\n","# #         pred_fake_S = Cls_X(fake_X, c_X)\n","# #         Cls_T2S_loss = criterion_cls(pred_fake_S[~mask], label_X[~mask])  if torch.sum(~mask).item()>0 else 0.\n","# #         pred_real_S = Cls_X(images_X, c_X)\n","# #         Cls_T2S_loss += criterion_cls(pred_fake_S[mask], pred_real_S[mask])  if torch.sum(mask).item()>0 else 0.\n","\n","#         pred_fake_T = Cls_Y(fake_Y, c_X)\n","#         Cls_S2T_loss = criterion_cls(pred_fake_T[~mask], label_X[~mask])  if torch.sum(~mask).item()>0 else 0.\n","#         pred_real_S = Cls_X(images_X, c_X)\n","#         Cls_S2T_loss += criterion_cls(pred_fake_T[mask], pred_real_S[mask])  if torch.sum(mask).item()>0 else 0.\n","\n","#         pred_fake_S = Cls_X(fake_X, c_Y)\n","#         Cls_T2S_loss = criterion_cls(pred_fake_S, label_Y)\n","\n","#         if opts.use_cycle_consistency_loss:\n","#             Cls_cycle_loss = criterion_cls(Cls_Y(G_XtoY(fake_X, c_Y),c_Y), label_Y)\n","#             + criterion_cls(Cls_X(G_YtoX(fake_Y, c_X),c_X)[~mask], label_X[~mask]) if torch.sum(~mask).item()>0 else 0.\n","#             + criterion_cls(Cls_X(G_YtoX(fake_Y, c_X),c_X)[mask], Cls_X(images_X, c_X)[mask]) if torch.sum(mask).item()>0 else 0.\n","\n","#         g_loss+=(Cls_S2T_loss + Cls_T2S_loss) * opts.lambda_cls + Cls_cycle_loss * opts.lambda_cycle_cls\n","\n","\n","#         # backprop the aggregated g losses and update G_XtoY and G_YtoX\n","#         g_optimizer.zero_grad()\n","#         g_loss.backward()\n","#         g_optimizer.step()\n","\n","#         # Step the learning rate scheduler\n","#         g_scheduler.step()\n","\n","        # Print the log info\n","        if iteration % opts.log_step == 0:\n","            print(\n","                'Iteration [{:5d}/{:5d}] | Cls_X_loss: {:6.4f}'.format(\n","                    iteration, opts.train_iters, Cls_X_loss\n","                )\n","            )\n","\n","#         # Save the generated samples\n","#         if iteration % opts.sample_every == 0:\n","#             # save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_fixed_Y, label_fixed_Y, fixed_X, c_fixed_X, id_fixed_X, label_fixed_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts)\n","#             save_samples(iteration, fixed_Y, c_Y, fixed_X, c_X, G_YtoX, G_XtoY, opts)\n","\n","        # Save the model parameters\n","        if iteration % opts.checkpoint_every == 0:\n","            # checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y, opts, d_real_loss,\n","            #         D_Y_loss, D_X_loss,\n","            #         d_fake_loss, g_loss,\n","            #         Cls_Y2X_loss + Cls_Y_loss, Cls_X2Y_loss + Cls_X_loss)\n","\n","            # # Start validation\n","            # X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"val\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","            # Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"val\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","            # # Create  dataloaders for images from the two domains X and Y\n","            # dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","            # dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","\n","            val_loss=Cls_X_loss\n","            # val_loss = val_loop(dataloader_X, dataloader_Y, X.normalizer, Y.normalizer, iteration, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y, opts)\n","            # G_XtoY.train(), G_YtoX.train(), D_X.train(), D_Y.train(), Cls_X.train(), Cls_Y.train()\n","            if val_loss<min_loss:\n","                checkpoint(iteration, Cls_X, opts, Cls_X_loss, best=True)\n","\n","                min_loss=val_loss\n","\n","            # # continue training\n","            # X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"train\", len_seq=opts.batch_size*opts.len_seq, transform=transform, mask_frac=opts.mask_frac)\n","            # Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"train\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","            # # # Create  dataloaders for images from the two domains X and Y\n","            # dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","            # dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=True, num_workers=opts.num_workers, drop_last=True)\n","\n","def main(opts):\n","    \"\"\"Loads the data and starts the training loop.\"\"\"\n","    torch.manual_seed(123)\n","    cur_path = '/data/nak168/spatial_temporal/stream_img/data/fpe-westbrook/'\n","#     for fol_X in [\"West Brook Lower_01171090\"]:\n","    for fol_X in [\"West Brook Reservoir_01171020\"]:\n","        for fol_Y in [\"West Brook Lower_01171090\"]:\n","#         for fol_Y in [\"Avery Brook_River Right_01171000\"]:\n","#         for fol_Y in [\"Avery Brook_River Left_01171000\"]:\n","#         for fol_Y in [\"Obear Brook Lower_01171070\"]:\n","\n","            # Create checkpoint and sample directories\n","            utils.create_dir(opts.sample_dir)\n","            utils.create_dir(opts.checkpoint_dir)\n","\n","            # Start training\n","            training_loop(cur_path, fol_X, fol_Y, opts)\n","            # training_loop(dataloader, opts)\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        if opts.__dict__[key]:\n","            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n","\n","\n","def create_parser():\n","    \"\"\"Creates a parser for command-line arguments.\n","    \"\"\"\n","    parser = argparse.ArgumentParser('Explainable Parser', add_help=False)\n","\n","    # Model hyper-parameters\n","    parser.add_argument('--image_size', type=int, default=64)\n","    parser.add_argument('--disc', type=str, default='patch')  # or 'patch'\n","    parser.add_argument('--gen', type=str, default='cnn_lstm')\n","    parser.add_argument('--classifier', type=str, default='CNN_LSTM')\n","    parser.add_argument('--g_conv_dim', type=int, default=32)\n","    parser.add_argument('--d_conv_dim', type=int, default=32)\n","    parser.add_argument('--norm', type=str, default='instance')\n","    parser.add_argument('--use_cycle_consistency_loss', action='store_true', default= True)\n","    parser.add_argument('--init_zero_weights', action='store_true', default=False)\n","    parser.add_argument('--init_type', type=str, default='naive')\n","\n","    # Training hyper-parameters\n","    parser.add_argument('--train_iters', type=int, default=10000)\n","    parser.add_argument('--batch_size', type=int, default=2)\n","    parser.add_argument('--len_seq', type=int, default=20)\n","    parser.add_argument('--num_workers', type=int, default=4)\n","    parser.add_argument('--lr', type=float, default=0.0003)\n","    parser.add_argument('--beta1', type=float, default=0.5)\n","    parser.add_argument('--beta2', type=float, default=0.999)\n","    parser.add_argument('--lambda_cycle', type=float, default=5)\n","    parser.add_argument('--lambda_cycle_cls', type=float, default=5)\n","    parser.add_argument('--lambda_cls', type=float, default=1, help='trade off for cls loss')\n","    parser.add_argument('--lambda_aug', type=float, default=1, help='trade off for cls augmentation loss')\n","\n","\n","\n","    parser.add_argument('--mask_frac', type=float, default=0.98)\n","\n","\n","\n","\n","    # Data sources\n","    parser.add_argument('--X', type=str, default='West Brook Reservoir')\n","    parser.add_argument('--Y', type=str, default='West Brook Lower')\n","\n","\n","\n","    parser.add_argument('--ext', type=str, default='*.png')\n","    parser.add_argument('--use_diffaug', action='store_true', default= True)\n","    parser.add_argument('--data_preprocess', type=str, default='basic')\n","\n","    # Saving directories and checkpoint/sample iterations\n","    parser.add_argument('--checkpoint_dir', default='checkpoints_cyclegan')\n","    parser.add_argument('--sample_dir', type=str, default='/data/nak168/spatial_temporal/stream_img/CGAN/')\n","    parser.add_argument('--log_step', type=int, default=10)\n","    parser.add_argument('--sample_every', type=int, default=100)\n","    parser.add_argument('--checkpoint_every', type=int, default=800)\n","\n","    parser.add_argument('--gpu', type=str, default='0')\n","\n","    # Time-lapse specific hyperparameters.\n","    parser.add_argument('--cond_lr',      help='Learning rate for all conditioning signals', metavar='FLOAT',       type=click.FloatRange(min=0), default=1.0)\n","    parser.add_argument('--trend_lr',     help='Additional LR multiplier for trend signal', metavar='FLOAT',        type=click.FloatRange(min=0), default=1.0)\n","    parser.add_argument('--cond',         help='Conditioning type',                                                 type=click.Choice(['fourier', 'f_concat', 'concat', 'none']), default='fourier')\n","    parser.add_argument('--noise',        help='Timestamp augmentation noise', metavar='[NAME|A,B,C|auto]',         default='auto')\n","    parser.add_argument('--days',         help='Number of days in sequence [default: use metadata]', metavar='INT', type=int, default=None)\n","\n","    return parser\n","\n","\n","if __name__ == '__main__':\n","    import sys\n","    sys.argv=[\"\"]\n","    parser=create_parser()\n","    opts = parser.parse_args()\n","    os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu\n","    opts.sample_dir = os.path.join(\n","        opts.sample_dir, 'output/'\n","        '%s_%s_%g_%g_%g/' % (opts.X.split('/')[0], opts.Y.split('/')[0], opts.mask_frac, opts.lambda_cls, opts.lambda_cycle_cls)\n","    )\n","    opts.sample_dir += '%s_%s_%s_%s_%g_%s' % (\n","        opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.lambda_cycle, opts.init_type\n","    )\n","    if opts.use_cycle_consistency_loss:\n","        opts.sample_dir += '_cycle'\n","    if opts.use_diffaug:\n","        opts.sample_dir += '_diffaug'\n","\n","    if os.path.exists(opts.sample_dir):\n","        cmd = 'rm %s/*' % opts.sample_dir\n","        os.system(cmd)\n","\n","    opts.checkpoint_dir = os.path.join(\n","        opts.sample_dir, opts.checkpoint_dir\n","    )\n","\n","    logger = SummaryWriter(opts.sample_dir)\n","\n","    print_opts(opts)\n","    batch_size=opts.batch_size\n","    len_seq=opts.len_seq\n","    main(opts)\n"],"metadata":{"id":"8QGTtdYg2dWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import axis\n","def loadcheckpoint(opts, Cls_X):\n","    \"\"\"Save generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\"\"\"\n","\n","    Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X-best.pkl')\n","    Cls_X.load_state_dict(torch.load(Cls_X_path, map_location=\"cuda\"))\n","\n","    return Cls_X\n","\n","def save_samples_test(iteration, fixed_Y, c_Y, id_Y, label_Y, fixed_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y, tp=\"all\")\n","    save_images(Y, c_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X, tp=\"all\")\n","\n","def save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_Y, label_Y, fixed_X, c_fixed_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_fixed_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y)\n","    save_images(Y, c_fixed_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X)\n","\n","def test_loop(dataloader_X, dataloader_Y, X_normalizer, Y_normalizer, opts):\n","    \"\"\"Runs the training loop.\n","        * Saves checkpoint every opts.checkpoint_every iterations\n","        * Saves generated samples every opts.sample_every iterations\n","    \"\"\"\n","    # # Create generators and discriminators\n","    cx = EasyDict() # Main config dict.\n","    cond_desc(cx, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_X))\n","    cy = EasyDict() # Main config dict.\n","    cond_desc(cy, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_Y))\n","    Cls_X = create_model(opts,cx,cy)\n","    Cls_X = loadcheckpoint(opts, Cls_X)\n","    Cls_X.eval()\n","\n","    iter_X = iter(dataloader_X)\n","    iter_Y = iter(dataloader_Y)\n","\n","    # Get some fixed data from domains X and Y for sampling.\n","    # These are images that are held constant throughout training,\n","    # that allow us to inspect the model's performance.\n","    fixed_X, c_fixed_X, label_fixed_X, id_fixed_X = next(iter_X)\n","    fixed_Y, c_fixed_Y, label_fixed_Y, id_fixed_Y = next(iter_Y)\n","\n","    fixed_X = fixed_X.squeeze(0)\n","    c_fixed_X = c_fixed_X.squeeze(0)\n","    label_fixed_X = label_fixed_X.squeeze(0)\n","#     label_fixed_X = label_fixed_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_X = label_fixed_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_X = label_fixed_X.reshape(label_fixed_X.shape[0] * label_fixed_X.shape[1], label_fixed_X.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_Y = fixed_Y.squeeze(0)\n","    c_fixed_Y = c_fixed_Y.squeeze(0)\n","    label_fixed_Y = label_fixed_Y.squeeze(0)\n","#     label_fixed_Y = label_fixed_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_Y = label_fixed_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_Y = label_fixed_Y.reshape(label_fixed_Y.shape[0] * label_fixed_Y.shape[1], label_fixed_Y.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_c_X = dequant(c_fixed_X)\n","    fixed_c_Y = dequant(c_fixed_Y)\n","    fixed_X = utils.to_var(fixed_X)\n","    fixed_Y = utils.to_var(fixed_Y)\n","    label_fixed_X = utils.to_var(label_fixed_X)\n","    label_fixed_Y = utils.to_var(label_fixed_Y)\n","    c_fixed_X = utils.to_var(c_fixed_X)\n","    c_fixed_Y = utils.to_var(c_fixed_Y)\n","\n","    iter_per_epoch_X, iter_per_epoch_Y = len(iter_X), len(iter_Y)\n","    iter_per_epoch=min(iter_per_epoch_X, iter_per_epoch_Y)\n","    d_real_loss_hist = D_Y_loss_hist = D_X_loss_hist = d_fake_loss_hist = g_loss_hist=0\n","    Cls_Y_loss_hist = Cls_X_loss_hist = Cls_Y2X_loss_hist = Cls_X2Y_loss_hist = Cls_Y2X_aug_loss_hist = Cls_X2Y_aug_loss_hist=0\n","    labels_X_list=np.array([])\n","    labels_Y_list=np.array([])\n","    preds_X_list=np.array([])\n","    preds_Y_list=np.array([])\n","    preds_X2Y_list=np.array([])\n","    preds_Y2X_list=np.array([])\n","    time_X_list=np.array([])\n","    time_Y_list=np.array([])\n","\n","    with torch.no_grad():\n","        for iteration in range(1, iter_per_epoch):\n","            # Reset data_iter for each epoch\n","            if iteration % iter_per_epoch_X == 0:\n","                iter_X = iter(dataloader_X)\n","            if iteration % iter_per_epoch_Y == 0:\n","                iter_Y = iter(dataloader_Y)\n","\n","            images_X, c_X, label_X, id_X = next(iter_X)\n","            images_Y, c_Y, label_Y, id_Y = next(iter_Y)\n","\n","            images_X = images_X.squeeze(0)\n","            c_X = c_X.squeeze(0)\n","            label_X = label_X.squeeze(0)\n","#             label_X = label_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_X = label_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_X = label_X.reshape(label_X.shape[0] * label_X.shape[1], label_X.shape[2])  # (seq_len * N, 1)\n","\n","            images_Y = images_Y.squeeze(0)\n","            c_Y = c_Y.squeeze(0)\n","            label_Y = label_Y.squeeze(0)\n","#             label_Y = label_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_Y = label_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_Y = label_Y.reshape(label_Y.shape[0] * label_Y.shape[1], label_Y.shape[2])  # (seq_len * N, 1)\n","\n","            c_X = dequant(c_X)\n","            c_Y = dequant(c_Y)\n","            images_X = utils.to_var(images_X)\n","            label_X = utils.to_var(label_X)\n","            images_Y = utils.to_var(images_Y)\n","            label_Y = utils.to_var(label_Y)\n","            c_X = utils.to_var(c_X)\n","            c_Y = utils.to_var(c_Y)\n","\n","\n","            # Class loss\n","\n","\n","            pred_real_X = Cls_X(images_X, c_X)\n","            Cls_X_loss = torch.mean((pred_real_X-label_X)**2)\n","\n","            cls_total_loss = Cls_X_loss\n","\n","            preds_X_list=np.append(preds_X_list, X_normalizer.inverse_transform(utils.to_data(pred_real_X)).squeeze())\n","            labels_X_list=np.append(labels_X_list, X_normalizer.inverse_transform(utils.to_data(label_X)).squeeze())\n","            time_X_list=np.append(time_X_list, utils.to_data(c_X).squeeze())\n","\n","\n","            Cls_X_loss_hist += Cls_X_loss\n","\n","            ## Save the generated samples\n","            #save_samples_test(iteration, images_Y, c_Y, id_Y, label_Y, images_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts)\n","            #save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_fixed_Y, label_fixed_Y, fixed_X, c_fixed_X, id_fixed_X, label_fixed_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts)\n","\n","    print(\n","        'Iteration [{}] | Cls_X_loss: {:6.4f}'.format(\n","            iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","\n","        ))\n","\n","    print(\"Error_X\", np.mean(np.abs(labels_X_list-preds_X_list)))\n","    NSE=1-np.sum((labels_X_list-preds_X_list)**2)/np.sum((labels_X_list-np.mean(labels_X_list))**2)\n","    print('NSE',NSE)\n","\n","    loss_all = 'Iteration [{}] | Cls_X_loss: {:6.4f}| '.format(\n","            iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch)\n","\n","#     torch.save({'Cls_Y_loss': Cls_Y_loss_hist.item()/iter_per_epoch, 'Cls_X_loss':Cls_X_loss_hist.item()/iter_per_epoch,\n","#             'Cls_Y2X_loss': Cls_Y2X_loss_hist.item()/iter_per_epoch, 'Cls_X2Y_loss':Cls_X2Y_loss_hist.item()/iter_per_epoch,\n","#             'Cls_Y2X_aug':Cls_Y2X_aug_loss_hist.item()/iter_per_epoch, 'Cls_X2Y_aug':Cls_X2Y_aug_loss_hist.item()/iter_per_epoch}, opts.checkpoint_dir+\"/loss_best.pth\")\n","\n","    return labels_X_list, preds_X_list, time_X_list, loss_all\n","\n","def main(opts):\n","    \"\"\"Loads the data and starts the training loop.\"\"\"\n","    cur_path = '/data/nak168/spatial_temporal/stream_img/data/fpe-westbrook/'\n","#     for fol_X in [\"West Brook Lower_01171090\"]:\n","    for fol_X in [\"West Brook Reservoir_01171020\"]:\n","        for fol_Y in [\"West Brook Lower_01171090\"]:\n","#         for fol_Y in [\"Avery Brook_River Right_01171000\"]:\n","#         for fol_Y in [\"Avery Brook_River Left_01171000\"]:\n","#         for fol_Y in [\"Obear Brook Lower_01171070\"]:\n","            X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","            Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","            # # Create  dataloaders for images from the two domains X and Y\n","            dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            # dataset = ImageNetVidDataset(path= cur_path, fol_A=fol_X, fol_B=fol_Y, phase=\"train\", len_seq=len_seq, transform=transform)\n","            # dataset_val = ImageNetVidDataset(path= cur_path, fol_A=fol_A, fol_B=fol_B, phase=\"val\", len_seq=len_seq, transform=transform)\n","            # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","\n","            # Create checkpoint and sample directories\n","            utils.create_dir(opts.sample_dir)\n","\n","            # Start training\n","            labels_X_list, preds_X_list, time_X_list, loss_all = test_loop(dataloader_X, dataloader_Y, X.normalizer, Y.normalizer, opts)\n","            # training_loop(dataloader, opts)\n","            time_X_list=[np.datetime64(int(cur), \"s\") for cur in time_X_list]\n","#             time_Y_list=[np.datetime64(int(cur), \"s\") for cur in time_Y_list]\n","    return labels_X_list, preds_X_list, time_X_list, loss_all\n","\n","if __name__ == '__main__':\n","\n","    opts.sample_dir = os.path.join(\n","        '/data/nak168/spatial_temporal/stream_img/CGAN/', 'output/'\n","        '%s_%s_%g_%g_%g/' % (opts.X.split('/')[0], opts.Y.split('/')[0], opts.mask_frac, opts.lambda_cls, opts.lambda_cycle_cls)\n","    )\n","    opts.sample_dir += '%s_%s_%s_%s_%g_%s' % (\n","        opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.lambda_cycle, opts.init_type\n","    )\n","    if opts.use_cycle_consistency_loss:\n","        opts.sample_dir += '_cycle'\n","    if opts.use_diffaug:\n","        opts.sample_dir += '_diffaug'\n","\n","    opts.sample_dir = os.path.join(\n","        opts.sample_dir, 'test/'\n","    )\n","\n","    if os.path.exists(opts.sample_dir):\n","        cmd = 'rm %s/*' % opts.sample_dir\n","        os.system(cmd)\n","\n","    batch_size=opts.batch_size\n","#     len_seq=opts.len_seq\n","\n","    labels_X_list, preds_X_list, time_X_list, loss_all = main(opts)\n"],"metadata":{"id":"9iWfINLVXapo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import axis\n","def loadcheckpoint(opts, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y):\n","    \"\"\"Save generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\"\"\"\n","\n","    G_XtoY_path = os.path.join(\n","        opts.checkpoint_dir, 'G_XtoY-best.pkl'\n","    )\n","    G_YtoX_path = os.path.join(\n","        opts.checkpoint_dir, 'G_YtoX-best.pkl'\n","    )\n","    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X-best.pkl')\n","    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y-best.pkl')\n","    Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X-best.pkl')\n","    Cls_Y_path = os.path.join(opts.checkpoint_dir, 'Cls_Y-best.pkl')\n","\n","    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=\"cuda\"))\n","    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=\"cuda\"))\n","    D_X.load_state_dict(torch.load(D_X_path, map_location=\"cuda\"))\n","    D_Y.load_state_dict(torch.load(D_Y_path, map_location=\"cuda\"))\n","    Cls_X.load_state_dict(torch.load(Cls_X_path, map_location=\"cuda\"))\n","    Cls_Y.load_state_dict(torch.load(Cls_Y_path, map_location=\"cuda\"))\n","    return G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y\n","\n","def save_samples_test(iteration, fixed_Y, c_Y, id_Y, label_Y, fixed_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y, tp=\"all\")\n","    save_images(Y, c_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X, tp=\"all\")\n","\n","def save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_Y, label_Y, fixed_X, c_fixed_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_fixed_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y)\n","    save_images(Y, c_fixed_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X)\n","\n","def test_loop(dataloader_X, dataloader_Y, X_normalizer, Y_normalizer, opts):\n","    \"\"\"Runs the training loop.\n","        * Saves checkpoint every opts.checkpoint_every iterations\n","        * Saves generated samples every opts.sample_every iterations\n","    \"\"\"\n","    # # Create generators and discriminators\n","    cx = EasyDict() # Main config dict.\n","    cond_desc(cx, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_X))\n","    cy = EasyDict() # Main config dict.\n","    cond_desc(cy, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_Y))\n","    G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y = create_model(opts,cx,cy)\n","    G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y = loadcheckpoint(opts,G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y)\n","    G_XtoY.eval(), G_YtoX.eval(), D_X.eval(), D_Y.eval(), Cls_X.eval(), Cls_Y.eval()\n","\n","    iter_X = iter(dataloader_X)\n","    iter_Y = iter(dataloader_Y)\n","\n","    # Get some fixed data from domains X and Y for sampling.\n","    # These are images that are held constant throughout training,\n","    # that allow us to inspect the model's performance.\n","    fixed_X, c_fixed_X, label_fixed_X, id_fixed_X = next(iter_X)\n","    fixed_Y, c_fixed_Y, label_fixed_Y, id_fixed_Y = next(iter_Y)\n","\n","    fixed_X = fixed_X.squeeze(0)\n","    c_fixed_X = c_fixed_X.squeeze(0)\n","    label_fixed_X = label_fixed_X.squeeze(0)\n","#     label_fixed_X = label_fixed_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_X = label_fixed_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_X = label_fixed_X.reshape(label_fixed_X.shape[0] * label_fixed_X.shape[1], label_fixed_X.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_Y = fixed_Y.squeeze(0)\n","    c_fixed_Y = c_fixed_Y.squeeze(0)\n","    label_fixed_Y = label_fixed_Y.squeeze(0)\n","#     label_fixed_Y = label_fixed_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_Y = label_fixed_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_Y = label_fixed_Y.reshape(label_fixed_Y.shape[0] * label_fixed_Y.shape[1], label_fixed_Y.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_c_X = dequant(c_fixed_X)\n","    fixed_c_Y = dequant(c_fixed_Y)\n","    fixed_X = utils.to_var(fixed_X)\n","    fixed_Y = utils.to_var(fixed_Y)\n","    label_fixed_X = utils.to_var(label_fixed_X)\n","    label_fixed_Y = utils.to_var(label_fixed_Y)\n","    c_fixed_X = utils.to_var(c_fixed_X)\n","    c_fixed_Y = utils.to_var(c_fixed_Y)\n","\n","    iter_per_epoch_X, iter_per_epoch_Y = len(iter_X), len(iter_Y)\n","    iter_per_epoch=min(iter_per_epoch_X, iter_per_epoch_Y)\n","    d_real_loss_hist = D_Y_loss_hist = D_X_loss_hist = d_fake_loss_hist = g_loss_hist=0\n","    Cls_Y_loss_hist = Cls_X_loss_hist = Cls_Y2X_loss_hist = Cls_X2Y_loss_hist = Cls_Y2X_aug_loss_hist = Cls_X2Y_aug_loss_hist=0\n","    labels_X_list=np.array([])\n","    labels_Y_list=np.array([])\n","    preds_X_list=np.array([])\n","    preds_Y_list=np.array([])\n","    preds_X2Y_list=np.array([])\n","    preds_Y2X_list=np.array([])\n","    time_X_list=np.array([])\n","    time_Y_list=np.array([])\n","\n","    with torch.no_grad():\n","        for iteration in range(1, iter_per_epoch):\n","            # Reset data_iter for each epoch\n","            if iteration % iter_per_epoch_X == 0:\n","                iter_X = iter(dataloader_X)\n","            if iteration % iter_per_epoch_Y == 0:\n","                iter_Y = iter(dataloader_Y)\n","\n","            images_X, c_X, label_X, id_X = next(iter_X)\n","            images_Y, c_Y, label_Y, id_Y = next(iter_Y)\n","\n","            images_X = images_X.squeeze(0)\n","            c_X = c_X.squeeze(0)\n","            label_X = label_X.squeeze(0)\n","#             label_X = label_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_X = label_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_X = label_X.reshape(label_X.shape[0] * label_X.shape[1], label_X.shape[2])  # (seq_len * N, 1)\n","\n","            images_Y = images_Y.squeeze(0)\n","            c_Y = c_Y.squeeze(0)\n","            label_Y = label_Y.squeeze(0)\n","#             label_Y = label_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_Y = label_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_Y = label_Y.reshape(label_Y.shape[0] * label_Y.shape[1], label_Y.shape[2])  # (seq_len * N, 1)\n","\n","            c_X = dequant(c_X)\n","            c_Y = dequant(c_Y)\n","            images_X = utils.to_var(images_X)\n","            label_X = utils.to_var(label_X)\n","            images_Y = utils.to_var(images_Y)\n","            label_Y = utils.to_var(label_Y)\n","            c_X = utils.to_var(c_X)\n","            c_Y = utils.to_var(c_Y)\n","\n","            # 1. Compute the discriminator losses on real images\n","            D_X_loss = torch.mean((D_X(images_X, c_X)-1)**2)\n","            D_Y_loss = torch.mean((D_Y(images_Y, c_Y)-1)**2)\n","\n","            d_real_loss = D_X_loss + D_Y_loss\n","\n","            # 2. Generate domain-X-like images based on real images in domain Y\n","            fake_X = G_YtoX(images_Y, c_X)\n","\n","            # 3. Compute the loss for D_X\n","            D_fake_X_loss = torch.mean(D_X(fake_X.detach(), c_X)**2)\n","\n","            # 4. Generate domain-Y-like images based on real images in domain X\n","            fake_Y=G_XtoY(images_X, c_Y)\n","\n","            # 5. Compute the loss for D_Y\n","            D_fake_Y_loss = torch.mean(D_Y(fake_Y.detach(), c_Y)**2)\n","\n","            d_fake_loss = D_fake_X_loss + D_fake_Y_loss\n","            d_total_loss = d_real_loss + d_fake_loss\n","\n","            # Class loss\n","\n","\n","            pred_real_X = Cls_X(images_X, c_X)\n","            Cls_X_loss = torch.mean((pred_real_X-label_X)**2)\n","            pred_fake_X = Cls_X(fake_X.detach(), c_Y)\n","            Cls_Y2X_loss = torch.mean((pred_fake_X-label_Y)**2)\n","\n","            pred_fake_Y = Cls_Y(fake_Y.detach(), c_X)\n","            Cls_X2Y_loss = torch.mean((pred_fake_Y-label_X)**2)\n","            pred_real_Y = Cls_Y(images_Y, c_Y)\n","            Cls_Y_loss = torch.mean((pred_real_Y-label_Y )**2)\n","\n","            # augmentation\n","            pred_fake_Y = Cls_X(fake_Y.detach(), c_X)\n","            Cls_X2Y_aug_loss = torch.mean((pred_fake_Y-label_X)**2)\n","            pred_fake_X = Cls_Y(fake_X.detach(), c_Y)\n","            Cls_Y2X_aug_loss = torch.mean((pred_fake_X-label_Y)**2)\n","\n","            cls_total_loss = Cls_X2Y_loss + Cls_X_loss + Cls_Y_loss + Cls_Y2X_loss + Cls_X2Y_aug_loss + Cls_Y2X_aug_loss\n","\n","\n","            preds_X2Y_list=np.append(preds_X2Y_list, Y_normalizer.inverse_transform(utils.to_data(pred_fake_Y)).squeeze())\n","            preds_Y2X_list=np.append(preds_Y2X_list, X_normalizer.inverse_transform(utils.to_data(pred_fake_X)).squeeze())\n","            preds_X_list=np.append(preds_X_list, X_normalizer.inverse_transform(utils.to_data(pred_real_X)).squeeze())\n","            preds_Y_list=np.append(preds_Y_list, Y_normalizer.inverse_transform(utils.to_data(pred_real_Y)).squeeze())\n","            labels_X_list=np.append(labels_X_list, X_normalizer.inverse_transform(utils.to_data(label_X)).squeeze())\n","            labels_Y_list=np.append(labels_Y_list, Y_normalizer.inverse_transform(utils.to_data(label_Y)).squeeze())\n","            # preds_X2Y_list=np.append(preds_X2Y_list, np.exp(utils.to_data(pred_fake_Y))-1)\n","            # preds_Y2X_list=np.append(preds_Y2X_list, np.exp(utils.to_data(pred_fake_X))-1)\n","            # preds_X_list=np.append(preds_X_list, np.exp(utils.to_data(pred_real_X))-1)\n","            # preds_Y_list=np.append(preds_Y_list, np.exp(utils.to_data(pred_real_Y))-1)\n","            # labels_X_list=np.append(labels_X_list, np.exp(utils.to_data(label_X))-1)\n","            # labels_Y_list=np.append(labels_Y_list, np.exp(utils.to_data(label_Y))-1)\n","            time_X_list=np.append(time_X_list, utils.to_data(c_X).squeeze())\n","            time_Y_list=np.append(time_Y_list, utils.to_data(c_Y).squeeze())\n","\n","            # 1. Generate domain-X-like images based on real images in domain Y\n","            fake_X = G_YtoX(images_Y, c_X)\n","\n","            # 2. Compute the generator loss based on domain X\n","            g_loss = torch.mean((D_X(fake_X, c_X)-1)**2)\n","\n","            if opts.use_cycle_consistency_loss:\n","                # 3. Compute the cycle consistency loss (the reconstruction loss)\n","                cycle_consistency_loss = torch.mean(torch.abs(images_Y-G_XtoY(fake_X, c_Y)))\n","\n","                g_loss += opts.lambda_cycle * cycle_consistency_loss\n","\n","            # X--Y-->X CYCLE\n","            # 1. Generate domain-Y-like images based on real images in domain X\n","            fake_Y = G_XtoY(images_X, c_Y)\n","\n","            # 2. Compute the generator loss based on domain Y\n","            g_loss += torch.mean((D_Y(fake_Y, c_Y)-1)**2)\n","\n","            if opts.use_cycle_consistency_loss:\n","                # 3. Compute the cycle consistency loss (the reconstruction loss)\n","                cycle_consistency_loss = torch.mean(torch.abs(images_X - G_YtoX(fake_Y, c_X)))\n","\n","                g_loss += opts.lambda_cycle * cycle_consistency_loss\n","\n","            pred_fake_T = Cls_Y(fake_Y, c_Y)\n","            Cls_S2T_loss = torch.mean((pred_fake_T-label_X)**2)\n","            pred_fake_S = Cls_X(fake_X, c_X)\n","            Cls_T2S_loss = torch.mean((pred_fake_S-label_Y)**2)\n","            if opts.use_cycle_consistency_loss:\n","                Cls_cycle_loss = torch.mean((Cls_Y(G_XtoY(fake_X, c_Y),c_Y)-label_Y)**2) + torch.mean((Cls_X(G_YtoX(fake_Y, c_X),c_X)-label_X)**2)\n","\n","            g_loss+=Cls_S2T_loss + Cls_T2S_loss + Cls_cycle_loss\n","\n","            d_real_loss_hist += d_real_loss\n","            D_Y_loss_hist += D_Y_loss\n","            D_X_loss_hist += D_X_loss\n","            d_fake_loss_hist += d_fake_loss\n","            g_loss_hist += g_loss\n","            Cls_Y_loss_hist += Cls_Y_loss\n","            Cls_X_loss_hist += Cls_X_loss\n","            Cls_Y2X_loss_hist += Cls_Y2X_loss\n","            Cls_X2Y_loss_hist += Cls_X2Y_loss\n","            Cls_Y2X_aug_loss_hist += Cls_Y2X_aug_loss\n","            Cls_X2Y_aug_loss_hist += Cls_X2Y_aug_loss\n","            ## Save the generated samples\n","            #save_samples_test(iteration, images_Y, c_Y, id_Y, label_Y, images_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts)\n","            #save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_fixed_Y, label_fixed_Y, fixed_X, c_fixed_X, id_fixed_X, label_fixed_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts)\n","\n","    print(\n","        'Iteration [{}] | d_real_loss: {:6.4f} | '\n","        'd_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n","        'd_fake_loss: {:6.4f} | g_loss: {:6.4f} | Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}'\n","        '| Cls_Y2X_loss: {:6.4f} | Cls_X2Y_loss: {:6.4f} | Cls_Y2X_aug_loss: {:6.4f} | Cls_X2Y_aug_loss: {:6.4f}'.format(\n","            iter_per_epoch, d_real_loss_hist.item()/iter_per_epoch,\n","            D_Y_loss_hist.item()/iter_per_epoch, D_X_loss_hist.item()/iter_per_epoch,\n","            d_fake_loss_hist.item()/iter_per_epoch, g_loss_hist.item()/iter_per_epoch,\n","            Cls_Y_loss_hist.item()/iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_loss_hist.item()/iter_per_epoch, Cls_X2Y_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_aug_loss_hist.item()/iter_per_epoch, Cls_X2Y_aug_loss_hist.item()/iter_per_epoch\n","\n","        ))\n","\n","    print(\"Error_X\", np.mean(np.abs(labels_X_list-preds_X_list)))\n","    NSE=1-np.sum((labels_X_list-preds_X_list)**2)/np.sum((labels_X_list-np.mean(labels_X_list))**2)\n","    print('NSE',NSE)\n","\n","    loss_all = 'Iteration [{}] | d_real_loss: {:6.4f} | ''d_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n","    'd_fake_loss: {:6.4f} | g_loss: {:6.4f} | Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}| '\n","    'Cls_Y2X_loss: {:6.4f} | Cls_X2Y_loss: {:6.4f} | Cls_Y2X_aug_loss: {:6.4f} | Cls_X2Y_aug_loss: {:6.4f}'.format(\n","            iter_per_epoch, d_real_loss_hist.item()/iter_per_epoch,\n","            D_Y_loss_hist.item()/iter_per_epoch, D_X_loss_hist.item()/iter_per_epoch,\n","            d_fake_loss_hist.item()/iter_per_epoch, g_loss_hist.item()/iter_per_epoch,\n","            Cls_Y_loss_hist.item()/iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_loss_hist.item()/iter_per_epoch, Cls_X2Y_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_aug_loss_hist.item()/iter_per_epoch, Cls_X2Y_aug_loss_hist.item()/iter_per_epoch)\n","\n","#     torch.save({'Cls_Y_loss': Cls_Y_loss_hist.item()/iter_per_epoch, 'Cls_X_loss':Cls_X_loss_hist.item()/iter_per_epoch,\n","#             'Cls_Y2X_loss': Cls_Y2X_loss_hist.item()/iter_per_epoch, 'Cls_X2Y_loss':Cls_X2Y_loss_hist.item()/iter_per_epoch,\n","#             'Cls_Y2X_aug':Cls_Y2X_aug_loss_hist.item()/iter_per_epoch, 'Cls_X2Y_aug':Cls_X2Y_aug_loss_hist.item()/iter_per_epoch}, opts.checkpoint_dir+\"/loss_best.pth\")\n","\n","    return labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all\n","\n","def main(opts):\n","    \"\"\"Loads the data and starts the training loop.\"\"\"\n","    cur_path = '/data/nak168/spatial_temporal/stream_img/data/fpe-westbrook/'\n","#     for fol_X in [\"West Brook Lower_01171090\"]:\n","    for fol_X in [\"West Brook Reservoir_01171020\"]:\n","        for fol_Y in [\"West Brook Lower_01171090\"]:\n","#         for fol_Y in [\"Avery Brook_River Right_01171000\"]:\n","#         for fol_Y in [\"Avery Brook_River Left_01171000\"]:\n","        # for fol_Y in [\"Obear Brook Lower_01171070\"]:\n","            X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","            Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","            # # Create  dataloaders for images from the two domains X and Y\n","            dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            # dataset = ImageNetVidDataset(path= cur_path, fol_A=fol_X, fol_B=fol_Y, phase=\"train\", len_seq=len_seq, transform=transform)\n","            # dataset_val = ImageNetVidDataset(path= cur_path, fol_A=fol_A, fol_B=fol_B, phase=\"val\", len_seq=len_seq, transform=transform)\n","            # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","\n","            # Create checkpoint and sample directories\n","            utils.create_dir(opts.sample_dir)\n","\n","            # Start training\n","            labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all = test_loop(dataloader_X, dataloader_Y, X.normalizer, Y.normalizer, opts)\n","            # training_loop(dataloader, opts)\n","            time_X_list=[np.datetime64(int(cur), \"s\") for cur in time_X_list]\n","            time_Y_list=[np.datetime64(int(cur), \"s\") for cur in time_Y_list]\n","    return labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all\n","\n","if __name__ == '__main__':\n","\n","    opts.sample_dir = os.path.join(\n","        '/data/nak168/spatial_temporal/stream_img/CGAN/', 'output/'\n","        '%s_%s_%g_%g_%g/' % (opts.X.split('/')[0], opts.Y.split('/')[0], opts.mask_frac, opts.lambda_cls, opts.lambda_cycle_cls)\n","    )\n","    opts.sample_dir += '%s_%s_%s_%s_%g_%s' % (\n","        opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.lambda_cycle, opts.init_type\n","    )\n","    if opts.use_cycle_consistency_loss:\n","        opts.sample_dir += '_cycle'\n","    if opts.use_diffaug:\n","        opts.sample_dir += '_diffaug'\n","\n","    opts.sample_dir = os.path.join(\n","        opts.sample_dir, 'test/'\n","    )\n","\n","    if os.path.exists(opts.sample_dir):\n","        cmd = 'rm %s/*' % opts.sample_dir\n","        os.system(cmd)\n","\n","    batch_size=opts.batch_size\n","#     len_seq=opts.len_seq\n","\n","    labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all = main(opts)\n"],"metadata":{"id":"_BFombPRE6f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# one2many\n","\n","from matplotlib import axis\n","def loadcheckpoint(opts, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y):\n","    \"\"\"Save generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\"\"\"\n","\n","    G_XtoY_path = os.path.join(\n","        opts.checkpoint_dir, 'G_XtoY-best.pkl'\n","    )\n","    G_YtoX_path = os.path.join(\n","        opts.checkpoint_dir, 'G_YtoX-best.pkl'\n","    )\n","    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X-best.pkl')\n","    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y-best.pkl')\n","    Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X-best.pkl')\n","    Cls_Y_path = os.path.join(opts.checkpoint_dir, 'Cls_Y-best.pkl')\n","\n","    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=\"cuda\"))\n","    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=\"cuda\"))\n","    D_X.load_state_dict(torch.load(D_X_path, map_location=\"cuda\"))\n","    D_Y.load_state_dict(torch.load(D_Y_path, map_location=\"cuda\"))\n","    Cls_X.load_state_dict(torch.load(Cls_X_path, map_location=\"cuda\"))\n","    Cls_Y.load_state_dict(torch.load(Cls_Y_path, map_location=\"cuda\"))\n","    return G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y\n","\n","def save_samples_test(iteration, fixed_Y, c_Y, id_Y, label_Y, fixed_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y, tp=\"all\")\n","    save_images(Y, c_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X, tp=\"all\")\n","\n","def save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_Y, label_Y, fixed_X, c_fixed_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts):\n","    fake_X = G_YtoX(fixed_Y, c_X)\n","    fake_Y = G_XtoY(fixed_X, c_Y)\n","    pred_Y = Cls_Y(fake_Y, c_Y)\n","    pred_X = Cls_X(fake_X, c_X)\n","\n","    X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), X_normalizer.inverse_transform(utils.to_data(label_X)), X_normalizer.inverse_transform(utils.to_data(pred_X))\n","    Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), Y_normalizer.inverse_transform(utils.to_data(label_Y)), Y_normalizer.inverse_transform(utils.to_data(pred_Y))\n","    # X, fake_X, label_X, pred_X = utils.to_data(fixed_X), utils.to_data(fake_X), np.exp(utils.to_data(label_X))-1, np.exp(utils.to_data(pred_X)-1)\n","    # Y, fake_Y, label_Y, pred_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y), np.exp(utils.to_data(label_Y))-1, np.exp(utils.to_data(pred_Y)-1)\n","    # dates_Y=np.datetime_as_string(utils.to_data(c_Y), timezone='UTC')\n","    # dates_X=np.datetime_as_string(utils.to_data(c_X), timezone='UTC')\n","    save_images(X, c_fixed_X, label_X, fake_Y, c_Y, pred_Y, opts, iteration,\"X_Y\", id_X, id_Y)\n","    save_images(Y, c_fixed_Y, label_Y, fake_X, c_X, pred_X, opts, iteration,\"Y_X\", id_Y, id_X)\n","\n","def test_loop(dataloader_X, dataloader_Y, X_normalizer, Y_normalizer, opts):\n","    \"\"\"Runs the training loop.\n","        * Saves checkpoint every opts.checkpoint_every iterations\n","        * Saves generated samples every opts.sample_every iterations\n","    \"\"\"\n","\n","    # # Create generators and discriminators\n","    cx = EasyDict() # Main config dict.\n","    cond_desc(cx, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_X))\n","    cy = EasyDict() # Main config dict.\n","    cond_desc(cy, opts, cond_type=opts.cond, noise=opts.noise, frames=len(dataloader_Y))\n","\n","    i=0\n","    dct={}\n","    wei_total=0\n","    for subdir in os.listdir('/data/nak168/spatial_temporal/stream_img/CGAN/output'):\n","        if f\"_{opts.mask_frac}_1_5\" in subdir:\n","            for f in os.listdir('/data/nak168/spatial_temporal/stream_img/CGAN/output/'+subdir):\n","                if \"cycle_time_me\" not in f:\n","                    continue\n","                opts.checkpoint_dir='/data/nak168/spatial_temporal/stream_img/CGAN/output/'+subdir+\"/\"+f+\"/checkpoints_cyclegan\"\n","\n","                loss=torch.load(os.path.join(opts.checkpoint_dir, \"loss_best.pth\"))\n","                dct[f'wei_{i}']=1/(loss['Cls_X_loss']+loss['Cls_X2Y_loss'])\n","#                 dct[f'wei_{i}']=1\n","                wei_total+=dct[f'wei_{i}']\n","                dct[f'G_XtoY_{i}'], dct[f'G_YtoX_{i}'], dct[f'D_X_{i}'], dct[f'D_Y_{i}'], dct[f'Cls_X_{i}'], dct[f'Cls_Y_{i}'] = create_model(opts,cx,cy)\n","                dct[f'G_XtoY_{i}'], dct[f'G_YtoX_{i}'], dct[f'D_X_{i}'], dct[f'D_Y_{i}'], dct[f'Cls_X_{i}'], dct[f'Cls_Y_{i}'] = loadcheckpoint(opts,dct[f'G_XtoY_{i}'], dct[f'G_YtoX_{i}'], dct[f'D_X_{i}'], dct[f'D_Y_{i}'], dct[f'Cls_X_{i}'], dct[f'Cls_Y_{i}'])\n","                dct[f'G_XtoY_{i}'].eval(), dct[f'G_YtoX_{i}'].eval(), dct[f'D_X_{i}'].eval(), dct[f'D_Y_{i}'].eval(), dct[f'Cls_X_{i}'].eval(), dct[f'Cls_Y_{i}'].eval()\n","                i+=1\n","    nums=i-1\n","\n","    iter_X = iter(dataloader_X)\n","    iter_Y = iter(dataloader_Y)\n","\n","    # Get some fixed data from domains X and Y for sampling.\n","    # These are images that are held constant throughout training,\n","    # that allow us to inspect the model's performance.\n","    fixed_X, c_fixed_X, label_fixed_X, id_fixed_X = next(iter_X)\n","    fixed_Y, c_fixed_Y, label_fixed_Y, id_fixed_Y = next(iter_Y)\n","\n","    fixed_X = fixed_X.squeeze(0)\n","    c_fixed_X = c_fixed_X.squeeze(0)\n","    label_fixed_X = label_fixed_X.squeeze(0)\n","#     label_fixed_X = label_fixed_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_X = label_fixed_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_X = label_fixed_X.reshape(label_fixed_X.shape[0] * label_fixed_X.shape[1], label_fixed_X.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_Y = fixed_Y.squeeze(0)\n","    c_fixed_Y = c_fixed_Y.squeeze(0)\n","    label_fixed_Y = label_fixed_Y.squeeze(0)\n","#     label_fixed_Y = label_fixed_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#     label_fixed_Y = label_fixed_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#     label_fixed_Y = label_fixed_Y.reshape(label_fixed_Y.shape[0] * label_fixed_Y.shape[1], label_fixed_Y.shape[2])  # (seq_len * N, 1)\n","\n","    fixed_c_X = dequant(c_fixed_X)\n","    fixed_c_Y = dequant(c_fixed_Y)\n","    fixed_X = utils.to_var(fixed_X)\n","    fixed_Y = utils.to_var(fixed_Y)\n","    label_fixed_X = utils.to_var(label_fixed_X)\n","    label_fixed_Y = utils.to_var(label_fixed_Y)\n","    c_fixed_X = utils.to_var(c_fixed_X)\n","    c_fixed_Y = utils.to_var(c_fixed_Y)\n","\n","    iter_per_epoch_X, iter_per_epoch_Y = len(iter_X), len(iter_Y)\n","    iter_per_epoch=min(iter_per_epoch_X, iter_per_epoch_Y)\n","    d_real_loss_hist = D_Y_loss_hist = D_X_loss_hist = d_fake_loss_hist = g_loss_hist=0\n","    Cls_Y_loss_hist = Cls_X_loss_hist=Cls_Y2X_loss_hist = Cls_X2Y_loss_hist=0\n","    labels_X_list=np.array([])\n","    labels_Y_list=np.array([])\n","    preds_X_list=np.array([])\n","    preds_Y_list=np.array([])\n","    preds_X2Y_list=np.array([])\n","    preds_Y2X_list=np.array([])\n","    time_X_list=np.array([])\n","    time_Y_list=np.array([])\n","\n","    with torch.no_grad():\n","        for iteration in range(1, min(iter_per_epoch_X, iter_per_epoch_Y)):\n","            # Reset data_iter for each epoch\n","            if iteration % iter_per_epoch_X == 0:\n","                iter_X = iter(dataloader_X)\n","            if iteration % iter_per_epoch_Y == 0:\n","                iter_Y = iter(dataloader_Y)\n","\n","            images_X, c_X, label_X, id_X = next(iter_X)\n","            images_Y, c_Y, label_Y, id_Y = next(iter_Y)\n","\n","            images_X = images_X.squeeze(0)\n","            c_X = c_X.squeeze(0)\n","            label_X = label_X.squeeze(0)\n","#             label_X = label_X.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_X = label_X.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_X = label_X.reshape(label_X.shape[0] * label_X.shape[1], label_X.shape[2])  # (seq_len * N, 1)\n","\n","            images_Y = images_Y.squeeze(0)\n","            c_Y = c_Y.squeeze(0)\n","            label_Y = label_Y.squeeze(0)\n","#             label_Y = label_Y.reshape(opts.batch_size, opts.len_seq, 1)\n","#             label_Y = label_Y.permute(1, 0, 2)  # (seq_len, N, 1)\n","#             label_Y = label_Y.reshape(label_Y.shape[0] * label_Y.shape[1], label_Y.shape[2])  # (seq_len * N, 1)\n","\n","            c_X = dequant(c_X)\n","            c_Y = dequant(c_Y)\n","            images_X = utils.to_var(images_X)\n","            label_X = utils.to_var(label_X)\n","            images_Y = utils.to_var(images_Y)\n","            label_Y = utils.to_var(label_Y)\n","            c_X = utils.to_var(c_X)\n","            c_Y = utils.to_var(c_Y)\n","\n","\n","            pred_fake_Y_avg=0\n","            pred_fake_X_avg=0\n","            pred_real_Y_avg=0\n","            pred_real_X_avg=0\n","#             wei_total=0\n","            i=0\n","\n","            for subdir in os.listdir('/data/nak168/spatial_temporal/stream_img/CGAN/output'):\n","                if f\"_{opts.mask_frac}_1_5\" in subdir:\n","                    for f in os.listdir('/data/nak168/spatial_temporal/stream_img/CGAN/output/'+subdir):\n","                        if \"cycle_time_me\" not in f:\n","                            continue\n","\n","            #             # 1. Compute the discriminator losses on real images\n","            #             D_X_loss = torch.mean((D_X(images_X, c_X)-1)**2)\n","            #             D_Y_loss = torch.mean((D_Y(images_Y, c_Y)-1)**2)\n","\n","            #             d_real_loss = D_X_loss + D_Y_loss\n","\n","                        # 2. Generate domain-X-like images based on real images in domain Y\n","                        fake_X = dct[f'G_YtoX_{i}'](images_Y, c_X)\n","\n","                        # 3. Compute the loss for D_X\n","                        D_fake_X_loss = torch.mean(dct[f'D_X_{i}'](fake_X.detach(), c_X)**2)\n","\n","                        # 4. Generate domain-Y-like images based on real images in domain X\n","                        fake_Y=dct[f'G_XtoY_{i}'](images_X, c_Y)\n","\n","        #                 # 5. Compute the loss for D_Y\n","        #                 D_fake_Y_loss = torch.mean(f'D_Y_{i}'(fake_Y.detach(), c_Y)**2)\n","\n","        #                 d_fake_loss = D_fake_X_loss + D_fake_Y_loss\n","        #                 d_total_loss = d_real_loss + d_fake_loss\n","\n","                        # Class loss\n","                        pred_fake_Y = dct[f'Cls_X_{i}'](fake_Y.detach(), c_Y)\n","                        Cls_X2Y_loss = torch.mean((pred_fake_Y-label_X)**2)\n","                        pred_real_X = dct[f'Cls_X_{i}'](images_X, c_X)\n","\n","                        pred_real_X_avg+=pred_real_X*(wei_total-dct[f'wei_{i}'])\n","#                         pred_real_X_avg+=pred_real_X*dct[f'wei_{i}']\n","\n","                        lbl_X=utils.to_var(torch.from_numpy(X_normalizer.inverse_transform(utils.to_data(label_X))))\n","                        pred_real_X=utils.to_var(torch.from_numpy(X_normalizer.inverse_transform(utils.to_data(pred_real_X))))\n","\n","#                         Cls_X_loss = torch.sqrt(criterion_cls(pred_real_X,lbl_X))\n","                        Cls_X_loss = torch.mean((pred_real_X-lbl_X)**2)\n","\n","                        pred_fake_X = dct[f'Cls_Y_{i}'](fake_X.detach(), c_X)\n","                        Cls_Y2X_loss = torch.mean((pred_fake_X-label_Y)**2)\n","                        pred_real_Y = dct[f'Cls_Y_{i}'](images_Y, c_Y)\n","\n","                        pred_real_Y_avg+=pred_real_Y*(wei_total-dct[f'wei_{i}'])\n","#                         pred_real_Y_avg+=pred_real_Y*dct[f'wei_{i}']\n","\n","                        lbl_Y=utils.to_var(torch.from_numpy(Y_normalizer.inverse_transform(utils.to_data(label_Y))))\n","                        pred_real_Y=utils.to_var(torch.from_numpy(Y_normalizer.inverse_transform(utils.to_data(pred_real_Y))))\n","\n","                        Cls_Y_loss = torch.sqrt(criterion_cls(pred_real_Y,lbl_Y))\n","#                         Cls_Y_loss = torch.mean((pred_real_Y-label_Y)**2)\n","\n","                        Cls_total_loss=(Cls_X2Y_loss + Cls_Y_loss + Cls_Y2X_loss + Cls_X_loss) * opts.lambda_cls\n","\n","                        pred_fake_Y_avg+=pred_fake_Y*(wei_total-dct[f'wei_{i}'])\n","                        pred_fake_X_avg+=pred_fake_X*(wei_total-dct[f'wei_{i}'])\n","#                         pred_fake_Y_avg+=pred_fake_Y*dct[f'wei_{i}']\n","#                         pred_fake_X_avg+=pred_fake_X*dct[f'wei_{i}']\n","\n","                        # pred_fake_Y_avg=pred_fake_Y_avg+pred_fake_Y\n","                        # pred_fake_X_avg=pred_fake_X_avg+pred_fake_X\n","                        # pred_real_Y_avg=pred_real_Y_avg+pred_real_Y\n","                        # pred_real_X_avg=pred_real_X_avg+pred_real_X\n","                        i+=1\n","\n","            pred_fake_Y_avg/=nums*wei_total\n","            pred_fake_X_avg/=nums*wei_total\n","            pred_real_Y_avg/=nums*wei_total\n","            pred_real_X_avg/=nums*wei_total\n","#             pred_fake_Y_avg=pred_fake_Y_avg/i\n","#             pred_fake_X_avg=pred_fake_X_avg/i\n","#             pred_real_Y_avg=pred_real_Y_avg/i\n","#             pred_real_X_avg=pred_real_X_avg/i\n","\n","            preds_X2Y_list=np.append(preds_X2Y_list, Y_normalizer.inverse_transform(utils.to_data(pred_fake_Y_avg)).squeeze())\n","            preds_Y2X_list=np.append(preds_Y2X_list, X_normalizer.inverse_transform(utils.to_data(pred_fake_X_avg)).squeeze())\n","            preds_X_list=np.append(preds_X_list, X_normalizer.inverse_transform(utils.to_data(pred_real_X_avg)).squeeze())\n","            preds_Y_list=np.append(preds_Y_list, Y_normalizer.inverse_transform(utils.to_data(pred_real_Y_avg)).squeeze())\n","            labels_X_list=np.append(labels_X_list, X_normalizer.inverse_transform(utils.to_data(label_X)).squeeze())\n","            labels_Y_list=np.append(labels_Y_list, Y_normalizer.inverse_transform(utils.to_data(label_Y)).squeeze())\n","            # preds_X2Y_list=np.append(preds_X2Y_list, np.exp(utils.to_data(pred_fake_Y))-1)\n","            # preds_Y2X_list=np.append(preds_Y2X_list, np.exp(utils.to_data(pred_fake_X))-1)\n","            # preds_X_list=np.append(preds_X_list, np.exp(utils.to_data(pred_real_X))-1)\n","            # preds_Y_list=np.append(preds_Y_list, np.exp(utils.to_data(pred_real_Y))-1)\n","            # labels_X_list=np.append(labels_X_list, np.exp(utils.to_data(label_X))-1)\n","            # labels_Y_list=np.append(labels_Y_list, np.exp(utils.to_data(label_Y))-1)\n","            time_X_list=np.append(time_X_list, utils.to_data(c_X).squeeze())\n","            time_Y_list=np.append(time_Y_list, utils.to_data(c_Y).squeeze())\n","\n","\n","            Cls_Y_loss_hist += Cls_Y_loss\n","            Cls_X_loss_hist += Cls_X_loss\n","            Cls_Y2X_loss_hist += Cls_Y2X_loss\n","            Cls_X2Y_loss_hist += Cls_X2Y_loss\n","            ## Save the generated samples\n","            #save_samples_test(iteration, images_Y, c_Y, id_Y, label_Y, images_X, c_X, id_X, label_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, X_normalizer, Y_normalizer, opts)\n","            #save_samples_fixed(iteration, fixed_Y, c_fixed_Y, id_fixed_Y, label_fixed_Y, fixed_X, c_fixed_X, id_fixed_X, label_fixed_X, G_YtoX, G_XtoY, Cls_X, Cls_Y, c_Y, c_X, X_normalizer, Y_normalizer, opts)\n","\n","    print(\n","        'Iteration [{}] |'\n","        'Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}| Cls_Y2X_loss: {:6.4f} | Cls_X2Y_loss: {:6.4f}'.format(\n","            iter_per_epoch,\n","            Cls_Y_loss_hist.item()/iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_loss_hist.item()/iter_per_epoch, Cls_X2Y_loss_hist.item()/iter_per_epoch\n","\n","        ))\n","\n","    print(\"Error_X\", np.mean(np.abs(labels_X_list-preds_X_list)))\n","    NSE=1-np.sum((labels_X_list-preds_X_list)**2)/np.sum((labels_X_list-np.mean(labels_X_list))**2)\n","    print('NSE',NSE)\n","\n","    loss_all = 'Iteration [{}] | Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}| Cls_Y2X_loss: {:6.4f} | Cls_X2Y_loss: {:6.4f}'.format(\n","            iter_per_epoch,\n","            Cls_Y_loss_hist.item()/iter_per_epoch, Cls_X_loss_hist.item()/iter_per_epoch,\n","            Cls_Y2X_loss_hist.item()/iter_per_epoch, Cls_X2Y_loss_hist.item()/iter_per_epoch)\n","\n","\n","    return labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all\n","\n","def main(opts):\n","    \"\"\"Loads the data and starts the training loop.\"\"\"\n","    cur_path = '/data/nak168/spatial_temporal/stream_img/data/fpe-westbrook/'\n","#     for fol_X in [\"West Brook Lower_01171090\"]:\n","    for fol_X in [\"West Brook Reservoir_01171020\"]:\n","        for fol_Y in [\"West Brook Lower_01171090\"]:\n","#         for fol_Y in [\"Avery Brook_River Right_01171000\"]:\n","        # for fol_Y in [\"Avery Brook_River Left_01171000\"]:\n","#         for fol_Y in [\"Obear Brook Lower_01171070\"]:\n","            X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","            Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"test\", len_seq=opts.batch_size*opts.len_seq, transform=transform)\n","\n","            # # Create  dataloaders for images from the two domains X and Y\n","            dataloader_X = torch.utils.data.DataLoader(X, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n","            # dataset = ImageNetVidDataset(path= cur_path, fol_A=fol_X, fol_B=fol_Y, phase=\"train\", len_seq=len_seq, transform=transform)\n","            # dataset_val = ImageNetVidDataset(path= cur_path, fol_A=fol_A, fol_B=fol_B, phase=\"val\", len_seq=len_seq, transform=transform)\n","            # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","\n","            # Create checkpoint and sample directories\n","            utils.create_dir(opts.sample_dir)\n","\n","            # Start training\n","            labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all = test_loop(dataloader_X, dataloader_Y, X.normalizer, Y.normalizer, opts)\n","            # training_loop(dataloader, opts)\n","            time_X_list=[np.datetime64(int(cur), \"s\") for cur in time_X_list]\n","            time_Y_list=[np.datetime64(int(cur), \"s\") for cur in time_Y_list]\n","    return labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all\n","\n","if __name__ == '__main__':\n","\n","    opts.sample_dir = os.path.join(\n","        '/data/nak168/spatial_temporal/stream_img/CGAN/', 'output_one2many/'\n","        '%s_%s_%g_%g_%g/' % (opts.X.split('/')[0], opts.Y.split('/')[0], opts.mask_frac, opts.lambda_cls, opts.lambda_cycle_cls)\n","    )\n","    opts.sample_dir += '%s_%s_%s_%s_%g_%s' % (\n","        opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.lambda_cycle, opts.init_type\n","    )\n","    if opts.use_cycle_consistency_loss:\n","        opts.sample_dir += '_cycle'\n","    if opts.use_diffaug:\n","        opts.sample_dir += '_diffaug'\n","\n","    opts.sample_dir = os.path.join(\n","        opts.sample_dir, 'test/'\n","    )\n","\n","    if os.path.exists(opts.sample_dir):\n","        cmd = 'rm %s/*' % opts.sample_dir\n","        os.system(cmd)\n","\n","    batch_size=opts.batch_size\n","#     len_seq=opts.len_seq\n","\n","    labels_X_list, labels_Y_list, preds_X_list, preds_Y_list, preds_X2Y_list, preds_Y2X_list, time_X_list, time_Y_list, loss_all = main(opts)\n"],"metadata":{"id":"166MVxXHSbD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# time_X_list=[np.datetime64(int(cur), \"s\") for cur in time_X_list]\n","# time_Y_list=[np.datetime64(int(cur), \"s\") for cur in time_Y_list]\n","\n","fig=plt.figure()\n","plt.plot(time_X_list,preds_X_list, label=\"Prediction_temp_X\",color='blue')\n","plt.plot(time_X_list,labels_X_list, label=\"Original_temp_X\",color='red')\n","plt.tick_params(axis='x', rotation=45,labelsize=10)\n","plt.ylabel(\"Flow\")\n","plt.xlabel(\"time\")\n","plt.title('X2X')\n","plt.legend()\n","\n","fig=plt.figure()\n","plt.plot(time_Y_list,preds_Y_list, label=\"Prediction_temp_Y\",color='blue')\n","plt.plot(time_Y_list,labels_Y_list, label=\"Original_temp_Y\",color='red')\n","plt.tick_params(axis='x', rotation=45,labelsize=10)\n","plt.ylabel(\"Flow\")\n","plt.xlabel(\"time\")\n","plt.title('Y2Y')\n","plt.legend()\n","\n","\n","fig=plt.figure()\n","plt.plot(time_Y_list,preds_Y2X_list, label=\"Prediction_temp_Y2X\",color='blue')\n","plt.plot(time_Y_list,labels_Y_list, label=\"Original_temp_Y\",color='red')\n","plt.tick_params(axis='x', rotation=45,labelsize=10)\n","plt.ylabel(\"Flow\")\n","plt.xlabel(\"time\")\n","plt.title('Y2X')\n","plt.legend()\n","\n","fig=plt.figure()\n","plt.plot(time_X_list,preds_X2Y_list, label=\"Prediction_temp_X2Y\",color='blue')\n","plt.plot(time_X_list,labels_X_list, label=\"Original_temp_X\",color='red')\n","plt.tick_params(axis='x', rotation=45,labelsize=10)\n","plt.ylabel(\"Flow\")\n","plt.xlabel(\"time\")\n","plt.title('X2Y')\n","plt.legend()"],"metadata":{"id":"aN7q8eqF4U3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save(opts.sample_dir + 'labels_X.npy',lbl_X_list)\n","np.save(opts.sample_dir + 'labels_Y.npy',lbl_Y_list)\n","np.save(opts.sample_dir + 'preds_X2Y.npy',preds_X2Y_list)\n","np.save(opts.sample_dir + 'preds_Y2X.npy',preds_Y2X_list)\n","np.save(opts.sample_dir + 'preds_X.npy',preds_X_list)\n","np.save(opts.sample_dir + 'preds_Y.npy',preds_Y_list)\n","np.save(opts.sample_dir + 'time_X.npy',time_X_list)\n","np.save(opts.sample_dir + 'time_Y.npy',time_Y_list)\n","np.save(opts.sample_dir + 'loss_all.npy',loss_all)"],"metadata":{"id":"jX1PgOV54fqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lbl_X_list=np.load(opts.sample_dir + 'labels_X.npy')\n","lbl_Y_list=np.load(opts.sample_dir + 'labels_Y.npy')\n","prd_X2Y_list=np.load(opts.sample_dir + 'preds_X2Y.npy')\n","prd_Y2X_list=np.load(opts.sample_dir + 'preds_Y2X.npy')\n","prd_X_list=np.load(opts.sample_dir + 'preds_X.npy')\n","prd_Y_list=np.load(opts.sample_dir + 'preds_Y.npy')\n","time_X_list=np.load(opts.sample_dir + 'time_X.npy')\n","time_Y_list=np.load(opts.sample_dir + 'time_Y.npy')\n","loss_all=np.load(opts.sample_dir + 'loss_all.npy')"],"metadata":{"id":"kdhY9ehX4gad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lsof -i :<port_number> | awk 'NR!=1 {print $2}' | xargs -r kill\n","# ps -aux | grep <port_number>\n","%load_ext tensorboard\n","%tensorboard --logdir '/data/nak168/spatial_temporal/stream_img/CGAN/output/_5basic_instance_patch_cycle_time_me_naive_cycle_diffaug/' --port=6010"],"metadata":{"id":"rDGSXJEr4jDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # paired\n","# from pickle import NONE\n","# # CMU 16-726 Learning-Based Image Synthesis / Spring 2023, Assignment 3\n","# #\n","# # The code base is based on the great work from CSC 321, U Toronto\n","# # https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip\n","# # This is the main training file for the second part of the assignment.\n","# #\n","# # Usage:\n","# # ======\n","# #    To train with the default hyperparamters:\n","# #       python cycle_gan.py\n","# #\n","# #    To train with cycle consistency loss:\n","# #       python cycle_gan.py --use_cycle_consistency_loss\n","# #\n","# #\n","# #    For optional experimentation:\n","# #    -----------------------------\n","# #    If you have a powerful computer (ideally with a GPU),\n","# #    then you can obtain better results by\n","# #    increasing the number of filters used in the generator\n","# #    and/or discriminator, as follows:\n","# #      python cycle_gan.py --g_conv_dim=64 --d_conv_dim=64\n","\n","# import argparse\n","# import os\n","\n","# import imageio\n","# from torch.utils.tensorboard import SummaryWriter\n","# import torch\n","# import torch.optim as optim\n","# import numpy as np\n","\n","# import utils\n","# from data_loader import get_data_loader\n","# from models import CycleGenerator, DCDiscriminator, PatchDiscriminator\n","# from diff_augment import DiffAugment\n","# from model_classifier import deeplabv2_resnet101\n","# policy = 'color,translation,cutout'\n","\n","\n","# SEED = 11\n","\n","# # Set the random seed manually for reproducibility.\n","# np.random.seed(SEED)\n","# torch.manual_seed(SEED)\n","# if torch.cuda.is_available():\n","#     torch.cuda.manual_seed(SEED)\n","\n","\n","# def print_models(G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y):\n","#     \"\"\"Prints model information for the generators and discriminators.\n","#     \"\"\"\n","#     print(\"                 G_XtoY                \")\n","#     print(\"---------------------------------------\")\n","#     print(G_XtoY)\n","#     print(\"---------------------------------------\")\n","\n","#     print(\"                 G_YtoX                \")\n","#     print(\"---------------------------------------\")\n","#     print(G_YtoX)\n","#     print(\"---------------------------------------\")\n","\n","#     print(\"                  D_X                  \")\n","#     print(\"---------------------------------------\")\n","#     print(D_X)\n","#     print(\"---------------------------------------\")\n","\n","#     print(\"                  D_Y                  \")\n","#     print(\"---------------------------------------\")\n","#     print(D_Y)\n","#     print(\"---------------------------------------\")\n","\n","#     print(\"                  Cls_X                  \")\n","#     print(\"---------------------------------------\")\n","#     print(Cls_X)\n","#     print(\"---------------------------------------\")\n","\n","#     print(\"                  Cls_Y                  \")\n","#     print(\"---------------------------------------\")\n","#     print(Cls_Y)\n","#     print(\"---------------------------------------\")\n","\n","# def create_model(opts):\n","#     \"\"\"Builds the generators and discriminators.\n","#     \"\"\"\n","#     model_dict = {'cycle': CycleGenerator}\n","#     G_XtoY = model_dict[opts.gen](conv_dim=opts.g_conv_dim, norm=opts.norm)\n","#     G_YtoX = model_dict[opts.gen](conv_dim=opts.g_conv_dim, norm=opts.norm)\n","\n","#     model_dict = {'dc': DCDiscriminator, 'patch': PatchDiscriminator}\n","#     D_X = model_dict[opts.disc](conv_dim=opts.d_conv_dim, norm=opts.norm)\n","#     D_Y = model_dict[opts.disc](conv_dim=opts.d_conv_dim, norm=opts.norm)\n","\n","#     model_dict = {'deep_resnet34': deep_resnet34, \"deeplabv2_resnet101\": deeplabv2_resnet101 }\n","#     Cls_X = model_dict[opts.classifier]()\n","#     Cls_Y = model_dict[opts.classifier]()\n","\n","#     print_models(G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y)\n","\n","#     if torch.cuda.is_available():\n","#         G_XtoY.cuda()\n","#         G_YtoX.cuda()\n","#         D_X.cuda()\n","#         D_Y.cuda()\n","#         Cls_X.cuda()\n","#         Cls_Y.cuda()\n","\n","#         print('Models moved to GPU.')\n","\n","#     return G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y\n","\n","\n","# def checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y, opts):\n","#     \"\"\"Save generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\"\"\"\n","#     G_XtoY_path = os.path.join(\n","#         opts.checkpoint_dir, 'G_XtoY_iter%d.pkl' % iteration\n","#     )\n","#     G_YtoX_path = os.path.join(\n","#         opts.checkpoint_dir, 'G_YtoX_iter%d.pkl' % iteration\n","#     )\n","#     D_X_path = os.path.join(opts.checkpoint_dir, 'D_X_iter%d.pkl' % iteration)\n","#     D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y_iter%d.pkl' % iteration)\n","#     Cls_X_path = os.path.join(opts.checkpoint_dir, 'Cls_X_iter%d.pkl' % iteration)\n","#     Cls_Y_path = os.path.join(opts.checkpoint_dir, 'Cls_Y_iter%d.pkl' % iteration)\n","#     torch.save(G_XtoY.state_dict(), G_XtoY_path)\n","#     torch.save(G_YtoX.state_dict(), G_YtoX_path)\n","#     torch.save(D_X.state_dict(), D_X_path)\n","#     torch.save(D_Y.state_dict(), D_Y_path)\n","#     torch.save(Cls_X.state_dict(), Cls_X_path)\n","#     torch.save(Cls_Y.state_dict(), Cls_Y_path)\n","\n","\n","# def merge_images(sources, targets, opts, k=10):\n","#     \"\"\"\n","#     Creates a grid consisting of pairs of columns, where the first column in\n","#     each pair contains images source images and the second column in each pair\n","#     contains images generated by the CycleGAN from the corresponding images in\n","#     the first column.\n","#     \"\"\"\n","#     _, _, h, w = sources.shape\n","#     row = int(np.sqrt(opts.batch_size))\n","#     merged = np.zeros([3, row * h, row * w * 2])\n","#     for idx, (s, t) in enumerate(zip(sources, targets)):\n","#         i = idx // row\n","#         j = idx % row\n","#         merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n","#         merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n","#         # merged[:, i * h:(i + 1) * h, (j * 2) * w:(j * 2 + 1) * w] = s\n","#         # merged[:, i * h:(i + 1) * h, (j * 2 + 1) * w:(j * 2 + 2) * w] = t\n","#     return merged.transpose(1, 2, 0)\n","\n","\n","# def save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n","#     \"\"\"Saves samples from both generators X->Y and Y->X.\n","#     \"\"\"\n","#     fake_X = G_YtoX(fixed_Y)\n","#     fake_Y = G_XtoY(fixed_X)\n","\n","#     X, fake_X = utils.to_data(fixed_X), utils.to_data(fake_X)\n","#     Y, fake_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y)\n","\n","#     merged = merge_images(X, fake_Y, opts)\n","#     path = os.path.join(\n","#         opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration)\n","#     )\n","#     merged = np.uint8(255 * (merged + 1) / 2)\n","#     imageio.imwrite(path, merged)\n","#     print('Saved {}'.format(path))\n","\n","#     merged = merge_images(Y, fake_X, opts)\n","#     path = os.path.join(\n","#         opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration)\n","#     )\n","#     merged = np.uint8(255 * (merged + 1) / 2)\n","#     imageio.imwrite(path, merged)\n","#     print('Saved {}'.format(path))\n","\n","# # def training_loop(dataloader_X, dataloader_Y, opts):\n","# def training_loop(dataloader, opts):\n","#     \"\"\"Runs the training loop.\n","#         * Saves checkpoint every opts.checkpoint_every iterations\n","#         * Saves generated samples every opts.sample_every iterations\n","#     \"\"\"\n","#     # Create generators and discriminators\n","#     G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y = create_model(opts)\n","\n","#     g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())\n","#     d_params = list(D_X.parameters()) + list(D_Y.parameters())\n","#     Cls_params = list(Cls_X.parameters()) + list(Cls_Y.parameters())\n","\n","#     # Create optimizers for the generators and discriminators\n","#     g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n","#     d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n","#     Cls_optimizer = optim.Adam(Cls_params, opts.lr, [opts.beta1, opts.beta2])\n","\n","#     # iter_X = iter(dataloader_X)\n","#     # iter_Y = iter(dataloader_Y)\n","#     itr = iter(dataloader)\n","\n","#     # Get some fixed data from domains X and Y for sampling.\n","#     # These are images that are held constant throughout training,\n","#     # that allow us to inspect the model's performance.\n","#     # fixed_X, fixed_label_X = next(iter_X)\n","#     # fixed_Y, fixed_label_Y = next(iter_Y)\n","#     fixed_X, fixed_label_X, fixed_Y, fixed_label_Y = next(itr)\n","#     fixed_X = utils.to_var(fixed_X)\n","#     fixed_Y = utils.to_var(fixed_Y)\n","\n","#     # iter_per_epoch = min(len(iter_X), len(iter_Y))\n","#     iter_per_epoch = len(itr)\n","\n","#     for iteration in range(1, opts.train_iters + 1):\n","\n","#         # Reset data_iter for each epoch\n","#         if iteration % iter_per_epoch == 0:\n","#             # iter_X = iter(dataloader_X)\n","#             # iter_Y = iter(dataloader_Y)\n","#             itr = iter(dataloader)\n","\n","#         # images_X, label_X = next(iter_X)\n","#         # images_Y, label_Y = next(iter_Y)\n","#         images_X, label_X, images_Y, label_Y = next(itr)\n","#         images_X = utils.to_var(images_X)\n","#         label_X = utils.to_var(label_X)\n","#         images_Y = utils.to_var(images_Y)\n","#         label_Y = utils.to_var(label_Y)\n","\n","#         # TRAIN THE DISCRIMINATORS\n","#         # 1. Compute the discriminator losses on real images\n","#         D_X_loss = torch.mean((D_X(images_X)-1)**2)\n","#         D_Y_loss = torch.mean((D_Y(images_Y)-1)**2)\n","\n","#         d_real_loss = D_X_loss + D_Y_loss\n","\n","#         # 2. Generate domain-X-like images based on real images in domain Y\n","#         fake_X = G_YtoX(images_Y)\n","\n","#         # 3. Compute the loss for D_X\n","#         D_fake_X_loss = torch.mean(D_X(fake_X.detach())**2)\n","\n","#         # 4. Generate domain-Y-like images based on real images in domain X\n","#         fake_Y=G_XtoY(images_X)\n","\n","#         # 5. Compute the loss for D_Y\n","#         D_fake_Y_loss = torch.mean(D_Y(fake_Y.detach())**2)\n","\n","#         d_fake_loss = D_fake_X_loss + D_fake_Y_loss\n","#         d_total_loss = d_real_loss + d_fake_loss\n","\n","#         # Class loss\n","#         # pred_fake_Y = Cls(fake_Y.detach())\n","#         # pred_real_X = Cls(images_X)\n","#         # loss_semantic_X2Y = criterion_semantic(pred_fake_Y, label_X) * opts.trade_off_semantic\n","#         # pred_fake_X = Cls(fake_X.detach())\n","#         # pred_real_Y = Cls(images_Y)\n","#         # loss_semantic_Y2X = criterion_semantic(pred_fake_X, pred_real_Y) * opts.trade_off_semantic\n","#         # d_total_loss+=loss_semantic_X2Y + loss_semantic_Y2X\n","\n","#         # sum up the losses and update D_X and D_Y\n","#         d_optimizer.zero_grad()\n","#         d_total_loss.backward()\n","#         d_optimizer.step()\n","\n","#         # plot the losses in tensorboard\n","#         logger.add_scalar('D/XY/real', D_X_loss, iteration)\n","#         logger.add_scalar('D/YX/real', D_Y_loss, iteration)\n","#         logger.add_scalar('D/XY/fake', D_fake_X_loss, iteration)\n","#         logger.add_scalar('D/YX/fake', D_fake_Y_loss, iteration)\n","\n","#         # TRAIN THE Classifier\n","#         # Class loss\n","#         pred_fake_Y = Cls_Y(fake_Y.detach())\n","#         pred_real_Y = Cls_Y(images_Y)\n","#         pred_fake_X = Cls_Y(fake_X.detach())\n","#         # loss_Cls_X2Y = criterion_semantic(pred_fake_Y, label_Y) * opts.trade_off_semantic\n","#         loss_Cls_X2Y = criterion_semantic(pred_fake_Y, label_X) * opts.trade_off_semantic\n","#         # loss_Cls_Y = criterion_semantic(pred_real_Y, label_Y) * opts.trade_off_semantic\n","\n","#         loss_Cls_Y = criterion_semantic(pred_fake_X, pred_real_Y) * opts.trade_off_semantic\n","\n","#         pred_fake_X = Cls_X(fake_X.detach())\n","#         pred_real_X = Cls_X(images_X)\n","#         pred_fake_Y = Cls_X(fake_Y.detach())\n","#         # loss_Cls_Y2X = criterion_semantic(pred_fake_X, label_X) * opts.trade_off_semantic\n","#         loss_Cls_Y2X = criterion_semantic(pred_fake_X, label_Y) * opts.trade_off_semantic\n","#         # loss_Cls_X = criterion_semantic(pred_real_X, label_X) * opts.trade_off_semantic\n","\n","#         loss_Cls_X = criterion_semantic(pred_fake_Y, pred_real_X) * opts.trade_off_semantic\n","#         Cls_total_loss=loss_Cls_X2Y + loss_Cls_Y + loss_Cls_Y2X + loss_Cls_X\n","\n","#         # sum up the losses and update D_X and D_Y\n","#         Cls_optimizer.zero_grad()\n","#         Cls_total_loss.backward()\n","#         Cls_optimizer.step()\n","\n","#         # plot the losses in tensorboard\n","#         logger.add_scalar('Cls/XY/real', loss_Cls_X2Y, iteration)\n","#         logger.add_scalar('Cls/Y/real', loss_Cls_Y, iteration)\n","#         logger.add_scalar('Cls/YX/fake', loss_Cls_Y2X, iteration)\n","#         logger.add_scalar('Cls/X/real', loss_Cls_X, iteration)\n","\n","#         # TRAIN THE GENERATORS\n","#         # 1. Generate domain-X-like images based on real images in domain Y\n","#         fake_X = G_YtoX(images_Y)\n","\n","#         # 2. Compute the generator loss based on domain X\n","#         g_loss = torch.mean((D_X(fake_X)-1)**2)\n","#         logger.add_scalar('G/XY/fake', g_loss, iteration)\n","\n","#         if opts.use_cycle_consistency_loss:\n","#             # 3. Compute the cycle consistency loss (the reconstruction loss)\n","#             cycle_consistency_loss = torch.mean(torch.abs(images_Y-G_XtoY(fake_X)))\n","\n","#             g_loss += opts.lambda_cycle * cycle_consistency_loss\n","#             logger.add_scalar('G/XY/cycle', opts.lambda_cycle * cycle_consistency_loss, iteration)\n","\n","#         # X--Y-->X CYCLE\n","#         # 1. Generate domain-Y-like images based on real images in domain X\n","#         fake_Y = G_XtoY(images_X)\n","\n","#         # 2. Compute the generator loss based on domain Y\n","#         g_loss += torch.mean((D_Y(fake_Y)-1)**2)\n","#         logger.add_scalar('G/YX/fake', g_loss, iteration)\n","\n","#         if opts.use_cycle_consistency_loss:\n","#             # 3. Compute the cycle consistency loss (the reconstruction loss)\n","#             cycle_consistency_loss = torch.mean(torch.abs(images_X - G_YtoX(fake_Y)))\n","\n","#             g_loss += opts.lambda_cycle * cycle_consistency_loss\n","#             logger.add_scalar('G/YX/cycle', cycle_consistency_loss, iteration)\n","\n","#         # # Semantic loss\n","#         # pred_fake_T = Cls(fake_Y)\n","#         # pred_real_S = Cls(images_X)\n","#         # loss_semantic_S2T = criterion_semantic(pred_fake_T, label_X) * opts.trade_off_semantic\n","#         # pred_fake_S = Cls(fake_X)\n","#         # pred_real_T = Cls(images_Y)\n","#         # loss_semantic_T2S = criterion_semantic(pred_fake_S, pred_real_T) * opts.trade_off_semantic\n","\n","#         # g_loss+=loss_semantic_S2T + loss_semantic_T2S\n","\n","\n","#         # backprop the aggregated g losses and update G_XtoY and G_YtoX\n","#         g_optimizer.zero_grad()\n","#         g_loss.backward()\n","#         g_optimizer.step()\n","\n","#         # Print the log info\n","#         if iteration % opts.log_step == 0:\n","#             print(\n","#                 'Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | '\n","#                 'd_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n","#                 'd_fake_loss: {:6.4f} | g_loss: {:6.4f} | Cls_Y_loss: {:6.4f} | Cls_X_loss: {:6.4f}'.format(\n","#                     iteration, opts.train_iters, d_real_loss.item(),\n","#                     D_Y_loss.item(), D_X_loss.item(),\n","#                     d_fake_loss.item(), g_loss.item(),\n","#                     loss_Cls_X2Y + loss_Cls_Y, loss_Cls_Y2X + loss_Cls_X\n","\n","#                 )\n","#             )\n","\n","#         # Save the generated samples\n","#         if iteration % opts.sample_every == 0:\n","#             save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n","\n","#         # Save the model parameters\n","#         if iteration % opts.checkpoint_every == 0:\n","#             checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, Cls_X, Cls_Y, opts)\n","\n","\n","# def main(opts):\n","#     \"\"\"Loads the data and starts the training loop.\"\"\"\n","#     len_seq=1\n","#     cur_path = '/content'\n","#     for fol_X in [\"West Brook Lower_01171090\"]:\n","#       for fol_Y in [\"Avery Brook_Bridge_01171000\"]:\n","#         # X=ImageNetVidDataset(path= cur_path, fol=fol_X, phase=\"train\", len_seq=len_seq, transform=transform)\n","#         # Y=ImageNetVidDataset(path= cur_path, fol=fol_Y, phase=\"train\", len_seq=len_seq, transform=transform)\n","\n","#         # # Create  dataloaders for images from the two domains X and Y\n","#         # dataloader_X = torch.utils.data.DataLoader(X, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","#         # dataloader_Y = torch.utils.data.DataLoader(Y, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","#         dataset = ImageNetVidDataset(path= cur_path, fol_A=fol_X, fol_B=fol_Y, phase=\"train\", len_seq=len_seq, transform=transform)\n","#         # dataset_val = ImageNetVidDataset(path= cur_path, fol_A=fol_A, fol_B=fol_B, phase=\"val\", len_seq=len_seq, transform=transform)\n","#         dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","\n","#         # Create checkpoint and sample directories\n","#         utils.create_dir(opts.checkpoint_dir)\n","#         utils.create_dir(opts.sample_dir)\n","\n","#         # Start training\n","#         # training_loop(dataloader_X, dataloader_Y, opts)\n","#         training_loop(dataloader, opts)\n","\n","\n","# def print_opts(opts):\n","#     \"\"\"Prints the values of all command-line arguments.\n","#     \"\"\"\n","#     print('=' * 80)\n","#     print('Opts'.center(80))\n","#     print('-' * 80)\n","#     for key in opts.__dict__:\n","#         if opts.__dict__[key]:\n","#             print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","#     print('=' * 80)\n","\n","\n","# def create_parser():\n","#     \"\"\"Creates a parser for command-line arguments.\n","#     \"\"\"\n","#     parser = argparse.ArgumentParser('Explainable Parser', add_help=False)\n","\n","#     # Model hyper-parameters\n","#     parser.add_argument('--image_size', type=int, default=64)\n","#     parser.add_argument('--disc', type=str, default='dc')  # or 'patch'\n","#     parser.add_argument('--gen', type=str, default='cycle')\n","#     parser.add_argument('--classifier', type=str, default='deep_resnet34')\n","#     parser.add_argument('--g_conv_dim', type=int, default=32)\n","#     parser.add_argument('--d_conv_dim', type=int, default=32)\n","#     parser.add_argument('--norm', type=str, default='instance')\n","#     parser.add_argument('--use_cycle_consistency_loss', action='store_true', default= False)\n","#     parser.add_argument('--init_zero_weights', action='store_true', default=False)\n","#     parser.add_argument('--init_type', type=str, default='naive')\n","\n","#     # Training hyper-parameters\n","#     parser.add_argument('--train_iters', type=int, default=10000)\n","#     parser.add_argument('--batch_size', type=int, default=16)\n","#     parser.add_argument('--num_workers', type=int, default=2)\n","#     parser.add_argument('--lr', type=float, default=0.0003)\n","#     parser.add_argument('--beta1', type=float, default=0.5)\n","#     parser.add_argument('--beta2', type=float, default=0.999)\n","#     parser.add_argument('--lambda_cycle', type=float, default=20)\n","#     parser.add_argument('--trade-off-semantic', type=float, default=1.0, help='trade off for semantic loss')\n","\n","#     # Data sources\n","#     parser.add_argument('--X', type=str, default='')\n","#     parser.add_argument('--Y', type=str, default='')\n","#     parser.add_argument('--ext', type=str, default='*.png')\n","#     parser.add_argument('--use_diffaug', action='store_true')\n","#     parser.add_argument('--data_preprocess', type=str, default='basic')\n","\n","#     # Saving directories and checkpoint/sample iterations\n","#     parser.add_argument('--checkpoint_dir', default='checkpoints_cyclegan')\n","#     parser.add_argument('--sample_dir', type=str, default='/content/drive/My Drive/spatial_temporal/stream_img/CGAN/output/')\n","#     parser.add_argument('--log_step', type=int, default=10)\n","#     parser.add_argument('--sample_every', type=int, default=100)\n","#     parser.add_argument('--checkpoint_every', type=int, default=800)\n","\n","#     parser.add_argument('--gpu', type=str, default='0')\n","\n","#     return parser\n","\n","\n","# if __name__ == '__main__':\n","#     import sys\n","#     sys.argv=[\"\"]\n","#     parser=create_parser()\n","#     opts = parser.parse_args()\n","#     os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu\n","#     opts.sample_dir = os.path.join(\n","#         'output/', opts.sample_dir,\n","#         '%s_%g' % (opts.X.split('/')[0], opts.lambda_cycle)\n","#     )\n","#     opts.sample_dir += '%s_%s_%s_%s_%s' % (\n","#         opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.init_type\n","#     )\n","#     if opts.use_cycle_consistency_loss:\n","#         opts.sample_dir += '_cycle'\n","#     if opts.use_diffaug:\n","#         opts.sample_dir += '_diffaug'\n","\n","#     if os.path.exists(opts.sample_dir):\n","#         cmd = 'rm %s/*' % opts.sample_dir\n","#         os.system(cmd)\n","\n","#     logger = SummaryWriter(opts.sample_dir)\n","\n","#     print_opts(opts)\n","#     main(opts)\n"],"metadata":{"id":"RGBB-5eu4pKm"},"execution_count":null,"outputs":[]}]}